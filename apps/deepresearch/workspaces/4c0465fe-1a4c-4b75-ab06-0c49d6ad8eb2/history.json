[{"parts":[{"content":"Research the latest advances in protein folding since AlphaFold 3","timestamp":"2026-02-16T21:47:07.440386Z","part_kind":"user-prompt"}],"timestamp":"2026-02-16T21:47:07.440653Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"tavily_tavily_search","args":"{\"query\":\"latest advances in protein folding since AlphaFold 3\",\"search_depth\":\"advanced\",\"max_results\":10}","tool_call_id":"call_K2jTvCcSG4yVgR553DxqMt8S","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":10335,"cache_write_tokens":0,"cache_read_tokens":10240,"output_tokens":38,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:08.480716Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:07Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0ipmwvzJjbIPWO9W8QUEy05QQyV","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"tavily_tavily_search","content":"Detailed Results:\n\nTitle: A comparative overview of AlphaFold and its derivatives\nURL: https://pubmed.ncbi.nlm.nih.gov/39970826/\nContent: This review provides a comprehensive analysis of AlphaFold (AF) and its derivatives (AF2 and AF3) in protein structure prediction. These tools have revolutionized structural biology with their highly accurate predictions, driving progress in protein modeling, drug discovery, and the study of protein dynamics. Its exceptional accuracy has redefined our understanding of protein folding, which enables groundbreaking advancements in protein design, disease research and discusses future integration with experimental techniques. In addition, their achievement features, architectures, important case studies, and noteworthy effects in the field of biology and medicine were evaluated. In consideration of the fact that AF2 is a relatively recent innovation, it has already been taken into account in [...] innovation, it has already been taken into account in many studies that highlight its applications in many ways. Moreover, the limitations of AF2 that directed to the introduction of AF3 are also reported, which is a great improvement as it provides precise predictions of the structures and interactions of proteins, DNA, RNA, and ligands, thereby aiding in the understanding of the molecular level. Addressing current challenges and forecasting future developments, this work underscores the lasting significance of AF in reshaping the scientific landscape of protein research. [...] ### Actions\n\n### Page navigation\n\n# Advancements in protein structure prediction: A comparative overview of AlphaFold and its derivatives\n\n### Affiliations\n\n# Advancements in protein structure prediction: A comparative overview of AlphaFold and its derivatives\n\n### Authors\n\n### Affiliations\n\n## Abstract\n\nTitle: Folding the Future: How AI is Reshaping Protein Engineering\nURL: https://www.synbiobeta.com/read/folding-the-future-how-ai-is-reshaping-protein-engineering\nContent: AlphaFold, which won the 2024 Nobel Prize in Chemistry for its researchers, was a true breakthrough in protein folding prediction. AlphaFold is a neural network-based model that predicts protein structures with atomic accuracy without requiring nearly as much computational power as physics-based models. To do that, the algorithm first searches genetic databases for similar protein sequences and creates a multiple-sequence alignment. Then, it generates a pairwise representation to encode spatial relationships between amino acids. The refined pairwise information is passed through a transformer network to produce a final prediction of the protein's structure. [...] This AI tool has revolutionized biology, enabling the design of new proteins for medicine, energy, and sustainability. Since 2020, AlphaFold has been used to predict the structures of millions of proteins, making waves across the scientific community. (As of the writing of this article, the Nature paper describing AlphaFold has been cited 32998 times). The latest AlphaFold 3 model is freely available for non-commercial research use, democratizing protein engineering—something we could not have even imagined a decade ago.\n\n#### Beyond AlphaFold [...] The majority of the initial models, such as Rosetta developed in 2024 Nobel Laurate David Baker’s lab at the University of Washington, were physics-based. While these models could accurately describe the thermodynamic forces driving protein folding, they lacked the computational power to brute-force 10300 possible configurations. The development of deep learning algorithms in the early 2010s was a breakthrough moment for protein structure prediction. In 2018, a team from DeepMind, the UK artificial intelligence lab bought by Google in 2014, entered into the thirteenth CASP competition with a model called AlphaFold and placed first. By the next competition in 2020, AlphaFold 2, a revamped algorithm from DeepMind, left all other models in the dust.\n\nTitle: Beyond AlphaFold 2: The next frontier in macromolecular structure ...\nURL: https://www.mindwalkai.com/blog/beyond-alphafold-2\nContent: In addition to proteins, AlphaFold 3 introduces capabilities for predicting the structures of nucleic acids (such as RNA) and small molecules. This expanded versatility makes it a powerful tool for drug discovery, as it can model the interactions between proteins and ligands. These substantial improvements are critical advancements for biotherapeutic development, where understanding these interactions is essential for developing targeted therapies like monoclonal antibodies and, in the broader sense, developing in silico screening strategies. [...] ## AlphaFold 3: Expanding the horizons of structural biology\n\nIn May 2024, DeepMind and Isomorphic Labs (a subsidiary of Alphabet founded by Demis Hassabis) released AlphaFold 3, with a closed-source web server accessible to academic researchers. At the protein structure prediction, AlphaFold 3 is an improvement over AlphaFold 2: it is better at predicting monomeric and multimeric structures , specifically in the field of antibody-antigen complex modeling where AlphaFold 2 was notoriously lacking . [...] Callaway, E. (2024). Who will make AlphaFold3 open source? Scientists race to crack AI model. Nature, 630(8015), 14-15.\n\n Callaway, E. (2022). After AlphaFold: protein-folding contest seeks next big breakthrough. Nature, 613: 13-14\n\n Editorial, Nature 629, 728 (2024)\n\n  , consulted 2024/10/21\n\n Proteins: Structure, Function, and Bioinformatics: Volume 91, Issue 12 - Special Issue: CASP15: Critical Assessment of methods for Structure Prediction, 15th round, C1-C4, 1535-1951 (2023)\n\n Wallner, B. (2023). Improved multimer prediction using massive sampling with AlphaFold in CASP15. Proteins: Structure, Function, and Bioinformatics, 91(12), 1734-1746.\n\nTitle: How does AlphaFold 3 work?\nURL: https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/\nContent: For instance, to reduce the amount of MSA processing, AlphaFold 3 replaces AlphaFold 2’s Evoformer with a new Pairformer module that has a smaller and simpler MSA embedding block. Some of AlphaFold 3’s advancements, notably its improved performance predictions of antigen-antibody complexes, are directly related to its reduced dependence on the MSA signal. Nevertheless, for protein structure prediction, AlphaFold 3 still makes heavy use of co-evolutionary data from the MSA.\n\nCrucially, AlphaFold 3 predicts the coordinates of individual atoms within a complex. This is distinct from AlphaFold 2, which predicted the positions of amino acid residues and their side chains. The new approach gives AlphaFold 3 the computational flexibility to tackle different types of molecules.\n\n### Tokenisation [...] ### Diffusion\n\nAlphaFold 3 predicts raw atom coordinates using a diffusion module, making it a “generative” ML model. Generative models create new data similar to the examples they learn from: in the case of AlphaFold 3, structures from the Protein Data Bank. This contrasts with non-generative models like AlphaFold 2, which identify patterns in the existing data. [...] Many practical details remain the same, notably the use of multiple sequence alignments (MSAs) as the main input of the neural network. Furthermore, the overall architecture of AlphaFold 3 retains a familiar structure, that of AlphaFold 2, where a large trunk processes a pairwise representation of the chemical complex, which is then used by a structure module to generate explicit atomic positions.\n\nHowever, despite these foundational similarities, AlphaFold 3’s architecture has substantially improved over AlphaFold 2. The new architecture accommodates more general chemical structures and improves the data efficiency of learning.\n\nTitle: AlphaFold — Google DeepMind\nURL: https://deepmind.google/science/alphafold/\nContent: Made from long chains of amino acids, each has a unique complex 3D structure. But figuring out just one of these can take several years, and hundreds of thousands of dollars. In 2020, AlphaFold solved this problem, with the ability to predict protein structures in minutes, to a remarkable degree of accuracy.\n\nThat’s helping researchers understand what individual proteins do and how they interact with other molecules. So valuable time and resources can be redirected into advancing research that could help solve society’s biggest medical and environmental challenges.\n\n### Timeline of a breakthrough\n\n#### In November 2020, AlphaFold was recognised as a solution to the 50-year “protein-folding problem”.\n\n### AlphaGo’s success proves AI’s readiness to tackle protein folding [...] ### Understanding the faulty proteins linked to cancer and autism\n\n### Creating plastic-eating enzymes that could save us from pollution\n\n### Advancing discovery of better drugs and medicine\n\n### The race to cure a billion people from a deadly parasitic disease\n\n### Accelerating the race against antibiotic resistance\n\n### AlphaFold unlocks one of the greatest puzzles in biology\n\nAlphaFold has allowed us to take our project to the next level, from a fundamental science stage to the preclinical and clinical development stage.\n\n## Powered by AlphaFold 3 — AlphaFold Server predicts how proteins will interact with other molecules throughout cells.\n\n### Try AlphaFold\n\n#### A powerful suite of tools, helping the scientific community advance research.\n\n### AlphaFold Protein Structure Database [...] ## AlphaFold Server\n\nPowered by AlphaFold 3 — AlphaFold Server predicts how proteins will interact with other molecules throughout cells.\n\n## AlphaFold Protein Structure Database\n\nView over 200 million protein structure predictions to support your research.\n\n## AlphaFold 3\n\nAccess the AlphaFold 3 model code and weights for academic use.\n\nAlphaFold has revealed millions of intricate 3D protein structures, and is helping scientists understand how life’s molecules interact.\n\n### What is AlphaFold?\n\n#### Proteins underpin every biological process, in every living thing.\n\nTitle: AlphaFold—for predicting protein structures\nURL: https://laskerfoundation.org/winners/alphafold-a-technology-for-predicting-protein-structures/\nContent: Since AlphaFold’s release, we have been thrilled by what the scientific community has done with it. To date it has been used by over a million researchers to advance a huge and diverse range of work, everything from enzyme design to disease understanding to drug discovery. The speed with which AlphaFold has been adopted by the biological community as a standard research tool has been very gratifying to see – it is everything we hoped for and more, and hopefully just the beginning of the impact it will make. [...] When we founded DeepMind in 2010, our goal was to use artificial intelligence to advance knowledge and accelerate scientific discovery. The watershed moment came in 2016 when our prior system, AlphaGo, became the first AI program to beat a world champion at the complex and ancient game of Go. I knew then we had the general AI knowhow and ideas to tackle as formidable a challenge as protein folding and the AlphaFold project was born. [...] It took many further years of development and innovation specific to the problem of course. AlphaFold proved to be the most difficult and complex AI system we had ever built, but when we received the results for the CASP13 competition in 2020 we knew we had achieved atomic accuracy on the target protein structures, an amazing moment that none of us on the team will ever forget.\n\nTitle: The Protein Folding Problem: The day AI unlocked a secret of life\nURL: https://medicine.iu.edu/blogs/research-updates/the-protein-folding-problem-the-day-ai-unlocked-a-secret-of-life\nContent: Comparing its performance to the top team from four years earlier illustrates the progress made. While Baker's team scored a maximum of around 80 points in 2014, AlphaFold achieved an accuracy close to 120 points in 2018. This signifies a substantial leap in the field of protein structure prediction.  \n  \nHow could AlphaFold achieve such remarkable results? Everyone at the competition waited for the AlphaFold team to reveal their experimental model, and the AlphaFold team explained their approach this way (Senior et al., 2020).\n\nFigure 7 [...] 3. Senior, A. W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., ... & Hassabis, D. (2020). Improved protein structure prediction using potentials from deep learning. Nature, 577, 706-710.   \n  \n4. Yang, J., Anishchenko, I., Park, H., Peng, Z., Ovchinnikov, S., & Baker, D. (2020). Improved protein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of Sciences, 117(3), 1496-1503.  \n  \n5. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... & Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596, 583-589. [...] So, how did AlphaFold2 manage to not only outperform all previous attempts by a wide margin but also significantly surpass its predecessor, AlphaFold1? This can be viewed from two perspectives (Jumper et al., 2021).\n\nTitle: AlphaFold Changed Science. After 5 Years, It's Still Evolving | WIRED\nURL: https://www.wired.com/story/alphafold-changed-science-after-5-years-its-still-evolving/\nContent: Its work culminated in the compilation of a database that now contains over 200 million predicted structures, essentially the entire known protein universe, and is used by nearly 3.5 million researchers in 190 countries around the world. The Nature article published in 2021 describing the algorithm has been cited 40,000 times to date. Last year, AlphaFold 3 arrived, extending the capabilities of artificial intelligence to DNA, RNA, and drugs. That transition is not without challenges—such as “structural hallucinations” in the disordered regions of proteins—but it marks a step toward the future.\n\nTo understand what the next five years holds for AlphaFold, WIRED spoke with Pushmeet Kohli, vice president of research at DeepMind and architect of its AI ​​for Science division. [...] We focus on what I call “root node problems,” areas where the scientific community agrees solutions would be transformative, but where conventional approaches won't get us there in the next five to 10 years. Think of it like a tree of knowledge—if you solve these root problems, you unlock entire new branches of research. Protein folding was definitely one of those.\n\nLooking ahead, I see three key areas of opportunity: building more powerful models that can truly reason and collaborate with scientists like a research partner, getting these tools into the hands of every scientist on the planet, and tackling even bolder ambitions, like creating the first accurate simulation of a complete human cell. [...] # AlphaFold Changed Science. After 5 Years, It’s Still Evolving\n\nalphafold\n\nAlphaFold, the artificial intelligence system developed by Google DeepMind, has just turned five. Over the past few years, we've periodically reported on its successes; last year, it won the Nobel Prize in Chemistry.\n\nUntil AlphaFold's debut in November 2020, DeepMind had been best known for teaching an artificial intelligence to beat human champions at the ancient game of Go. Then it started playing something more serious, aiming its deep learning algorithms at one of the most difficult problems in modern science: protein folding. The result was AlphaFold2, a system capable of predicting the three-dimensional shape of proteins with atomic accuracy.\n\nTitle: The Isomorphic Labs Drug Design Engine unlocks a new frontier ...\nURL: https://www.isomorphiclabs.com/articles/the-isomorphic-labs-drug-design-engine-unlocks-a-new-frontier\nContent: Since our release of AlphaFold 3 in 2024 together with Google DeepMind, the field of AI drug discovery has moved at an extraordinary pace. Whilst AlphaFold 3 delivered a dramatic leap in performance from previous generations of structure prediction models, a key challenge remained: understanding biomolecular structures alone was not sufficient for unlocking real-world drug discovery programs in silico (on a computer). [...] ## Structure Prediction of Truly Novel Systems\n\nAccurately predicting the structure of biomolecules and how they interact remains a crucial capability for rational drug design. Many critical downstream tasks are unlocked by being able to accurately model the small nuances in a protein’s geometry - whether understanding the impact of disease-causing mutations, or predicting which molecules will bind to a target protein.\n\nAlphaFold 3 transformed protein-ligand structure prediction at the time of its release and the freely available AlphaFold Protein Database accelerated science on a scale that was previously unimaginable. To date, it has been used by over 3 million researchers in more than 190 countries.\n\nTitle: AlphaFold: Five Years of Impact - Google DeepMind\nURL: https://deepmind.google/blog/alphafold-five-years-of-impact/\nContent: A set of five vertical, stylized graphic panels, each showing abstract, colorful renderings of protein or molecular structures with helical and ribbon elements, representing the impact of AlphaFold.\n\nSince 2020, AlphaFold has accelerated the pace of science and fueled a global wave of biological discovery — an achievement recognized with a Nobel Prize\n\nFive years ago, AlphaFold 2 solved the protein structure prediction problem, unlocking new avenues of biological research and providing our first major proof point that AI can be a powerful tool to advance science. [...] ## A global engine for discovery\n\nIn 2021, we launched the AlphaFold Protein Database in partnership with EMBL-EBI, which was a tipping point toward AlphaFold becoming a scientific tool adopted around the world. And one year later, we released AlphaFold 2’s predictions for more than 200 million protein structures, achieving what would take hundreds of millions of years to solve experimentally.\n\nThe freely available AlphaFold Protein Database has accelerated science on a scale that was previously unimaginable. It has been used by over 3 million researchers in more than 190 countries, including over 1 million users in low- and middle-income countries. Over 30% of AlphaFold-related research is focused on better understanding disease, benefiting human welfare. [...] The model is designed to predict the structure and interactions of all of life's molecules — not just proteins, but DNA, RNA, and ligands (the small molecules that make up most drugs). It can also generate the joint 3D structures of entire molecular complexes, allowing a holistic view of how a potential drug molecule binds to its target protein, or how proteins interact with genetic material.\n\nThe AlphaFold Server is empowering non-commercial researchers globally to harness this technology, accelerating their ability to formulate and test new hypotheses. So far, it’s helped make more than 8 million folds - predictions of structures and interactions - for thousands of researchers around the world.","tool_call_id":"call_K2jTvCcSG4yVgR553DxqMt8S","metadata":null,"timestamp":"2026-02-16T21:47:12.558144Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:12.566940Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"tavily_tavily_extract","args":"{\"urls\":[\"https://pubmed.ncbi.nlm.nih.gov/39970826/\",\"https://www.synbiobeta.com/read/folding-the-future-how-ai-is-reshaping-protein-engineering\",\"https://www.mindwalkai.com/blog/beyond-alphafold-2\",\"https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/\",\"https://www.isomorphiclabs.com/articles/the-isomorphic-labs-drug-design-engine-unlocks-a-new-frontier\",\"https://deepmind.google/blog/alphafold-five-years-of-impact/\"],\"extract_depth\":\"advanced\",\"format\":\"markdown\"}","tool_call_id":"call_HFXh8vfYZnlARISwHj6vRFUp","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":14288,"cache_write_tokens":0,"cache_read_tokens":10368,"output_tokens":178,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:13.939037Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:13Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0ivkNYURAHbsRAL7030MwC3Lq5v","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"tavily_tavily_extract","content":"Tool result too large, saved to: /large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\n\nRead the result using read_file with offset and limit parameters.\nExample: read_file(path=\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\", offset=0, limit=100)\n\nPreview (head/tail):\n\nDetailed Results:\n\nTitle: A comparative overview of AlphaFold and its derivatives - PubMed\nURL: https://pubmed.ncbi.nlm.nih.gov/39970826/\nContent: undefined\n\n... [1471 lines truncated] ...\n\n[Edit](https://www.ebi.ac.uk/node/106902)\n\nThis website requires cookies, and the limited processing of your personal data in order to function. By using the site you are agreeing to this as outlined in our [Privacy Notice](https://www.ebi.ac.uk/data-protection/privacy-notice/embl-ebi-public-website) and [Terms of Use](https://www.ebi.ac.uk/about/terms-of-use).\n\n[I agree, dismiss this banner](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)\n","tool_call_id":"call_HFXh8vfYZnlARISwHj6vRFUp","metadata":null,"timestamp":"2026-02-16T21:47:19.956087Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:19.972769Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":null,"metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":0,\"limit\":150}","tool_call_id":"call_MBoRVH1gc15qc1S74Ua8bJq2","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":14757,"cache_write_tokens":0,"cache_read_tokens":14336,"output_tokens":43,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:21.638555Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:20Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0j2shha8tjXhL3wNTzATZvbSHdN","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"Detailed Results:\n\nTitle: A comparative overview of AlphaFold and its derivatives - PubMed\nURL: https://pubmed.ncbi.nlm.nih.gov/39970826/\nContent: undefined\nRaw Content: Clipboard, Search History, and several other advanced features are temporarily unavailable.\n\n [Skip to main page content](#article-details) \n\nAn official website of the United States government\n\n**The .gov means it’s official.**   \n Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you’re on a federal government site.\n\n**The site is secure.**   \n The **https://** ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely.\n\n[Log in](https://account.ncbi.nlm.nih.gov)\n\n#### Account\n\nLogged in as:  \n **username**\n\n* [Dashboard](/myncbi/)\n* [Publications](/myncbi/collections/bibliography/)\n* [Account settings](/account/settings/)\n* [Log out](/account/signout/)\n\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys) [NCBI Homepage](https://www.ncbi.nlm.nih.gov) [MyNCBI Homepage](/myncbi/) [Main Content](#maincontent) [Main Navigation](#)\n\n* [Clipboard](#)\n* [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F39970826%2F%23open-bibliography-panel)\n* [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F39970826%2F%23open-collections-panel)\n* [Citation manager](#)\n\nDisplay options\n\nFormat\n\n## Save citation to file\n\n## Email citation\n\nEmail address has not been verified. Go to  [My NCBI account settings](https://account.ncbi.nlm.nih.gov/settings/)  to confirm your email and then refresh this page.\n\n### Add to Collections\n\n### Add to My Bibliography\n\n## Your saved search\n\n## Create a file for external citation management software\n\n## Your RSS Feed\n\nReview\n\nActions\n\n* [Search in PubMed](/?term=%22Comput+Biol+Med%22%5Bjour%5D&sort=date&sort_order=desc)\n* [Search in NLM Catalog](https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Comput+Biol+Med%22%5BTitle+Abbreviation%5D)\n* [Add to Search](#)\n\n. 2025 Apr:188:109842.\n\n doi: 10.1016/j.compbiomed.2025.109842.   Epub 2025 Feb 18.\n\n# Advancements in protein structure prediction: A comparative overview of AlphaFold and its derivatives\n\n[Yuktika Malhotra](/?term=Malhotra+Y&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Jerry John](/?term=John+J&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Deepika Yadav](/?term=Yadav+D&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Deepshikha Sharma](/?term=Sharma+D&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Vanshika](/?term=Vanshika&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Kamal Rawal](/?term=Rawal+K&cauthor_id=39970826)[1](#full-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Vaibhav Mishra](/?term=Mishra+V&cauthor_id=39970826)[2](#full-view-affiliation-2 \"Amity Institute of Microbial Technology, Amity University, Uttar Pradesh, 201303, India.\") , [Navaneet Chaturvedi](/?term=Chaturvedi+N&cauthor_id=39970826)[3](#full-view-affiliation-3 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India. Electronic address: nchaturvedi@amity.edu.\")\n\nAffiliations\n\n### Affiliations\n\n* 1 Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\n* 2 Amity Institute of Microbial Technology, Amity University, Uttar Pradesh, 201303, India.\n* 3 Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India. Electronic address: nchaturvedi@amity.edu.\n\n* PMID:  **39970826**\n* DOI:   [10.1016/j.compbiomed.2025.109842](https://doi.org/10.1016/j.compbiomed.2025.109842)\n\nItem in Clipboard\n\nReview\n\n# Advancements in protein structure prediction: A comparative overview of AlphaFold and its derivatives\n\nYuktika Malhotra et al.   Comput Biol Med.   2025 Apr.\n\nDisplay options\n\nFormat\n\nActions\n\n* [Search in PubMed](/?term=%22Comput+Biol+Med%22%5Bjour%5D&sort=date&sort_order=desc)\n* [Search in NLM Catalog](https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Comput+Biol+Med%22%5BTitle+Abbreviation%5D)\n* [Add to Search](#)\n\n. 2025 Apr:188:109842.\n\n doi: 10.1016/j.compbiomed.2025.109842.   Epub 2025 Feb 18.\n\n### Authors\n\n[Yuktika Malhotra](/?term=Malhotra+Y&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Jerry John](/?term=John+J&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Deepika Yadav](/?term=Yadav+D&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Deepshikha Sharma](/?term=Sharma+D&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Vanshika](/?term=Vanshika&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Kamal Rawal](/?term=Rawal+K&cauthor_id=39970826)[1](#short-view-affiliation-1 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\") , [Vaibhav Mishra](/?term=Mishra+V&cauthor_id=39970826)[2](#short-view-affiliation-2 \"Amity Institute of Microbial Technology, Amity University, Uttar Pradesh, 201303, India.\") , [Navaneet Chaturvedi](/?term=Chaturvedi+N&cauthor_id=39970826)[3](#short-view-affiliation-3 \"Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India. Electronic address: nchaturvedi@amity.edu.\")\n\n### Affiliations\n\n* 1 Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India.\n* 2 Amity Institute of Microbial Technology, Amity University, Uttar Pradesh, 201303, India.\n* 3 Amity Institute of Biotechnology, Amity University, Noida, Uttar Pradesh, 201303, India. Electronic address: nchaturvedi@amity.edu.\n\n* PMID:  **39970826**\n* DOI:   [10.1016/j.compbiomed.2025.109842](https://doi.org/10.1016/j.compbiomed.2025.109842)\n\nItem in Clipboard\n\nDisplay options\n\nFormat\n\n## Abstract\n\nThis review provides a comprehensive analysis of AlphaFold (AF) and its derivatives (AF2 and AF3) in protein structure prediction. These tools have revolutionized structural biology with their highly accurate predictions, driving progress in protein modeling, drug discovery, and the study of protein dynamics. Its exceptional accuracy has redefined our understanding of protein folding, which enables groundbreaking advancements in protein design, disease research and discusses future integration with experimental techniques. In addition, their achievement features, architectures, important case studies, and noteworthy effects in the field of biology and medicine were evaluated. In consideration of the fact that AF2 is a relatively recent innovation, it has already been taken into account in many studies that highlight its applications in many ways. Moreover, the limitations of AF2 that directed to the introduction of AF3 are also reported, which is a great improvement as it provides precise predictions of the structures and interactions of proteins, DNA, RNA, and ligands, thereby aiding in the understanding of the molecular level. Addressing current challenges and forecasting future developments, this work underscores the lasting significance of AF in reshaping the scientific landscape of protein research.\n\n**Keywords:**  AlphaFold2; AlphaFold3; Artificial intelligence; DeepMind; Protein structure prediction.\n\nCopyright © 2025 Elsevier Ltd. All rights reserved.\n\n[PubMed Disclaimer](/disclaimer/)\n\n## Conflict of interest statement\n\nDeclaration of competing interest No any authors declare conflict of interest.\n\n## Publication types\n\n* Actions\n\n  + [Search in PubMed](/?term=%22Review%22%5Bpt%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Review)\n  + [Add to Search](#)\n\n## MeSH terms\n\n* Actions\n\n\n[... 1331 more lines. Use offset=150 to read more.]","tool_call_id":"call_MBoRVH1gc15qc1S74Ua8bJq2","metadata":null,"timestamp":"2026-02-16T21:47:21.960673Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:21.962607Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":150,\"limit\":200}","tool_call_id":"call_D5e8BmDitoXNJfjL45HcCZ89","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":17236,"cache_write_tokens":0,"cache_read_tokens":14720,"output_tokens":43,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:23.187045Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:22Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0j4TlQ7GyPpdUvUQwboK1uOZ6W2","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"  + [Search in PubMed](/?term=%22Computational+Biology%2Fmethods%22%5BMAJR%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Computational+Biology)\n  + [Add to Search](#)\n* Actions\n\n  + [Search in PubMed](/?term=%22Humans%22%5BMeSH%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Humans)\n  + [Add to Search](#)\n* Actions\n\n  + [Search in PubMed](/?term=%22Models%2C+Molecular%22%5BMAJR%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Models%2C+Molecular)\n  + [Add to Search](#)\n* Actions\n\n  + [Search in PubMed](/?term=%22Protein+Conformation%22%5BMeSH%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Protein+Conformation)\n  + [Add to Search](#)\n* Actions\n\n  + [Search in PubMed](/?term=%22Protein+Folding%22%5BMAJR%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Protein+Folding)\n  + [Add to Search](#)\n* Actions\n\n  + [Search in PubMed](/?term=%22Proteins%2Fchemistry%22%5BMAJR%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Proteins)\n  + [Add to Search](#)\n\n## Substances\n\n* Actions\n\n  + [Search in PubMed](/?term=%22Proteins%22%5Bnm%5D&sort=date&sort_order=desc)\n  + [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=%22Proteins%22)\n  + [Add to Search](#)\n\n## LinkOut - more resources\n\n* ### Full Text Sources\n\n  + [ClinicalKey](https://www.clinicalkey.com/content/playBy/pii?v=S0010-4825(25)00192-1)\n  + [Elsevier Science](https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(25)00192-1)\n\n**Full text links**\n\n[Elsevier Science](https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(25)00192-1 \"See full text options at Elsevier Science\")\n\n**Send To**\n\n* [Clipboard](#)\n* [Email](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F39970826%2F%23open-email-panel)\n* [Save](#)\n* [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F39970826%2F%23open-bibliography-panel)\n* [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F39970826%2F%23open-collections-panel)\n* [Citation Manager](#)\n\nNCBI Literature Resources\n\n[MeSH](https://www.ncbi.nlm.nih.gov/mesh/) [PMC](https://www.ncbi.nlm.nih.gov/pmc/) [Bookshelf](https://www.ncbi.nlm.nih.gov/books) [Disclaimer](/disclaimer/)\n\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.\n\n \n\nTitle: Folding the Future: How AI is Reshaping Protein Engineering\nURL: https://www.synbiobeta.com/read/folding-the-future-how-ai-is-reshaping-protein-engineering\nContent: undefined\nRaw Content: # Folding the Future: How AI is Reshaping Protein Engineering\n\nWith its power to reduce costs, increase efficiency, and ability to address pressing global challenges, generative protein design is poised to transform the biotech landscape and the broader bioeconomy\n\nMar 31, 2025\n\nSince the birth of biotechnology, researchers have been on a quest to develop better proteins: enzymes that retain their function at high temperatures, antibodies that bind targets with one-in-a-million specificity, or structural proteins that are stronger, lighter, and cheaper to make than traditional materials. Nature’s versatile biological machines, proteins, are composed of strings of amino acids that fold into complex, dynamic shapes—like origami. The function of the protein is determined by its structure, which, in turn, depends on the amino acid sequence.\n\nThe vision of engineering custom proteins with specified properties and functions by designing a specific amino acid sequence has captivated scientists for a long time. The problem is that protein sequence space is practically immeasurable, and trying to engineer a protein to perform a specific function is a Sisyphean task. For decades, protein engineers evolved functions—such as an enzyme’s thermostability or substrate affinity—in the lab using a method called directed evolution. Alternatively, they would attempt to rationally design the desired functions by looking at the protein structure (a 3D model obtained through X-ray crystallography or cryo-electron microscopy) and trying to figure out which protein regions they needed to change, followed by making hundreds of different mutated variants and testing them in the lab.\n\nAs artificial intelligence continues to transform protein engineering, thought leaders and startup pioneers alike will converge at this year’s [SynBioBeta: The Global Synthetic Biology Conference](https://www.synbiobeta.com/events/synbiobeta-2025) to discuss the latest breakthroughs in generative models for protein design. Presenters will highlight how these computational tools are fast becoming an essential driver of innovation across the synthetic biology landscape.\n\n#### The AlphaFold Revolution\n\nGiven the arduous and inefficient nature of engineering proteins in the lab, the field was long overdue for a computational makeover. However, until about 2018, the computational power required to solve a protein structure was considered unattainable. Back in 1969, molecular biologist Cyrus Levinthal estimated that it would take longer than the age of the known universe to solve the protein folding problem since a typical-length protein can fold into 10300 possible configurations. But that doesn’t mean no one has tried it.\n\nSince the late 1980s, scientists developed molecular physics-based models to try to figure out the rules of protein folding. To compare how well the different models worked, researchers established the [Critical Assessment of Structure Prediction](https://predictioncenter.org/) (CASP) challenge, a protein structure prediction competition that began in 1994. In the course of the competition, teams would attempt to solve the structure of a protein based on their models and then compare those predictions with experimentally determined structures.\n\nThe majority of the initial models, such as [Rosetta](https://rosettacommons.org/software/) developed in 2024 Nobel Laurate [David Baker’s lab](https://www.bakerlab.org/) at the [University of Washington](https://www.washington.edu/), were physics-based. While these models could accurately describe the thermodynamic forces driving protein folding, they lacked the computational power to brute-force 10300 possible configurations. The development of deep learning algorithms in the early 2010s was a breakthrough moment for protein structure prediction. In 2018, a team from [DeepMind](https://deepmind.google/), the UK artificial intelligence lab bought by Google in 2014, entered into the thirteenth CASP competition with a model called AlphaFold and [placed first](https://www.forbes.com/sites/samshead/2018/12/03/deepmind-starts-to-show-how-ai-can-be-used-to-solve-scientific-problems/). By the next competition in 2020, AlphaFold 2, a revamped algorithm from DeepMind, left all other models in the dust.\n\nAlphaFold, which won the [2024 Nobel Prize in Chemistry](https://www.nobelprize.org/prizes/chemistry/2024/press-release/) for its researchers, was a true breakthrough in protein folding prediction. AlphaFold is a neural network-based model that predicts protein structures with atomic accuracy without requiring nearly as much computational power as physics-based models. To do that, the algorithm first searches genetic databases for similar protein sequences and creates a multiple-sequence alignment. Then, it generates a pairwise representation to encode spatial relationships between amino acids. The refined pairwise information is passed through a transformer network to produce a final prediction of the protein's structure.\n\nThis AI tool has revolutionized biology, enabling the design of new proteins for medicine, energy, and sustainability. Since 2020, AlphaFold has been used to predict the structures of millions of proteins, making waves across the scientific community. (As of the writing of this article, the [Nature paper](https://www.nature.com/articles/s41586-021-03819-2) describing AlphaFold has been cited 32998 times). The latest AlphaFold 3 model is [freely available](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/) for non-commercial research use, democratizing protein engineering—something we could not have even imagined a decade ago.\n\n#### Beyond AlphaFold\n\nSince the breakthrough of AlphaFold, other AI models have been developed by academic labs and biotech companies, including  [EvoBind](https://www.biorxiv.org/content/10.1101/2022.07.23.501214v1), [ESMFold](https://www.science.org/doi/10.1126/science.ade2574), and [RFDiffusion](https://www.bakerlab.org/2023/07/11/diffusion-model-for-protein-design/), which can predict protein structure and function, as well as elucidate the folding of other molecule types (DNA and RNA) and model complex molecular interactions between proteins and their ligands.\n\n[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8) is a model built on an open-source algorithm called [RoseTTAFold](https://www.science.org/doi/10.1126/science.abj8754), developed by David Baker’s lab. It generates new protein structures in a manner similar to how DALL-E or Midjourney generate art. This tool was able to solve new design challenges, including molecular binding and oligomer design. Both RoseTTAFold and RFdiffusion have since then been upgraded to [All-Atom](https://www.ipd.uw.edu/2023/10/introducing-rosettafold-and-rfdiffusion-all-atom/) versions, which enable modeling of not just proteins but biological complexes made from different molecules, including DNA, RNA, small molecules, metals, and other bonded atoms.\n\nAs large language models like ChatGPT took the world by storm, scientists realized that you can use them for proteins as well—and this has turned out to be a very successful strategy. These gigantic models trained on extensive volumes of data enable scientists to generate de novo designs based not on protein structure but function. Dubbed [protein language models](https://www.nature.com/articles/s41598-025-86519-5) (PLMs), these tools are a lot more accessible for researchers because tuning a natural language model is much easier than developing conventional machine learning algorithms.\n\nThe progress in generative AI for protein design is by no means finished. Researchers are still working to refine the algorithms to create better proteins for specific applications, as well as expand the capabilities of what is possible for biology in the age of AI. Last month, NVIDIA released [GenMol](https://developer.nvidia.com/blog/evaluating-genmol-as-a-generalist-foundation-model-for-molecular-generation/), a foundational model for molecular generation. GenMol offers a versatile molecular generative framework based on discrete diffusion and non-autoregressive decoding designed to streamline drug discovery from molecule design to lead optimization.\n\n#### Generative Design in Synthetic Biology\n\nSo, what does this mean for synthetic biology and the bioeconomy?\n\nThe dream of being able to design custom proteins has finally come true. For example, we can make new enzymes that are stable at high temperatures. Using enzymes that do not degrade in high-temperature industrial processes (such as making paper from wood pulp) translates into millions of dollars in savings. AI is also revolutionizing drug discovery research by allowing scientists to model how proteins and small-molecule drugs interact with their targets. Additionally, we can engineer proteins that have no equivalents in nature—which could help us solve the challenges of the twenty-first century.\n\nCompanies such as [Cradle](https://www.cradle.bio/) have been using protein language models to design better [enzymes](https://www.cradle.bio/blog/case-study-combining-ai-generated-mutations-with-human-expertise-for-better-enzyme-designs), [receptor-binding proteins](https://www.cradle.bio/blog/adaptyv-protein-design-competition), and [antibodies](https://www.cradle.bio/blog/antibody-ngs-data-initiative), with applications across the entire biotechnology spectrum, from industrial and food biotech to biopharma. [Absci](https://www.absci.com/) is transforming the biologics discovery pipeline by enabling [de novo](https://www.synbiobeta.com/read/absci-and-amd-team-up-to-revolutionize-drug-discovery-with-ai) antibody design. And [Ginkgo’s](https://www.ginkgo.bio/) models have been able to improve [PETase activity](https://foundrytheory.substack.com/p/the-petase-mask-fill-challenge), an enzyme capable of degrading plastic.\n\nAs the recent [Advanced Biotech for Sustainability (AB4S](https://www.ab4s.org/report)) [report](https://www.ab4s.org/report) highlights, advanced biotechnology could reduce global emissions by 5% and generate $1 trillion in economic value across the food, agriculture, chemicals, personal care, and transportation fuels sectors. With generative AI becoming a mainstay in biotechnology, transforming the world through synthetic biology is no longer a pipe dream—it is just a work in progress.\n\n###### Read More\n\n[View All](../read)\n\nAaron Blotnick\n\nFeb 11, 2026\n\n###### [Burn, Baby, Burn: EPA Moves to Dismantle the Legal Backbone of U.S. Climate Policy](./burn-baby-burn-epa-moves-to-dismantle-the-legal-backbone-of-u.s.-climate-policy)\n\nAaron Blotnick\n\nFeb 11, 2026\n\n###### [Burn, Baby, Burn: EPA Moves to Dismantle the Legal Backbone of U.S. Climate Policy](./burn-baby-burn-epa-moves-to-dismantle-the-legal-backbone-of-u.s.-climate-policy)\n\n[Longevity](../)\n\nAaron Blotnick\n\nFeb 10, 2026\n\n###### [J. Craig Venter Launches Diploid Genomics, Inc. to Revolutionize Human Genomics](./j.-craig-venter-launches-diploid-genomics-inc.-to-revolutionize-human-genomics)\n\n[Longevity](../)\n\nAaron Blotnick\n\nFeb 10, 2026\n\n###### [J. Craig Venter Launches Diploid Genomics, Inc. to Revolutionize Human Genomics](./j.-craig-venter-launches-diploid-genomics-inc.-to-revolutionize-human-genomics)\n\n[Biomanufacturing Scale Up\n\nBio Design\n\nAi Digital Biology](../)\n\nAaron Blontick\n\nFeb 10, 2026\n\n###### [Ginkgo Bioworks' Autonomous Laboratory Driven by OpenAI's GPT-5 Achieves 40% Improvement Over State-of-the-Art Scientific Benchmark](./ginkgo-bioworks-autonomous-laboratory-driven-by-openai-s-gpt-5-achieves-40-improvement-over-state-of-the-art-scientific-benchmark)\n\n[Biomanufacturing Scale Up\n\nBio Design\n\nAi Digital Biology](../)\n\nAaron Blontick\n\nFeb 10, 2026\n\n###### [Ginkgo Bioworks' Autonomous Laboratory Driven by OpenAI's GPT-5 Achieves 40% Improvement Over State-of-the-Art Scientific Benchmark](./ginkgo-bioworks-autonomous-laboratory-driven-by-openai-s-gpt-5-achieves-40-improvement-over-state-of-the-art-scientific-benchmark)\n\n[Bioeconomy Policy\n\nBiomanufacturing Scale Up\n\nChemicals Materials](../)\n\nMohamed Soufi\n\nFeb 10, 2026\n\n###### [Why the Department of War Is Backing Visolis to Scale Bio-Based Solvents](./why-the-department-of-war-is-backing-visolis-to-scale-bio-based-solvents)\n\n[Bioeconomy Policy\n\nBiomanufacturing Scale Up\n\nChemicals Materials](../)\n\nMohamed Soufi\n\nFeb 10, 2026\n\n###### [Why the Department of War Is Backing Visolis to Scale Bio-Based Solvents](./why-the-department-of-war-is-backing-visolis-to-scale-bio-based-solvents)\n\n[')\">](./the-bioelectric-tech-stack)\n\n[Biomanufacturing Scale Up\n\nBioeconomy Policy\n\nAi Digital Biology\n\n\n[... 1131 more lines. Use offset=350 to read more.]","tool_call_id":"call_D5e8BmDitoXNJfjL45HcCZ89","metadata":null,"timestamp":"2026-02-16T21:47:23.440007Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:23.442710Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":350,\"limit\":200}","tool_call_id":"call_UkREzagw2htsDqjePiL40Xyo","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":21075,"cache_write_tokens":0,"cache_read_tokens":17152,"output_tokens":43,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:24.645942Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:24Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0j6jIINq0aCQNykW8Jz3WVejT2l","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"Bio Design\n\nChemicals Materials](../)\n\nCameron Watson\n\nFeb 6, 2026\n\n###### [The Bioelectric Tech Stack](./the-bioelectric-tech-stack)\n\n[')\">](./the-bioelectric-tech-stack)\n\n[Biomanufacturing Scale Up\n\nBioeconomy Policy\n\nAi Digital Biology\n\nBio Design\n\nChemicals Materials](../)\n\nCameron Watson\n\nFeb 6, 2026\n\n###### [The Bioelectric Tech Stack](./the-bioelectric-tech-stack)\n\n[Bioeconomy Policy\n\nEnvironment\n\nBiosecurity Bioethics](../)\n\nAaron Blotnick\n\nFeb 3, 2026\n\n###### [Colossal Biosciences Unveils Global BioVault Initiative with UAE to Preserve Endangered Species](./colossal-biosciences-unveils-global-biovault-initiative-with-uae-to-preserve-endangered-species)\n\n[Bioeconomy Policy\n\nEnvironment\n\nBiosecurity Bioethics](../)\n\nAaron Blotnick\n\nFeb 3, 2026\n\n###### [Colossal Biosciences Unveils Global BioVault Initiative with UAE to Preserve Endangered Species](./colossal-biosciences-unveils-global-biovault-initiative-with-uae-to-preserve-endangered-species)\n\nAaron Blotnick\n\nFeb 11, 2026\n\n###### [Burn, Baby, Burn: EPA Moves to Dismantle the Legal Backbone of U.S. Climate Policy](./burn-baby-burn-epa-moves-to-dismantle-the-legal-backbone-of-u.s.-climate-policy)\n\n[Longevity](../)\n\nAaron Blotnick\n\nFeb 10, 2026\n\n###### [J. Craig Venter Launches Diploid Genomics, Inc. to Revolutionize Human Genomics](./j.-craig-venter-launches-diploid-genomics-inc.-to-revolutionize-human-genomics)\n\n[Biomanufacturing Scale Up\n\nBio Design\n\nAi Digital Biology](../)\n\nAaron Blontick\n\nFeb 10, 2026\n\n###### [Ginkgo Bioworks' Autonomous Laboratory Driven by OpenAI's GPT-5 Achieves 40% Improvement Over State-of-the-Art Scientific Benchmark](./ginkgo-bioworks-autonomous-laboratory-driven-by-openai-s-gpt-5-achieves-40-improvement-over-state-of-the-art-scientific-benchmark)\n\n[Bioeconomy Policy\n\nBiomanufacturing Scale Up\n\nChemicals Materials](../)\n\nMohamed Soufi\n\nFeb 10, 2026\n\n###### [Why the Department of War Is Backing Visolis to Scale Bio-Based Solvents](./why-the-department-of-war-is-backing-visolis-to-scale-bio-based-solvents)\n\n###### Newletter & More\n\n[SynBioBeta\n\n2026\n\nMay 4-7,\n\n2026\n\nSan Jose Convention Center ·\n\nCA, USA\n\nEvent Details](https://www.syntheticbiologysummit.com/?utm_source=sbb-website&utm_medium=ad-card&utm_campaign=website)\n\n[SynBioBeta\n\n2026\n\nMay 4-7,\n\n2026\n\nSan Jose Convention Center ·\n\nCA, USA\n\nEvent Details](https://www.syntheticbiologysummit.com/?utm_source=sbb-website&utm_medium=ad-card&utm_campaign=website)\n\n[SynBioBeta\n\n2026\n\nMay 4-7,\n\n2026\n\nSan Jose Convention Center ·\n\nCA, USA\n\nEvent Details](https://www.syntheticbiologysummit.com/?utm_source=sbb-website&utm_medium=ad-card&utm_campaign=website)\n\n## SynBioBeta\n\nJoin the innovators shaping the future with SynBio + AI. From health to ag, materials & more—be part of the revolution.\n\n[Purchase Pass](https://www.syntheticbiologysummit.com/#purchase-pass-new)\n\n## SynBioBeta\n\nJoin the innovators shaping the future with SynBio + AI. From health to ag, materials & more—be part of the revolution.\n\n[Purchase Pass](https://www.syntheticbiologysummit.com/#purchase-pass-new)\n\n## SynBioBeta\n\nJoin the innovators shaping the future with SynBio + AI. From health to ag, materials & more—be part of the revolution.\n\n[Purchase Pass](https://www.syntheticbiologysummit.com/#purchase-pass-new)\n\n[## Newsletter\n\nJoin List](../signup)\n\n[## Newsletter\n\nJoin List](../signup)\n\n[## Newsletter\n\nJoin List](../signup)\n\n[Log In](../sign-in)\n\n[Sign Up](../sign-up)\n\n#### Newsletter\n\n[Subscribe](../signup)\n\n[Insights](../insights)\n\n[Industry News](../synthetic-bio-news)\n\n[Multimedia](../multimedia)\n\n[Events](../events)\n\n[Advertising](../about/advertising)\n\n[About](../about/about)\n\nContent\n\n[Insights](../insights)\n\n[Industry News](../synthetic-bio-news)\n\n[Podcast](../listen)\n\n[Video](../watch)\n\nCategories\n\n[Ai Digital Biology](../categories/ai-digital-biology)\n\n[Bio Design](../categories/biodesign)\n\n\n[... 931 more lines. Use offset=550 to read more.]","tool_call_id":"call_UkREzagw2htsDqjePiL40Xyo","metadata":null,"timestamp":"2026-02-16T21:47:25.042377Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:25.044215Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\": \"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\", \"offset\": 550, \"limit\": 200}","tool_call_id":"call_xnvOsUFC1oa3zPuRH2kmLHVj","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"},{"tool_name":"read_file","args":"{\"path\": \"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\", \"offset\": 750, \"limit\": 200}","tool_call_id":"call_Vlp2kNAK3kXr4CMuqmV6iHbM","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":22197,"cache_write_tokens":0,"cache_read_tokens":20992,"output_tokens":102,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:26.938725Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:25Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0j7GenQVKRsxnxdOlIZz6qNG7jm","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"[Bioeconomy Policy](../categories/bioeconomy-policy)\n\n[Biomanufacturing Scale Up](../categories/biomanufacturing-scale-up)\n\n[Biopharma Solutions Tools Tech](../categories/biopharma-solutions-tools-tech)\n\n[Biosecurity Bioethics](../categories/biosecurity-bioethics)\n\n[Capital Markets](../categories/capital-markets)\n\n[Chemicals Materials](../categories/chemicals-materials)\n\n[Consumer Products](../categories/consumer-products)\n\n[Desci](../categories/desci)\n\n[Engineered Human Therapies](../categories/engineered-human-therapies)\n\n[Environment](../categories/environment)\n\n[Food Agriculture](../categories/food-agriculture)\n\n[Longevity](../categories/longevity)\n\n[Neurotech](../categories/neurotech)\n\n[Psychedelics](../categories/psychedelics)\n\n[Reading Writing And Editing Dna](../categories/reading-writing-and-editing-dna)\n\n[Space Exploration](../categories/space-exploration)\n\n[Sponsored Content](../categories/sponsored-content)\n\n[About](../about/about)\n\n[Advertising](../about/advertising)\n\n[Newsletter](../signup)\n\n[Events](../events)\n\nSocial Media\n\n[')\">](https://www.facebook.com/SynBioBeta)\n\n[')\">](https://www.instagram.com/synbiobeta/)\n\n[')\">](https://x.com/SynBioBeta)\n\n[')\">](https://linkedin.com/company/synbiobeta/)\n\n[')\">](https://www.youtube.com/@synbiobeta)\n\n[')\">](https://www.tiktok.com/@synbiobeta)\n\n[')\">](https://www.tiktok.com/@synbiobeta)\n\n[')\">](https://www.tiktok.com/@synbiobeta)\n\n© Copyright SynBioBeta\n\n2026\n\n[Cookie Settings](../policies/cookie-settings)\n\n[Privacy Policy](../policies/privacy-policy)\n\n[Insights](../insights)\n\n[Industry News](../synthetic-bio-news)\n\n[Multimedia](../multimedia)\n\n[Events](../events)\n\n[Advertising](../about/advertising)\n\n[About](../about/about)\n\n \n\nTitle: Beyond AlphaFold 2: Next Frontier in Structure Prediction\nURL: https://www.mindwalkai.com/blog/beyond-alphafold-2\nContent: undefined\nRaw Content: [Skip to the main content.](#main-content)\n\n[Get started](https://www.mindwalkai.com/company/contact)\n\n* [We Build It](#) \n  + [Drug Discovery Overview](https://www.mindwalkai.com/biologics-discovery/drug-discovery)\n  + [Advanced Lab Research](https://www.mindwalkai.com/biologics-discovery/advanced-lab-research)\n  + [Therapeutics Discovery](https://www.mindwalkai.com/biologics-discovery/therapeutic)\n  + [Anti-drug Antibody Discovery](https://www.mindwalkai.com/biologics-discovery/anti-drug)\n  + [Diagnostic Antibody Discovery](https://www.mindwalkai.com/biologics-discovery/diagnostic)\n  + [Protein Engineering & Production](https://www.mindwalkai.com/biologics-discovery/protein)\n  + [Partnership](https://www.mindwalkai.com/ai-collaboration/partnership)\n* [You Build It](#) \n  + [LensAI SaaS](https://www.mindwalkai.com/ai-collaboration/lensai-saas)\n  + [Partnership](https://www.mindwalkai.com/ai-collaboration/partnership)\n* [Technology](#) \n  + [LensAI Overview](https://www.mindwalkai.com/ai-technology/lensai-overview)\n  + [Data Overview](https://www.mindwalkai.com/ai-technology/data)\n  + [Technology Partners](https://www.mindwalkai.com/ai-technology/technology-partners)\n* [Resource Hub](#) \n  + [Resource Overview](https://www.mindwalkai.com/discovery-resources)\n  + [Blog](https://www.mindwalkai.com/blog)\n  + [MindWalk Collaborative](https://www.mindwalkai.com/discovery-resources/mindwalk-collaborative)\n  + [Newsroom](https://www.mindwalkai.com/discovery-resources/news)\n  + [Events](https://www.mindwalkai.com/discovery-resources/events)\n* [Partner-ready Assets](#) \n  + [The Vault](https://www.mindwalkai.com/partner-ready-assets/biologics-pipeline)\n* [Company](#) \n  + [About Us](https://www.mindwalkai.com/company)\n  + [Investor Relations](https://ir.mindwalkai.com/)\n  + [Brand Inspiration](https://www.mindwalkai.com/company/brand-story)\n  + [Careers](https://www.mindwalkai.com/company/careers)\n* [Connect](#) \n  + [Contact Us](https://www.mindwalkai.com/company/contact)\n  + [Locations](https://www.mindwalkai.com/company/locations)\n\n##### CaseXCase Series\n\n---\n\nA full collection of the latest validated results showing how transformative technology can help create change.\n\n [Explore series](https://www.mindwalkai.com/discovery-resources/casexcase)\n\n\n\nfalse\n\n# Beyond AlphaFold 2: The next frontier in macromolecular structure prediction\n\n[Sébastien Lemal PhD](https://www.mindwalkai.com/blog/author/sébastien-lemal-phd)  :   01.21.25\n\n###### Audio version\n\nBeyond AlphaFold 2: The next frontier in macromolecular structure prediction\n\n13:26\n\nRead time 9 min\n\n## **Introduction**\n\nAs a testament to the recent breakthrough of deep-learning technologies in the field of (structural) bioinformatics, half of the Nobel Prize in Chemistry 2024 [[1](https://www.nobelprize.org/prizes/chemistry/2024/press-release/)] has been awarded to John Jumper and Demis Hassabis, the main contributors to AlphaFold 2, the other half to Prof. David Baker (University of Washington, Seattle). Speaking about breakthroughs is not an understatement: as of this time of writing, the original AlphaFold2 publication [2] has been cited more than 27,800 times (according to Google Scholar [[3](https://scholar.google.com/scholar?cites=6286436358625670901)]). For comparison, on Feb 21, 2023 (roughly 1.5 years ago), the number of citations was just 8,783. AlphaFold 2 is a solution to the protein folding problem and can predict with near experimental accuracy the structure of proteins as long as their primary structures (the sequence of amino acids along the protein chain) are known. This technology has been integrated with the LensAIⁱ™ *in silico* discovery platform, and we have discussed it in length in different blog posts since the public release of AlphaFold2 [[4](https://blog.biostrand.ai/explained-a-brief-look-into-alphafold-2), [5](https://blog.biostrand.ai/explained-how-to-plot-the-prediction-quality-metrics-with-alphafold2), [6](https://blog.biostrand.ai/scaling-up-structural-biology-with-alphafold2)]. In this new blog post, we will review how these latest developments are impacting drug discovery, what can be technically achieved with current technology, and assess limitations that hinder discovery processes and future outcomes. Finally, we will briefly present how these breakthrough technologies are integrated within the MindWalk LensAI platform.\n\n## **AlphaFold 3: Expanding the horizons of structural biology**\n\nIn May 2024, DeepMind and Isomorphic Labs (a subsidiary of Alphabet founded by Demis Hassabis) released AlphaFold 3, with a closed-source web server accessible to academic researchers. At the protein structure prediction, AlphaFold 3 is an improvement over AlphaFold 2: it is better at predicting monomeric and multimeric structures [7], specifically in the field of antibody-antigen complex modeling where AlphaFold 2 was notoriously lacking [8].\n\nIn addition to proteins, AlphaFold 3 introduces capabilities for predicting the structures of nucleic acids (such as RNA) and small molecules. This expanded versatility makes it a powerful tool for drug discovery, as it can model the interactions between proteins and ligands. These substantial improvements are critical advancements for biotherapeutic development, where understanding these interactions is essential for developing targeted therapies like monoclonal antibodies and, in the broader sense, developing *in silico* screening strategies.\n\nWhile the authors' study in the original publication shows beyond state-of-the-art performance for many tasks, third-party benchmarks are still missing for AlphaFold 3, partly due to the limited capacity of the web server and its initial closed-source nature. As announced earlier in the year [[13](https://x.com/pushmeet/status/1790086453520691657)], the source code was released in November 2024, although with a restricted license; thus AlphaFold 3 is less susceptible to tweaking, in-depth analysis, and integration to protein design pipelines compared to AlphaFold 2 [[6](https://blog.biostrand.ai/scaling-up-structural-biology-with-alphafold2)].\n\nIn addition to the base AlphaFold 3 code, several third-party initiatives have taken the initiative to reproduce the architecture of the model, as it was done for AlphaFold 2 before its release [[3](https://scholar.google.com/scholar?cites=6286436358625670901)], and many AlphaFold 3-like prediction pipelines have been released, such as Boltz-1 or Chai-1.\n\nAlphaFold 3 success rate on different benchmark sets, for (from the left to the right) ligand docking, nucleic acids, covalent modifications and protein predictions; and compared to state-of-the-art methods. Adapted from Ref. 7.\n\nSo far, the substantial improvements of AlphaFold 3 outbalance its known limitations: for instance, the algorithm struggles with molecule chirality. Atomic clashes also occur, specifically for large proteins, so molecules can partially overlap, which is physically impossible. As success rates for some tasks remain low, “hallucinations” may happen. Finally, predictions remain static in nature and completely ignore any dynamical aspect of molecular interaction. These limitations are, of course, not specific to AlphaFold 3, and there are many ways to mitigate these shortcomings by integrating structure prediction within a broader framework for molecular modeling. For instance, models generated by AlphaFold can be used in Molecular Dynamics simulations to assess conformational dynamics, interaction energies between molecular partners, and much more.\n\n## **Structure prediction in practice**\n\nDespite their fame, the practical use of structure prediction tools such as AlphaFold is not often well understood. These tools works within the paradigm that for a given input of sequential molecular data (sequence of amino acids for protein, nucleic acids, …), there is a “static” 3D structure (atomic position) which can be predicted solely from this data, representative on the interaction between all involved atoms. While this picture is simplistic and ignores the dynamical nature of macromolecular interactions, which is only partially captured by static representations.\n\nWithin this paradigm, ideally, one would expect that a given input yields a single prediction. Yet, this is not the case. For AlphaFold 2 monomers, there are 5 trained model weights which outputs 5 predictions for a single inputs. These predictions are scored and ranked by the model, using a so-called confidence metric. The most accurate model is expected to be ranked at the top. For AlphaFold 2 Multimer, it has been found that more than 5 predictions are necessary to obtain accurate models; thus the standard pipeline outputs 25 models which can be later inspected.\n\nHowever, it is not always guaranteed that the most accurate prediction (compared to a ground structure structure) is always ranked at the top. Typically, a criterion is defined, and the top ranking model match that criterion, then the prediction is considered correct. In benchmarks, the top-N success rate is the number of correct predictions up to rank N. For instance, the top-1 success rate is the number of case with a successful top-1 prediction over the full dataset, the top-5 success rate consider all ranks up to 5, and so on. For a given set of prediction, the probability of finding a correct prediction increases.\n\nIn the case of protein complexes, it is notoriously hard to predict bound conformations using traditional docking techniques. The top-1 success rate for traditional methods (docking) is typically low (a few percent), and for these methods, it is often necessary to consider a wider pool of predictions along with complementary methods such as molecular dynamics to assess what is the likely correct method. AlphaFold 2 Multimer became the gold standard for protein complex predictions, and AlphaFold 3 extends to a much larger landscape of interactions, involving nearly all kinds of molecules in life science.\n\nOnce a satisfying prediction is obtained, downstream tasks may be performed with other tools than AlphaFold. Long molecular dynamics simulations can be used to sample the conformational landscape, identifying key functional domains, assessing the stability, performing mutagenesis analysis, and so on. Structure prediction is thus one of the early step in the drug discovery phases, and must be complemented with additional analyses.\n\n## **Is AlphaFold 2 obsolete?**\n\nWith the release of AlphaFold 3, one might wonder if AlphaFold 2 is now outdated. The answer is rather nuanced. While AlphaFold 3 offers improvements in specific areas like nucleic acid/protein predictions and ligand docking, AlphaFold 2 remains highly relevant.\n\nThe reality is that AlphaFold 2 has been integrated within more intricate workflows, which, in some cases, extends its use beyond simple structure prediction and, in other cases, significantly improves its performance in specific tasks such as multimeric predictions as witnessed from the results of CASP15 [14]. For example, AlphaFold 2 and ProteinMPNN have been integrated into a pipeline for a complete *de novo* complex protein fold design with targeted properties [23, 24]. Another example is protein complex prediction, which is highly improved through techniques like massive sampling and dropout layer activation during inference [15]. This improvement beyond base performance is done through slight tweaking, without re-training or fine-tuning the neural networks.\n\n## **Antibody-antigen modeling: A persistent challenge**\n\nOne particular shortcoming of the first release of the AlphaFold 2 pipeline is its lack of accuracy for predicting antibody-antigen or nanobody-antigen bound complexes [8]. The problem itself is notoriously difficult, and it comes as no surprise that the observed accuracy of AlphaFold 2 on many other tasks motivated further inquiry with respect to their performance on this specific use case. An initial benchmark showed very low success rate (~10%) in this area [8], compared to other tasks.\n\nIt has been argued that while the integration of coevolution data was as the source of AlphaFold 2’s overall performance, such data do not exist for antibody-antigen binding, which partially explains this lack of accurate results.\n\nNevertheless, a much more recent study [17] highlighted increased performance for newer versions of AlphaFold Multimer (2.2 and 2.3) compared to the initial release. Moreover, novel strategies, such as the aforementioned augmented sampling approach, have shown larger leap in success rates. Indeed, a key feature of AlphaFold 2 (and successors) is the ability to rank its own predictions using predicted accuracy metrics: in massive sampling approaches, such metrics can be used to identify conformational models of relevance [16]. Using a benchmark dataset of 37 antibody-antigen complexes (not part of the training set of AlphaFold 2), it has been reported [17] that the top-1 success rate was ~60%, which is quite close to the ~64% top-1 success rate of AlphaFold 3 (albeit on a much larger dataset [7], sampling 1,000 seeds); similar metrics were reported by other groups as well on other benchmark datasets [9]. In less than two years, the top-1 success rate has been multiplied by a factor of 6!\n\nIf we consider larger pools of predictions from the top ranked one, up to top-25, the success-rate come close to 75% percent, meaning there is at least one correct prediction amongst 25, in 3 out of 4 cases. Combining physics-based approaches with deep-learning predictions typically increase complex structure prediction success rate. In massive sampling approaches, a large amount of predictions are analysed (a few thousands at least), and in practice correct predictions have a large probability of being retrieved in the set.\n\nAntibody–antigen success rate by different AlphaFold versions/implementation. The success rate is calculated based on the percentage of cases that had at least one model among their top N predictions that met a specified level of CAPRI accuracy. Adapted from Ref. 17\n\n## **Powering up drug discovery with LensAI**\n\nAt MindWalk, we have integrated AlphaFold 2 into our LensAI platform to enhance drug discovery workflows. The platform allows users to perform protein structure predictions within an optimized environment that balances speed and accuracy. Most comparable services contain limitations such as limited sequence lengths or reduced database search (~600 GB of storage, compared to the 2.62 TB of storage for the full database), which are tradeoffs to accommodate heavy usage, with a potential drop in accuracy in some cases.\n\nAlphaFold workflows readily available in AWS HealthOmics Ready2Run.\n\nImprovements like GPU acceleration (at inference and structure relaxation levels) may be desired, especially if the input sequences are large. To further improve performance, parallelization (which is not a feature of the official DeepMind release) may be highly desired in the case of augmented sampling. Beyond standard structure prediction tasks, LensAI incorporates advanced features like automated reporting and augmented sampling to improve prediction confidence.\n\n[... 731 more lines. Use offset=750 to read more.]","tool_call_id":"call_xnvOsUFC1oa3zPuRH2kmLHVj","metadata":null,"timestamp":"2026-02-16T21:47:27.286343Z","part_kind":"tool-return"},{"tool_name":"read_file","content":"\nMoreover, LensAI integrates AlphaFold into specialized pipelines such as Epitope Mapping and Affinity Maturation (a case study has been documented and is accessible in the following [link](https://www.biostrand.ai/insight-hub/use-cases) [[22](https://www.biostrand.ai/insight-hub/use-cases)]). These pipelines exploit state-of-the-art methodologies (physics- and data-driven approaches) to accelerate discovery rates in biotherapeutic research.\n\n## **Conclusion: The future of AI-driven structural biology**\n\nThe field of structural biology witnessed groundbreaking progress within the past few years. AlphaFold’s journey from version 1 to version 3 represents a transformative leap in our ability to predict biological macromolecule structures with unprecedented accuracy. While AlphaFold 3 expands into new territories like nucleic acids and small molecules, it does not render its predecessor obsolete. Both versions offer unique strengths that can be leveraged depending on specific research needs and pave new ways toward more intricate *in silico* and *de novo* generation of biotherapeutics to be integrated within pre-clinical research workflows. As we continue to integrate these models into platforms like LensAI, we are improving our ability to predict protein structures and accelerating the entire drug discovery process—from target identification to lead optimization. The future is bright for AI-driven structural biology, and MindWalk is at the forefront of this exciting revolution.\n\n## **References**\n\n[1] <https://www.nobelprize.org/prizes/chemistry/2024/press-release/> , consulted 2024/10/09\n\n[2] Jumper, John, et al. \"Highly accurate protein structure prediction with AlphaFold.\" *Nature* 596.7873 (2021): 583-589.\n\n[3] <https://scholar.google.com/scholar?cites=6286436358625670901>, consulted 2024/10/21\n\n[4] <https://blog.biostrand.ai/explained-a-brief-look-into-alphafold-2> , consulted 2024/10/21\n\n[5] <https://blog.biostrand.ai/explained-how-to-plot-the-prediction-quality-metrics-with-alphafold2> , consulted 2024/10/21\n\n[6] <https://blog.biostrand.ai/scaling-up-structural-biology-with-alphafold2> , consulted 2024/10/21\n\n[7] Abramson, Josh, et al. \"Accurate structure prediction of biomolecular interactions with AlphaFold 3.\" *Nature* (2024): 1-3.\n\n[8] Yin, R., Feng, B. Y., Varshney, A., & Pierce, B. G. (2022). Benchmarking AlphaFold for protein complex modeling reveals accuracy determinants. Protein Science, 31(8), e4379.\n\n[9] Bernard, C., Postic, G., Ghannay, S., & Tahi, F. (2024). Has AlphaFold 3 reached its success for RNAs?. *bioRxiv*, 2024-06.\n\n[10] [Callaway, E. (2024). Who will make AlphaFold3 open source? Scientists race to crack AI model. Nature, 630(8015), 14-15.](https://www.nature.com/articles/d41586-024-01555-x)\n\n[11] [Callaway, E. (2022). After AlphaFold: protein-folding contest seeks next big breakthrough. Nature, 613: 13-14](https://www.nature.com/articles/d41586-022-04438-1)\n\n[12] [Editorial, Nature 629, 728 (2024)](https://www.nature.com/articles/d41586-024-01463-0)\n\n[13] <https://x.com/pushmeet/status/1790086453520691657> , consulted 2024/10/21\n\n[14] *Proteins: Structure, Function, and Bioinformatics*: Volume 91, Issue 12 - Special Issue: CASP15: Critical Assessment of methods for Structure Prediction, 15th round, C1-C4, 1535-1951 (2023)\n\n[15] Wallner, B. (2023). Improved multimer prediction using massive sampling with AlphaFold in CASP15. *Proteins: Structure, Function, and Bioinformatics*, 91(12), 1734-1746.\n\n[16] Raouraoua, N., Lensink, M., & Brysbaert, G. (2024). Massive sampling strategy for antibody-antigen targets in CAPRI Round 55 with MassiveFold. *Authorea Preprints*.\n\n[17] Yin, R., & Pierce, B. G. (2024). Evaluation of AlphaFold antibody–antigen modeling with implications for improving predictive accuracy. Protein Science, 33(1), e4865.\n\n[18] Hitawala, F. N., & Gray, J. J. (2024). What has AlphaFold3 learned about antibody and nanobody docking, and what remains unsolved?. *bioRxiv*, 2024-09.\n\n[19] Harmalkar, A., Lyskov, S., & Gray, J. J. (2023). Reliable protein-protein docking with AlphaFold, Rosetta, and replica-exchange. *bioRxiv*.\n\n[20] Gao, M., & Skolnick, J. (2024). Improved deep learning prediction of antigen–antibody interactions. *Proceedings of the National Academy of Sciences*, *121*(41), e2410529121.\n\n[21] Zheng, W., Wuyun, Q., Freddolino, P. L., & Zhang, Y. (2023). Integrating deep learning, threading alignments, and a multi‐MSA strategy for high‐quality protein monomer and complex structure prediction in CASP15. *Proteins: Structure, Function, and Bioinformatics*, *91*(12), 1684-1703.\n\n[22] <https://www.biostrand.ai/insight-hub/use-cases> , consulted 2024/10/21\n\n[23] Goverde, C. A., Pacesa, M., Goldbach, N., Dornfeld, L. J., Balbi, P. E., Georgeon, S., ... & Correia, B. E. (2024). Computational design of soluble and functional membrane protein analogues. *Nature*, 1-10.\n\n[24] Dauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., ... & Baker, D. (2022). Robust deep learning–based protein sequence design using ProteinMPNN. *Science*, *378*(6615), 49-56.\n\n#### **Tags**\n\n[Protein structure prediction](https://www.mindwalkai.com/blog/tag/protein-structure-prediction) [AlphaFold](https://www.mindwalkai.com/blog/tag/alphafold) [Structural biology](https://www.mindwalkai.com/blog/tag/structural-biology)\n\n#### **Sign-up**\n\n#### Subscribe to news, insights, events, blogs\n\n### Related articles\n\n#### [Minimizing ADA risk with in silico immunogenicity screening](https://www.mindwalkai.com/blog/minimizing-ada-risk-with-in-silico-immunogenicity-screening)\n\n#### [The rise of in silico epitope mapping: faster insights, near X-ray precision](https://www.mindwalkai.com/blog/the-rise-of-in-silico-epitope-mapping-faster-insights-near-x-ray-precision)\n\n#### [Drug discovery at PMWC 2025: What’s next?](https://www.mindwalkai.com/blog/drug-discovery-at-pmwc-2025)\n\n## Keep up to date\n\nSign-up for news, insights,  \nupcoming events and blogs\n\nFollow us  [Linkedin](https://www.linkedin.com/company/mindwalkcorp/)   [X](https://x.com/mindwalkcorp)   [YouTube](https://www.youtube.com/@mindwalkAI)\n\nTitle: The Isomorphic Labs Drug Design Engine unlocks a new frontier ...\nURL: https://www.isomorphiclabs.com/articles/the-isomorphic-labs-drug-design-engine-unlocks-a-new-frontier\nContent: undefined\nRaw Content: [Our Team](/our-team)[Our Tech](/our-tech)\n\n[Our Team](/our-team)[Our Tech](/our-tech)[Partnerships](/partnerships)\n\nCareers\n\n[Life at Iso](/life-at-iso)[Work with Us](/work-with-us)[Job Openings](/job-openings)\n\n[News](/news)\n\nVision\n\n# The Isomorphic Labs Drug Design Engine unlocks a new frontier beyond AlphaFold\n\nFebruary 10, 2026\n\nmin Read\n\nmin Read\n\nmin Read\n\nmin Read\n\nmin listen\n\nmin watch\n\n[Copy url](#)\n\nListen:\n\n[Spotify](#)\n\n[Apple Podcasts](#)\n\nToday, we are excited to share an update on our progress towards a new frontier of drug design. We have unlocked a new paradigm of predictive accuracy in understanding our biomolecular world, allowing us to rationally design new medicines on a computer with unprecedented understanding and precision.\n\nWe are giving a glimpse at a subset of the powerful and expansive capabilities of the Isomorphic Labs Drug Design Engine (IsoDDE), a unified computational drug-design system, progressing beyond AlphaFold 3 (AF3) in its predictive accuracy and introducing new capabilities which bridge the gap between structure prediction and real-world drug discovery.\n\nWe demonstrate that our IsoDDE more than doubles the accuracy of AlphaFold 3 on a challenging protein-ligand structure prediction generalisation benchmark, predicts small molecule binding-affinities with accuracies that exceed gold-standard physics-based methods at a fraction of the time and cost, and is able to accurately identify novel binding pockets on target proteins using only the amino acid sequence as input.\n\nIsoDDE offers a scalable foundation for AI drug design, providing the predictive fidelity required to navigate novel biological systems with unprecedented accuracy.\n\nSince our [release of AlphaFold 3](https://www.isomorphiclabs.com/articles/alphafold-3-predicts-the-structure-and-interactions-of-all-of-lifes-molecules) in 2024 together with Google DeepMind, the field of AI drug discovery has moved at an extraordinary pace. Whilst AlphaFold 3 delivered a dramatic leap in performance from previous generations of structure prediction models, a key challenge remained: understanding biomolecular structures alone was not sufficient for unlocking real-world drug discovery programs *in silico* (on a computer).\n\nProgress in rational drug design - vital for solving human disease - requires highly accurate predictive models, across an expansive range of biochemical properties and interactions, that are able to work in concert with one another. Crucially, with so much of biological and chemical space still unexplored, these models need the ability to generalise their predictive power beyond their training sets to novel, unseen systems.\n\nAs we continue to address these challenges, we are excited to introduce the Isomorphic Labs Drug Design Engine (IsoDDE), and to preview a subset of IsoDDE's capabilities below and in our technical report.\n\n[Read Our Technical Report](https://storage.googleapis.com/isomorphiclabs-website-public-artifacts/isodde_technical_report.pdf)\n\n## Structure Prediction of Truly Novel Systems\n\nAccurately predicting the structure of biomolecules and how they interact remains a crucial capability for rational drug design. Many critical downstream tasks are unlocked by being able to accurately model the small nuances in a protein’s geometry - whether understanding the impact of disease-causing mutations, or predicting which molecules will bind to a target protein.\n\nAlphaFold 3 transformed protein-ligand structure prediction at the time of its release and the freely available AlphaFold Protein Database accelerated science on a scale that was previously unimaginable. To date, it has been used by over 3 million researchers in more than 190 countries.\n\nBenchmarks have subsequently revealed that there remained a gap in accuracy for structures that were dissimilar to the examples AlphaFold 3 had been trained on. In other words, that it can struggle to generalise to unexplored regions of biomolecular space where some of the biggest challenges and opportunities in drug discovery lie.\n\nIsoDDE demonstrates a step change in the ability to generalise to protein-ligand structures that are highly dissimilar to those in its training set.\n\nOn the 'Runs N' Poses' benchmark ([Škrinjar et al. 2025](https://www.biorxiv.org/content/10.1101/2025.02.03.636309v3)) - designed specifically to test generalisation to novel pockets and ligands - IsoDDE more than doubles the accuracy of AlphaFold 3 on the most difficult systems.\n\nIn the report, we demonstrate through several examples that we can successfully model complex, out-of-distribution events such as induced fits (where a protein adapts its shape to accommodate a bound ligand) and the opening of cryptic pockets (those hidden in the absence of a bound ligand) - critical biological mechanisms - even when these systems are distant from the training sets of such models.\n\nIsoDDE is able to successfully predict the structure of a protein-protein interaction inhibitor bound to a cryptic pocket on the NKG2D homo-dimer interface (8EA6) from the lowest 0-20 similarity bin of the Runs’n’Poses test set (AlphaFold 3 fails on this example)\n\n## Opening a New Window for Complex Biologics\n\nBut small molecules (like aspirin) are only one piece of the puzzle. As therapeutic modalities expand toward complex biologics (like insulin), the ability to accurately model antibody-antigen interfaces is paramount.\n\nIsoDDE provides a step change in accuracy for this domain. It outperforms AlphaFold 3 by 2.3x and Boltz-2 by 19.8x in the high-fidelity regime (DockQ > 0.8) on a challenging, novel antibody-antigen test set.\n\nCrucially, IsoDDE shows remarkable performance on the CDR-H3 loop - the most variable and difficult part of an antibody to predict - effectively unlocking new possibilities for *de novo* antibody design.\n\n## A New Gold-Standard for Binding Affinity Prediction\n\nKnowing the 3D structure of a biochemical system is only the first step; effective drug optimisation requires knowing how strongly a molecule will bind to its target.\n\nTraditional approaches are either limited to chemical space similar to the training data or by their high computational cost and difficulty of execution (e.g., physics-based approaches). Deep-learning based methods have more recently emerged that bring new speed to this task, but still lag behind physics-based approaches for accuracy.\n\nIsoDDE surpasses all deep-learning methods by a considerable margin on three public benchmarks - FEP+ 4, OpenFE, and the recent CASP16 blind binding affinity prediction task.\n\nIn fact, remarkably, IsoDDE can surpass the performance of physics-based methods such as FEP, despite the fact that these require grounding in experimental crystal structures and IsoDDE does not.\n\nBy delivering highly accurate binding affinity predictions at speed, IsoDDE allows researchers to rapidly rank and optimise potential molecules across diverse chemical series during drug design programs.\n\n## Expanding the Ligandable Proteome\n\nThe ability to identify all of the potential pockets on a protein, in the *absence* of a known ligand, unlocks a number of unique opportunities. Whether dealing with a first-in-class drug discovery target lacking structural annotation or pursuing a novel way to modulate a well-studied protein, a general pocket identification capability can be used to reveal the full set of possible mechanisms of action to pursue for molecular design.\n\nIsoDDE exhibits the capability to identify novel, ligandable pockets even in the absence of a known ligand and far from the model’s training set. This capacity for ‘blind’ pocket identification demonstrates performance levels approaching experimental techniques like fragment-soaking which require large investments in time, significant cost and real-world experimental work. In comparison, IsoDDE runs on a computer in a matter of seconds.\n\nWe can see the power of this capability in the example of cereblon - a substrate receptor for the CRL4 E3 ligase complex - which plays a key role in tagging damaged or misfolded proteins for proteasomal degradation. For the last 15 years, it was believed that there was one principal way to drug cereblon: through the classic thalidomide-binding pocket. However, a recent study ([Dippon et al. 2026](https://www.nature.com/articles/s41586-025-09994-w)) experimentally discovered a novel binding pocket that was both allosteric (away from the traditional binding site) and cryptic (hidden in the absence of a binding ligand).\n\nIsoDDE was able to recapitulate the discovery of this pocket, predicting the location of both the known and the novel cryptic sites using only the sequence of cereblon as input, without specifying the identity of the ligands. Further, once the ligands were specified, IsoDDE was able to correctly fold them into their respective pockets in the correct orientation.\n\n## Advancing Drug Discovery\n\nIsoDDE represents a leap forward in accuracy and capability, bringing deeper understanding to the molecular machines that make up the human body, and advancing the process of designing drugs to modulate them.\n\nOur dedicated drug design teams at Isomorphic Labs are using these capabilities every day across our programs – to understand unseen structures, identify uncharacterised pockets, and create novel chemical matter in the pursuit of new medicines for patients.\n\nWe look forward to continuing to push the frontiers of *in silico* drug design and bringing our new, more powerful capabilities to bear on historically challenging drug targets.\n\n*We thank our friends at Google DeepMind for productive discussions and collaboration.*\n\nRelated\n\n© 2026 ISOMORPHIC LABS\n\n[Home](/)[Life at Iso](/life-at-iso)[Our Team](/our-team)[Work With Us](/work-with-us)[Our Tech](/our-tech)[Job Openings](/job-openings)[Partnerships](/partnerships)[News](/news)\n\nCONTACT US\n\n[Privacy Notice](/privacy-notice)[Cookie Notice](/cookie-notice)[Terms & Conditions](/terms-and-conditions)[Supplier Code of Conduct](/supplier-code-of-conduct)[Manage Cookies](#)\n\n[... 531 more lines. Use offset=950 to read more.]","tool_call_id":"call_Vlp2kNAK3kXr4CMuqmV6iHbM","metadata":null,"timestamp":"2026-02-16T21:47:27.387107Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:27.396865Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":950,\"limit\":150}","tool_call_id":"call_Bfac95CexNFF9hhhSAyRO6tG","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":30472,"cache_write_tokens":0,"cache_read_tokens":22272,"output_tokens":43,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:28.772452Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:28Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0jAuSR2e4f6DJiszCqQSwbFWGY8","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"\nTitle: AlphaFold: Five Years of Impact - Google DeepMind\nURL: https://deepmind.google/blog/alphafold-five-years-of-impact/\nContent: undefined\nRaw Content: Models\n\n## Explore our next generation AI systems\n\n[Explore models](/models/)\n\nGemini\n\n[Gemini\n\nLearn, build, and plan anything](/models/gemini/) [Nano Banana\n\nCreate and edit detailed images](/models/gemini-image/) [Gemini Audio\n\nTalk, create and control audio](/models/gemini-audio/)\n\nSpecialized models\n\n[Veo\n\nGenerate cinematic video with audio](/models/veo/) [Imagen\n\nGenerate high-quality images from text](/models/imagen/) [Lyria\n\nGenerate high fidelity music and audio](/models/lyria/) [SynthID\n\nWatermark and identify AI content](/models/synthid/)\n\nWorld models & embodied AI\n\n[Genie 3\n\nGenerate and explore interactive worlds](/models/genie/) [Gemini Robotics\n\nPerceive, reason, use tools and interact](/models/gemini-robotics/)\n\nOpen models\n\n[Gemma\n\nBuild responsible AI applications at scale](/models/gemma/)\n\nResearch\n\n## Our latest AI breakthroughs and updates from the lab\n\n[Explore research](/research/)\n\nBreakthroughs\n\n[SIMA 2\n\nAn agent that plays, reasons, and learns with you](/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/) [Genie 3\n\nGenerate and explore interactive worlds](/blog/genie-3-a-new-frontier-for-world-models/) [AlphaGo\n\nMastering the game of Go](/research/alphago/) [Gemini Robotics\n\nPerceive, reason, use tools and interact](/models/gemini-robotics/)\n\nLearn more\n\n[Evals](/research/evals/) [Publications](/research/publications/) [Responsibility](/responsibility-and-safety/)\n\nScience\n\n## Unlocking a new era of discovery with AI\n\n[Explore science](/science/)\n\nLife sciences\n\n[AlphaFold\n\nPredict protein structures with high accuracy](/science/alphafold/) [AlphaGenome\n\nDecode genetics to pinpoint diseases](/blog/alphagenome-ai-for-better-understanding-the-genome/) [AlphaMissense\n\nFind root causes of rare genetic diseases](/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/)\n\nClimate and sustainability\n\n[AlphaEarth Foundations\n\nMap our planet in unprecedented detail](/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/) [WeatherNext\n\nFast and accurate AI weather forecasting](/science/weathernext/) [Weather Lab\n\nTest our experimental weather models](https://deepmind.google.com/science/weatherlab/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)\n\nAbout\n\n## Our mission is to build AI responsibly to benefit humanity\n\n[About Google DeepMind](/about/)\n\nResponsibility\n\nEnsuring AI safety through proactive security, even against evolving threats\n\n[Learn more](/responsibility-and-safety/)\n\nNews\n\nDiscover our latest AI breakthroughs, projects, and updates\n\n[Learn more](/blog/)\n\nCareers\n\nWe’re looking for people who want to make a real, positive impact on the world\n\n[Learn more](/careers/)\n\nLearn more\n\n[Education](/education/) [National Partnerships for AI](/national-partnerships-for-ai/) [The Podcast](/the-podcast/)\n\n[Build with Gemini](https://aistudio.google.com/prompts/new_chat?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)   [Try Gemini](https://gemini.google.com/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)\n\n[Google DeepMind   DeepMind](/)\n\n[Google AI\n\nLearn about all our AI](https://ai.google/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Google DeepMind\n\nExplore the frontier of AI](https://deepmind.google?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Google Labs\n\nTry our AI experiments](https://labs.google/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Google Research\n\nExplore our research](https://research.google?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)\n\nProducts and apps\n\n[Gemini app\n\nChat with Gemini](https://gemini.google.com/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Google AI Studio\n\nBuild with our next-gen AI models](https://aistudio.google.com/prompts/new_chat?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Google Antigravity\n\nOur agentic development platform](https://antigravity.google/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)\n\n[Models](/models/) [Research](/research/) [Science](/science/) [About](/about/)\n\n[Build with Gemini](https://aistudio.google.com/prompts/new_chat?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)  [Try Gemini](https://gemini.google.com/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=)\n\n\n[... 381 more lines. Use offset=1100 to read more.]","tool_call_id":"call_Bfac95CexNFF9hhhSAyRO6tG","metadata":null,"timestamp":"2026-02-16T21:47:29.063797Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:29.065159Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":1100,\"limit\":150}","tool_call_id":"call_ycCzMy76jizhXA2ktPYjxoOT","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":31723,"cache_write_tokens":0,"cache_read_tokens":30464,"output_tokens":44,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:30.436699Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:29Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0jBxn2JWgM1NZGXZs6pteXt40l0","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"\n\nNovember 25, 2025  Science\n\n# AlphaFold: Five years of impact\n\nDemis Hassabis, John Jumper, Pushmeet Kohli and Anna Koivuniemi on behalf of the AlphaFold team\n\nSince 2020, AlphaFold has accelerated the pace of science and fueled a global wave of biological discovery — an achievement recognized with a Nobel Prize\n\nFive years ago, AlphaFold 2 solved the protein structure prediction problem, unlocking new avenues of biological research and providing our first major proof point that AI can be a powerful tool to advance science.\n\nProteins are the complex, microscopic machines that drive every process in a living cell. Composed of long, unique chains of amino acids, they precisely fold into a 3D structure that largely defines the protein’s function - making knowledge of this shape critical for drug discovery and understanding disease.\n\nIf a protein misfolds, it can lose its function and lead to disease, like Alzheimer’s and Parkinson’s. For decades, determining these structures was a monumental task, often taking a year or more of expensive, painstaking experimental work.\n\nAt the CASP 14 (Critical Assessment of protein Structure Prediction) competition in 2020, AlphaFold 2 predicted the structures of proteins based just on their amino acid sequences with astonishing accuracy - an achievement widely hailed as a solution to this 50-year-old grand challenge in biology. But the true, lasting impact of this breakthrough came when we put AlphaFold in the hands of the research community.\n\n## A global engine for discovery\n\nIn 2021, we launched the [AlphaFold Protein Database](https://alphafold.ebi.ac.uk/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) in partnership with [EMBL-EBI](https://www.ebi.ac.uk/), which was a tipping point toward AlphaFold becoming a scientific tool adopted around the world. And one year later, we released AlphaFold 2’s predictions for more than 200 million protein structures, achieving what would take hundreds of millions of years to solve experimentally.\n\nThe freely available AlphaFold Protein Database has accelerated science on a scale that was previously unimaginable. It has been used by over 3 million researchers in more than 190 countries, including over 1 million users in low- and middle-income countries. Over 30% of AlphaFold-related research is focused on better understanding disease, benefiting human welfare.\n\nThe profound scientific and societal value of this work was recognized in 2024 with the Nobel Prize in Chemistry.\n\n## Real-world transformation\n\nAlphaFold has become a standard tool for scientists tackling some of the world's most pressing issues, from conservation to heart health.\n\n### Breeding healthier and stronger honeybees\n\nScientists in Europe used AlphaFold to understand a key immunity protein in honeybees, Vitellogenin (Vg). These structural insights are now being applied to conservation efforts for endangered bee populations and guiding the development of AI-assisted breeding programs for healthier, more resilient pollinators.\n\n[Learn more](/blog/breeding-healthier-and-stronger-honeybees/)\n\n### Revealing a key protein behind heart disease\n\nAtherosclerosis, caused by “bad cholesterol” (LDL), is the leading cause of global mortality. For decades, the structure of the central protein in LDL, apolipoprotein B100 (apoB100), remained elusive. AlphaFold 2 helped finally reveal its complex, cage-like shape. This long-awaited blueprint gives pharmaceutical researchers the atomic-level detail needed to design new preventative heart therapies.\n\n[Learn more](/blog/revealing-a-key-protein-behind-heart-disease/)\n\n[Read more about AlphaFold’s impact](/science/alphafold/)\n\n## Revolutionizing Research\n\nAlphaFold is also transforming scientific research - broadening access, accelerating timelines and dramatically lowering the cost.\n\n### Expanding access\n\nTurkish undergraduate students Alper and Taner Karagöl taught themselves structural biology during the pandemic using [online AlphaFold tutorials](https://www.ebi.ac.uk/training/online/courses/alphafold/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) – with no prior training. They've now published 15 research papers.\n\n### Increasing speed of discovery\n\nCyril Zipfel, professor of Molecular & Cellular Plant Physiology at the University of Zurich and Sainsbury Lab, saw research timelines shrink drastically. They used AlphaFold alongside comparative genomics to better understand how plants perceive changes in their environment, paving the way for more resilient crops.\n\nAlphaFold has been cited in more than 35,000 papers and more than 200,000 papers incorporated elements of AlphaFold 2 in their methodology. It’s also enhancing the quality of work being produced.\n\nAn [independent analysis](https://www.innovationgrowthlab.org/resources/ai-in-science-alphafold-2) of AlphaFold 2’s impact, carried out by the Innovation Growth Lab, suggests that researchers using AlphaFold 2 see an increase of over 40% in their submission of novel experimental protein structures. Those protein structures are more likely to be dissimilar to known structures, encouraging the exploration of uncharted areas of science. Also, research linked to AlphaFold 2 is twice as likely to be cited in clinical articles, and is significantly more likely to be cited by a patent, than typical works in structural biology.\n\n## A new era of digital biology\n\nOne of the most exciting examples of AlphaFold's impact is [Isomorphic Labs](https://www.isomorphiclabs.com/) – an AI drug discovery company founded in 2021 when the breakthrough model proved to be powerful enough to be applied to rational drug design. Isomorphic Labs has since developed a unified drug design engine to dramatically change how it designs new medicines and speed up scientific discovery with an ambition to one day solve all diseases.\n\nTogether with Isomorphic Labs, we developed [AlphaFold 3](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=), which offers an unprecedented view into cells that we expect to drive a transformation of the drug discovery process and usher in an era of \"digital biology.\"\n\nThe model is designed to predict the structure and interactions of all of life's molecules — not just proteins, but DNA, RNA, and ligands (the small molecules that make up most drugs). It can also generate the joint 3D structures of entire molecular complexes, allowing a holistic view of how a potential drug molecule binds to its target protein, or how proteins interact with genetic material.\n\nThe [AlphaFold Server](https://alphafoldserver.com/welcome) is empowering non-commercial researchers globally to harness this technology, accelerating their ability to formulate and test new hypotheses. So far, it’s helped make more than 8 million folds - predictions of structures and interactions - for thousands of researchers around the world.\n\n## The future of AI for science\n\nInspired by AlphaFold, we’ve developed a new generation of models to solve problems across biology. [AlphaMissense](https://deepmind.google/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/) and [AlphaGenome](https://deepmind.google/blog/alphagenome-ai-for-better-understanding-the-genome/) use AI to assess the genetic mutations that underpin disease. Our [AlphaProteo](https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/) model can design novel, high-strength protein binders that target diverse molecules – including those associated with cancer and diabetes. These models are deepening our understanding of complex diseases and ultimately driving the development of new treatments.\n\nBiology was our first frontier, but we view AlphaFold as the template for how AI can accelerate all of science to digital speed. From [fusion](https://deepmind.google/blog/bringing-ai-to-the-next-generation-of-fusion-energy/) and [Earth sciences](https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/) to [scientific discovery](https://deepmind.google/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) as a whole, we’re pursuing the next AlphaFold-like breakthroughs. We’re excited to continue partnering with the global scientific community, empowering researchers everywhere to tackle the biggest challenges facing humanity.\n\n[Visit our AlphaFold page](/science/alphafold/) [Explore the AlphaFold Protein Structure Database](https://alphafold.ebi.ac.uk/?utm_source=deepmind.google&utm_medium=referral&utm_campaign=gdm&utm_content=) [Access the AlphaFold Server](https://alphafoldserver.com/welcome) [Listen to our podcast with John Jumper](https://www.youtube.com/watch?v=-pGs0btGmgY)\n\n### Related posts\n\n### AlphaGenome: AI for better understanding the genome\n\nScience\n\n[Learn more](/blog/alphagenome-ai-for-better-understanding-the-genome/)\n\n### AlphaProteo generates novel proteins for biology and health research\n\nScience\n\n[Learn more](/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/)\n\n### A glimpse of the next generation of AlphaFold\n\nScience\n\n[Learn more](/blog/a-glimpse-of-the-next-generation-of-alphafold/)\n\n### A catalogue of genetic mutations to help pinpoint the cause of diseases\n\nScience\n\n[Learn more](/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/)\n\n### AlphaFold reveals the structure of the protein universe\n\nScience\n\n[Learn more](/blog/alphafold-reveals-the-structure-of-the-protein-universe/)\n\n### AlphaFold: a solution to a 50-year-old grand challenge in biology\n\nScience\n\n[Learn more](/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/)\n\n \n\nTitle: How does AlphaFold 3 work?\nURL: https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/\nContent: undefined\nRaw Content: How does AlphaFold 3 work? | AlphaFold\n===============\n\n[Edit](https://www.ebi.ac.uk/node/6682)\n\n×\n\nSearch all of EMBL-EBI \n\n*   [EMBL-EBI](https://www.ebi.ac.uk/)\n*   [Search](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/#)\n*   [About us](https://www.ebi.ac.uk/about)\n*   [Training](https://www.ebi.ac.uk/training)\n*   [Research](https://www.ebi.ac.uk/research)\n*   [Services](https://www.ebi.ac.uk/services)\n*   [EMBL-EBI home](https://www.ebi.ac.uk/)\n\n*   [EMBL-EBI Training](https://www.ebi.ac.uk/training)\n*   [On-demand training](http://www.ebi.ac.uk/training/on-demand)\n*   [online_tutorial](http://www.ebi.ac.uk/training/on-demand?facets=type:online_tutorial)\n*    AlphaFold \n\n[My learning](https://www.ebi.ac.uk/training/user)\n\n[My playlists](https://www.ebi.ac.uk/training/playlists)\n\n[My account](https://www.ebi.ac.uk/training/user/account)\n\n[Contact us](https://www.ebi.ac.uk/support/training)\n\n[... 231 more lines. Use offset=1250 to read more.]","tool_call_id":"call_ycCzMy76jizhXA2ktPYjxoOT","metadata":null,"timestamp":"2026-02-16T21:47:31.090517Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:31.099857Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":1250,\"limit\":150}","tool_call_id":"call_eoUgctBq2RvXKdodxKSUaBsm","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":34266,"cache_write_tokens":0,"cache_read_tokens":31744,"output_tokens":44,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:32.651901Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:31Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0jDHYNZwf4S8eCiUWGVVjy4AAZm","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"\n[Logout](https://www.ebi.ac.uk/training/user/logout)\n\n*   [Login](https://www.ebi.ac.uk/training/user/login)\n*   [Register](https://www.ebi.ac.uk/training/user/register)\n\n[AlphaFold ---------](https://www.ebi.ac.uk/training/online/courses/alphafold)\nA practical guide\n\nCourse progress:  0% \n\n*   Open Tree\n\n*   [Course overview](https://www.ebi.ac.uk/training/online/courses/alphafold)\n*   [Search within this course](https://www.ebi.ac.uk/training/online/courses/alphafold/search/page?s=)\n*   [An introductory guide to AlphaFold’s strengths and limitations Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/)\n    *   [What are proteins and how do we know their structures?](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/what-are-proteins-and-how-do-we-know-their-structures/)\n    *   [What is the protein folding problem?](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/what-is-the-protein-folding-problem/)\n    *   [What is AlphaFold?](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/what-is-alphafold/)\n    *   [Strengths and limitations of AlphaFold 2](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/strengths-and-limitations-of-alphafold/)\n    *   [Test your knowledge](https://www.ebi.ac.uk/training/online/courses/alphafold/an-introductory-guide-to-its-strengths-and-limitations/test-your-knowledge/)\n\n*   [Validation and impact Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/validation-and-impact/)\n    *   [How have AlphaFold2’s predictions of protein structure been validated?](https://www.ebi.ac.uk/training/online/courses/alphafold/validation-and-impact/how-have-alphafolds-predictions-of-protein-structure-been-validated/)\n    *   [How accurate are AlphaFold 2 structure predictions?](https://www.ebi.ac.uk/training/online/courses/alphafold/validation-and-impact/how-accurate-are-alphafold-structure-predictions/)\n    *   [How is AlphaFold 2 used by scientists?](https://www.ebi.ac.uk/training/online/courses/alphafold/validation-and-impact/how-is-alphafold-used-by-scientists/)\n\n*   [Inputs and outputs Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/)\n    *   [AlphaFold2: A high-level overview](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/a-high-level-overview/)\n    *   [Evaluating AlphaFold2’s predicted structures using confidence scores Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/evaluating-alphafolds-predicted-structures-using-confidence-scores/)\n        *   [pLDDT: Understanding local confidence](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/evaluating-alphafolds-predicted-structures-using-confidence-scores/plddt-understanding-local-confidence/)\n        *   [PAE: A measure of global confidence in AlphaFold2 predictions](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/evaluating-alphafolds-predicted-structures-using-confidence-scores/pae-a-measure-of-global-confidence-in-alphafold-predictions/)\n        *   [Confidence scores in AlphaFold-Multimer](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/evaluating-alphafolds-predicted-structures-using-confidence-scores/confidence-scores-in-alphafold-multimer/)\n\n    *   [AlphaFold2 inputs and outputs – Recap](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/alphafold-inputs-and-outputs-recap/)\n    *   [Other tools to check quality of the predicted structures](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/other-tools-to-check-quality-of-the-predicted-structures/)\n\n*   [Accessing and predicting protein structures with AlphaFold 2 Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/)\n    *   [Choosing how to access AlphaFold2](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/choosing-how-to-access-alphafold/)\n    *   [Accessing predicted protein structures in the AlphaFold Database Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/accessing-predicted-protein-structures-in-the-alphafold-database/)\n        *   [What’s the best way to access the database?](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/accessing-predicted-protein-structures-in-the-alphafold-database/whats-the-best-way-to-access-the-database/)\n        *   [Using the AlphaFold database for analysis Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/accessing-predicted-protein-structures-in-the-alphafold-database/using-the-alphafold-database-for-analysis/)\n            *   [Navigating the structural landscape](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/accessing-predicted-protein-structures-in-the-alphafold-database/using-the-alphafold-database-for-analysis/navigating-the-structural-landscape/)\n\n    *   [Predicting protein structures with ColabFold and AlphaFold2 Colab](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/predicting-protein-structures-with-colabfold-and-alphafold-colab/)\n    *   [Predicting protein structures using the AlphaFold2 open-source code](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/predicting-protein-structures-using-the-alphafold-open-source-code/)\n    *   [Other ways to access predicted protein structures](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/other-ways-to-access-predicted-protein-structures/)\n    *   [How to cite AlphaFold](https://www.ebi.ac.uk/training/online/courses/alphafold/accessing-and-predicting-protein-structures-with-alphafold/how-to-cite-alphafold/)\n\n*   [Advanced modelling and applications of predicted protein structures Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/advanced-modeling-and-applications-of-predicted-protein-structures/)\n    *   [Customising AlphaFold2 structure predictions Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/advanced-modeling-and-applications-of-predicted-protein-structures/customising-alphafold-structure-predictions/)\n        *   [Outputs from ColabFold](https://www.ebi.ac.uk/training/online/courses/alphafold/advanced-modeling-and-applications-of-predicted-protein-structures/customising-alphafold-structure-predictions/outputs-from-colabfold/)\n\n    *   [Using AlphaFold2 predicted structures to tackle deeper questions](https://www.ebi.ac.uk/training/online/courses/alphafold/advanced-modeling-and-applications-of-predicted-protein-structures/using-alphafold-predicted-structures-to-tackle-deeper-questions/)\n    *   [Enhancements to AlphaFold2 from the wider scientific community](https://www.ebi.ac.uk/training/online/courses/alphafold/advanced-modeling-and-applications-of-predicted-protein-structures/enhancements-to-alphafold-from-the-wider-scientific-community/)\n\n*   [Classifying the effects of missense variants using AlphaMissense Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/classifying-the-effects-of-missense-variants-using-alphamissense/)\n    *   [Introducing missense variants and AlphaMissense](https://www.ebi.ac.uk/training/online/courses/alphafold/classifying-the-effects-of-missense-variants-using-alphamissense/introducing-missense-variants-and-alphamissense/)\n    *   [Understanding pathogenicity scores from AlphaMissense](https://www.ebi.ac.uk/training/online/courses/alphafold/classifying-the-effects-of-missense-variants-using-alphamissense/understanding-pathogenicity-scores-from-alphamissense/)\n    *   [AlphaMissense in the AlphaFold Database](https://www.ebi.ac.uk/training/online/courses/alphafold/classifying-the-effects-of-missense-variants-using-alphamissense/alphamissense-in-the-alphafold-database/)\n\n*   [AlphaFold 3 and AlphaFold Server Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/)\n    *   [Introducing AlphaFold 3 Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/)\n        *   [How does AlphaFold 3 work?](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)\n        *   [What AlphaFold 3 struggles with](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/what-alphafold-3-struggles-with/)\n        *   [Should I use AlphaFold 2 or AlphaFold 3?](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/should-i-use-alphafold-2-or-alphafold-3/)\n        *   [How have AlphaFold 3’s predictions been validated?](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-have-alphafold-3s-predictions-been-validated/)\n\n    *   [AlphaFold Server: Your gateway to AlphaFold 3 Open Tree](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/alphafold-server-your-gateway-to-alphafold-3/)\n        *   [A step-by-step guide to generating predictions with AlphaFold Server](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/alphafold-server-your-gateway-to-alphafold-3/a-step-by-step-guide-to-generating-predictions-with-alphafold-server/)\n        *   [Advanced features of AlphaFold Server](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/alphafold-server-your-gateway-to-alphafold-3/advanced-features-of-alphafold-server/)\n        *   [Interpreting results from AlphaFold Server](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/alphafold-server-your-gateway-to-alphafold-3/interpreting-results-from-alphafold-server/)\n\n    *   [How to assess the quality of AlphaFold 3 predictions](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/how-to-assess-the-quality-of-alphafold-3-predictions/)\n    *   [Using the AlphaFold 3 source code](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/using-the-alphafold-3-source-code/)\n\n*   [Summary](https://www.ebi.ac.uk/training/online/courses/alphafold/summary/)\n*   [Course slides](https://www.ebi.ac.uk/training/online/courses/alphafold/course-slides/)\n*   [Your feedback](https://www.ebi.ac.uk/training/online/courses/alphafold/your-feedback/)\n*   [Glossary of terms](https://www.ebi.ac.uk/training/online/courses/alphafold/glossary-of-terms/)\n*   [Acknowledgements](https://www.ebi.ac.uk/training/online/courses/alphafold/acknowledgements/)\n\n ​ \n[![Image 1: Creative Commons](https://www.ebi.ac.uk/training/online/courses/alphafold/wp-content/themes/ebi-academy/images/CC_BY.png)](https://creativecommons.org/licenses/by/4.0/)\n\nAll materials are free cultural works licensed under a Creative Commons [Attribution 4.0 International (CC BY 4.0) license](https://creativecommons.org/licenses/by/4.0/), except where further licensing details are provided.\n\nShare this page with:\n\n*   [twitter](https://twitter.com/intent/tweet?text=How%20does%20AlphaFold%203%20work?%20,%20AlphaFold&amp;url=https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)\n*   [facebook](https://www.facebook.com/sharer/sharer.php?u=https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)\n*   [linkedin](https://www.linkedin.com/shareArticle?mini=true&url=https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/&title=How%20does%20AlphaFold%203%20work?%20,%20AlphaFold&summary=How%20does%20AlphaFold%203%20work?%20,%20AlphaFold&source=EBIAcademy)\n\nHow does AlphaFold 3 work?\n==========================\n\nBeta\n\nGot questions? Feel free to ask [Ada](https://www.ebi.ac.uk/training/ada), our training AI assistant.\n\nAlphaFold 3 is built on similar general principles as AlphaFold 2 (for more on AlphaFold 2, see section [AlphaFold 2: A high-level overview](https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/a-high-level-overview/)).\n\nMany practical details remain the same, notably the use of multiple sequence alignments (MSAs) as the main input of the neural network. Furthermore, the overall architecture of AlphaFold 3 retains a familiar structure, that of AlphaFold 2, where a large trunk processes a pairwise representation of the chemical complex, which is then used by a structure module to generate explicit atomic positions.\n\nHowever, despite these foundational similarities, AlphaFold 3’s architecture has substantially improved over AlphaFold 2. The new architecture accommodates more general chemical structures and improves the data efficiency of learning.\n\n![Image 2](http://www.ebi.ac.uk/training/online/courses/alphafold/wp-content/uploads/sites/259/2025/06/AFS-Education-Figure-2-240828-r01-1024x265.png)\n\nFigure 35. The architecture of AlphaFold 3. Rectangles represent processing modules, while arrows show the flow of data. Yellow, input data; blue, abstract network activations; green, output data. The coloured spheres represent physical atom coordinates.\n\nFor instance, to reduce the amount of MSA processing, AlphaFold 3 replaces AlphaFold 2’s Evoformer with a new Pairformer module that has a smaller and simpler MSA embedding block. Some of AlphaFold 3’s advancements, notably its improved performance predictions of antigen-antibody complexes, are directly related to its reduced dependence on the MSA signal. Nevertheless, for protein structure prediction, AlphaFold 3 still makes heavy use of co-evolutionary data from the MSA.\n\nCrucially, AlphaFold 3 predicts the coordinates of individual atoms within a complex. This is distinct from AlphaFold 2, which predicted the positions of amino acid residues and their side chains. The new approach gives AlphaFold 3 the computational flexibility to tackle different types of molecules.\n\n### Tokenisation\n\nA further change in AlphaFold 3, compared to AlphaFold 2, is how the system subdivides a complex. Both divide the complex of interest into “tokens”. In AlphaFold 2, tokens directly correspond to amino acids, reflecting the system’s strict focus on protein structures. In contrast, AlphaFold 3 is designed to effectively model chemically different molecules, ranging from ions and small molecule ligands to macromolecules made of hundreds of amino acids or nucleotides. A one-token-per-atom approach for the whole structure, including proteins and nucleic acids, would offer the maximum flexibility: however, memory constraints necessitate a compromise.\n\nThus, AlphaFold 3’s tokenisation strategy balances the need for flexibility required for small molecules with computational practicality. Hence, a token may correspond to:\n\n*   One standard amino acid in the protein chain\n*   One standard nucleotide in the nucleic acid chain\n*   One atom of a ligand\n*   One atom of an ion\n*   One atom of a chemically-modified amino acid residue or nucleotide. In order to predict the structure of a chemically-modified amino acid residue or nucleotide, AlphaFold 3 will tokenise the entire residue/nucleotide as atoms. In other words, these are treated more like ligands than standard amino acids or nucleotides.\n\nTo understand this, consider a structure made of 100 amino acid residues and a ligand containing 20 atoms. AlphaFold 3 will represent this structure using 100 + 20 = 120 tokens. This is important because confidence metrics like PAE (see subsection “[Interpreting results from AlphaFold Server](https://www.ebi.ac.uk/training/online/courses/alphafold/interpreting-results-from-alphafold-server/)“) are now calculated for tokens rather than amino acids.\n\n![Image 3](http://www.ebi.ac.uk/training/online/courses/alphafold/wp-content/uploads/sites/259/2025/06/tokens.png)\n\nFigure 36. Illustrating tokenisation. Here, three amino acid residues take three tokens to model; three nucleotides take three tokens as well, and one citric acid takes 13 tokens, for 13 non-hydrogen atoms in this ligand. \n\n### Diffusion\n\nAlphaFold 3 predicts raw atom coordinates using a diffusion module, making it a “generative” ML model. Generative models create new data similar to the examples they learn from: in the case of AlphaFold 3, structures from the Protein Data Bank. This contrasts with non-generative models like AlphaFold 2, which identify patterns in the existing data.\n\nDiffusion is a standard technique in machine learning. First, a high-information dataset is distorted by varying levels of random noise; subsequently, the neural network learns to restore it to the original data. The system uses a conditioning description for each example that should be restored. For example, when generating images, a typical conditioning might be a text description of the final image. During training, the network learns to generate plausible data to fit any given conditioning by incrementally stepping from pure noise to the final output.\n\nIn AlphaFold 3, the diffusion module was trained to receive “noised” atomic coordinates and predict the correct coordinates. The conditioning information is the sequence information for the target molecule or complex.\n\nFor more details on the internal mechanics of AlphaFold 3, see the Supplementary Material to the AlphaFold 3 paper ([Abramson et al., 2024](https://doi.org/10.1038/s41586-024-07487-w)).\n\nOverview of AlphaFold 3 outputs\n-------------------------------\n\nAlphaFold 3 outputs the predicted structure of a protein or complex, returning the coordinates of all atoms in the mmCIF format. By default, AlphaFold 3 produces five predicted structures from a single seed. These are generated by sampling the diffusion process five times.\n\nAs with AlphaFold 2, AlphaFold 3 supplies multiple confidence metrics to help you critically assess its predictions:\n\n*   **Predicted LDDT (pLDDT):**predicted atomic coordinates are accompanied by pLDDT scores. These reflect AlphaFold 3’s local confidence in the prediction of the position of that particular atom.\n\n[... 81 more lines. Use offset=1400 to read more.]","tool_call_id":"call_eoUgctBq2RvXKdodxKSUaBsm","metadata":null,"timestamp":"2026-02-16T21:47:32.948345Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:32.956078Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","args":"{\"path\":\"/large_tool_results/call_HFXh8vfYZnlARISwHj6vRFUp\",\"offset\":1400,\"limit\":150}","tool_call_id":"call_fIjkXe9N8JKekZZyB3esy5HF","id":null,"provider_name":null,"provider_details":null,"part_kind":"tool-call"}],"usage":{"input_tokens":39244,"cache_write_tokens":0,"cache_read_tokens":34304,"output_tokens":44,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:34.350633Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:33Z","finish_reason":"tool_calls"},"provider_response_id":"chatcmpl-DA0jF9RuCVTOZJlh0kLp6U3BBZzqc","finish_reason":"tool_call","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"tool_name":"read_file","content":"*   **Predicted Aligned Error (PAE) scores and a PAE plot:** an indication of AlphaFold’s confidence in the packing and relative positions of domains, molecular chains such as proteins and DNA, and other entities like ligands and ions.\n*   **Predicted TM (pTM) score**: a single-value metric reflecting the accuracy of the overall predicted structure.\n*   **Interface-predicted TM (ipTM) score:**measures the accuracy of predictions of one component of the complex relative to the other components of the complex.\n*   **Per chain pTM and per-chain pair ipTM:**confidence in individual chains or pairs of chains.\n\n* * *\n\n![Image 4](https://www.ebi.ac.uk/training/online/courses/alphafold/wp-content/themes/ebi-academy/images/Checkmark.svg) Mark as complete\n\n*   [page Introducing AlphaFold 3](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/)\n*   [page What AlphaFold 3 struggles with](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/what-alphafold-3-struggles-with/)\n\n![Image 5](https://ebi.emblstatic.net/web_guidelines/EBI-Framework/v1.2/images/logos/assorted/elixir_kitemark-60px.png)\n\n#### [EMBL-EBI Training is part of the ELIXIR infrastructure](https://www.elixir-europe.org/platforms/training)\n\nThe EMBL-EBI Training Programme contributes to the ELIXIR Training Platform\n\nCongratulations!\n----------------\n\nYou have completed this tutorial.\n\nWhy not share your success on social media?\n\n*   [whatsapp](https://api.whatsapp.com/send?text=I%20have%20just%20completed%20a%20bioinformatics%20online%20tutorial%20from%20EMBL-EBI:%20AlphaFold%20-%20A%20practical%20guide%20https://www.ebi.ac.uk/training/online/courses/alphafold)\n*   [twitter](https://twitter.com/intent/tweet?text=I%20have%20just%20completed%20a%20bioinformatics%20online%20tutorial%20from%20@EBItraining:%20AlphaFold%20-%20A%20practical%20guide%20&url=https://www.ebi.ac.uk/training/online/courses/alphafold)\n*   [facebook](https://www.facebook.com/sharer/sharer.php?u=https://www.ebi.ac.uk/training/online/courses/alphafold)\n*   [linkedin](https://www.linkedin.com/sharing/share-offsite/?url=https://www.ebi.ac.uk/training/online/courses/alphafold)\n\nContinue on to the final pages of this online tutorial for recommendations on what to learn next and to tell us what you thought of this tutorial.\n\n[EMBL-EBI is the home for big data in biology.](https://www.ebi.ac.uk/about/our-impact)\n\nWe help scientists exploit complex information to make discoveries that benefit humankind.\n\n#### [Services](https://www.ebi.ac.uk/services)\n\n*   [Data resources and tools](https://www.ebi.ac.uk/services/data-resources-and-tools)\n*   [Data submission](https://www.ebi.ac.uk/submission)\n*   [Support and feedback](https://www.ebi.ac.uk/support)\n*   [Licensing](https://www.ebi.ac.uk/licencing)\n*   [Long-term data preservation](https://www.ebi.ac.uk/long-term-data-preservation)\n\n#### [Research](https://www.ebi.ac.uk/research)\n\n*   [Publications](https://www.ebi.ac.uk/research/publications)\n*   [Research groups](https://www.ebi.ac.uk/research/groups)\n*   [Postdocs](https://www.ebi.ac.uk/research/postdocs) and [PhDs](https://www.ebi.ac.uk/research/eipp)\n\n#### [Training](https://www.ebi.ac.uk/training)\n\n*   [Live training](https://www.ebi.ac.uk/training/live-events)\n*   [On-demand training](https://www.ebi.ac.uk/training/on-demand)\n*   [Support for trainers](https://www.ebi.ac.uk/training/trainer-support)\n*   [Contact organisers](https://www.ebi.ac.uk/training/contact-us)\n\n#### [Industry](https://www.ebi.ac.uk/industry)\n\n*   [Members Area](https://www.ebi.ac.uk/industry/private/members-area/)\n*   [Contact Industry team](https://www.ebi.ac.uk/industry/contact-us)\n\n#### [About](https://www.ebi.ac.uk/about)\n\n*   [FAQ](https://www.ebi.ac.uk/about/faq)\n*   [Contact us](https://www.ebi.ac.uk/about/contact)\n*   [Events](https://www.ebi.ac.uk/about/events)\n*   [Jobs](https://www.ebi.ac.uk/about/jobs)\n*   [News](https://www.ebi.ac.uk/about/news)\n*   [People and groups](https://www.ebi.ac.uk/about/people)\n*   [Intranet for staff](https://intranet.ebi.ac.uk/)\n\n[EMBL-EBI, Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.](https://www.google.co.uk/maps/place/Hinxton,+Saffron+Walden+CB10+1SD/@52.0815334,0.1891518,17z/data=!3m1!4b1!4m5!3m4!1s0x47d87ccbfbd2538b:0x7bbdb4cde2779ff3!8m2!3d52.0800838!4d0.186415)[Tel: +44 (0)1223 49 44 44](tel:00441223494444)[Full contact details](https://www.ebi.ac.uk/about/contact)\n\nCopyright © EMBL 2026  EMBL-EBI is part of the [European Molecular Biology Laboratory](https://www.embl.org/)[Terms of use](https://www.ebi.ac.uk/about/terms-of-use)\n\n[Edit](https://www.ebi.ac.uk/node/106902)\n\nThis website requires cookies, and the limited processing of your personal data in order to function. By using the site you are agreeing to this as outlined in our [Privacy Notice](https://www.ebi.ac.uk/data-protection/privacy-notice/embl-ebi-public-website) and [Terms of Use](https://www.ebi.ac.uk/about/terms-of-use).\n\n[I agree, dismiss this banner](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)","tool_call_id":"call_fIjkXe9N8JKekZZyB3esy5HF","metadata":null,"timestamp":"2026-02-16T21:47:34.606560Z","part_kind":"tool-return"}],"timestamp":"2026-02-16T21:47:34.609717Z","instructions":"You are a Deep Agent, an AI assistant that helps users accomplish tasks using tools. You respond with text and tool calls.\n\n## Core Behavior\n\n- Be concise and direct. Don't over-explain unless asked.\n- NEVER add unnecessary preamble (\"Sure!\", \"Great question!\", \"I'll now...\").\n- Don't say \"I'll now do X\" — just do it.\n- If the request is ambiguous, ask questions before acting.\n- Prioritize accuracy over validating the user's beliefs.\n\n## Workflow\n\nWhen the user asks you to do something:\n\n1. **Understand first** — read relevant files, check existing patterns. Gather enough context to start, then iterate.\n2. **Act** — implement the solution. Work quickly but accurately.\n3. **Verify** — check your work against what was asked. Your first attempt is rarely perfect — iterate.\n\nKeep working until the task is fully complete. Don't stop partway and explain what you would do — just do it. Only yield back to the user when the task is done or you're genuinely blocked.\n\n## Tool Usage\n\n- Use specialized tools over shell equivalents when available (e.g., `read_file` over `cat`, `edit_file` over `sed`, `glob` over `find`).\n- When performing multiple independent operations, make all tool calls in a single response — don't make sequential calls when parallel is possible.\n- Read files before editing them — understand existing content before making changes.\n- Mimic existing code style, naming conventions, and patterns.\n\n## File Reading\n\nWhen reading multiple files or exploring large files, use pagination:\n- Start with `read_file(path, limit=100)` to scan structure.\n- Read targeted sections with offset/limit.\n- Only read full files when necessary for editing.\n\n## Subagent Delegation\n\n- Delegate specialized or independent subtasks to subagents to work in parallel.\n- Be specific in task descriptions — subagents don't have your full context.\n- Synthesize subagent results before presenting to the user.\n\n## Error Handling\n\n- If something fails repeatedly, stop and analyze *why* — don't keep retrying the same approach.\n- If blocked, explain what's wrong and ask for guidance.\n- Consider alternative approaches before giving up.\n\n## Progress Updates\n\nFor longer tasks, provide brief progress updates — a concise sentence recapping what you've done and what's next.\n\n\nYou are DeepResearch, an autonomous research agent with Excalidraw diagram capabilities.\n\n## DECISION RULES — read these FIRST, follow them ALWAYS\n\nBefore responding to ANY user message, classify it:\n\n| Request type | What to do | What NOT to do |\n|---|---|---|\n| **Greeting / simple chat** | Reply directly in 1-2 sentences | Do NOT create agents, search, or use tools |\n| **User shares personal info** (name, preferences, anything about themselves) | FIRST call `edit_file` to append it to `/workspace/MEMORY.md`, THEN reply | Do NOT just acknowledge verbally — you WILL forget it |\n| **\"Remember this\" / \"save this\" / \"note that\"** | FIRST call `edit_file` to write to `/workspace/MEMORY.md`, THEN confirm | NEVER say \"I'll remember\" without writing to the file |\n| **Simple question** (you know the answer) | Reply directly with your knowledge | Do NOT create agents or use planner |\n| **Simple file operation** (\"create X file\") | Use write_file/read_file directly | Do NOT create agents |\n| **Draw / visualize / diagram request** | Call Excalidraw MCP tools YOURSELF | Do NOT create agents, do NOT delegate, do NOT write files first |\n| **Complex research** (multi-source, comparative) | Use planner, then web search tools | This is the ONLY case for subagents |\n\n### MEMORY RULE — YOUR MEMORY RESETS EVERY SESSION\n\nYou have NO long-term memory. When this conversation ends, you forget EVERYTHING. The file `/workspace/MEMORY.md` is your ONLY way to persist information across sessions.\n\n**WHEN TO WRITE TO MEMORY.md** — if the user's message contains ANY of these, call `edit_file` on `/workspace/MEMORY.md` BEFORE responding:\n- Their name, age, location, job, company, team (\"mam na imie Kacper\", \"I'm a developer at X\")\n- Preferences (\"I prefer dark mode\", \"use Polish when responding\", \"call me K\")\n- Project context (\"we're building a SaaS app\", \"the stack is Next.js + FastAPI\")\n- Explicit \"remember\" / \"save\" / \"note\" / \"zapamiętaj\" / \"zapisz\"\n- Any fact about themselves or their work they'd expect you to know next time\n\n**HOW** — append to the right section:\n```\nedit_file(\"/workspace/MEMORY.md\", \"## User Preferences\", \"## User Preferences\\n- Name: Kacper\")\n```\n\n**NEVER** just reply \"Got it, I'll remember!\" without actually calling `edit_file`. If you don't write it to the file, it's gone forever.\n\n### DRAWING RULE — THIS IS THE #1 RULE\n\nWhen the user asks to draw, visualize, diagram, or sketch ANYTHING:\n1. Call `excalidraw_read_diagram_guide` to learn element format and available tools\n2. Create elements using `excalidraw_batch_create_elements` (preferred) or `excalidraw_create_element`\n3. Use `excalidraw_align_elements` and `excalidraw_distribute_elements` for clean layout\n4. Call `excalidraw_describe_scene` to verify the result\n5. Describe what you drew in plain text\n\nCRITICAL — tools to NEVER use:\n- Do NOT use `excalidraw_create_from_mermaid` — it does NOT render on the canvas. ALWAYS use `excalidraw_batch_create_elements` with proper element JSON instead.\n- Do NOT call `excalidraw_export_to_excalidraw_url` or `excalidraw_export_scene`\n- Do NOT call `create_agent()` or `task()` — subagents CANNOT access Excalidraw\n- Do NOT call `load_skill()` first — just draw directly\n- Do NOT write intermediate files (Mermaid, SVG, markdown descriptions)\n\nWHY: Excalidraw MCP tools are only available to YOU (the main agent). They are NOT passed to subagents. Any subagent you create will NOT have Excalidraw tools and WILL fail. The `create_from_mermaid` tool is broken and produces invisible results — never use it.\n\n### SUBAGENT RULE — USE SPARINGLY\n\nSubagents (`create_agent`, `task`) are ONLY for:\n- Complex multi-source research with 3+ independent sub-topics\n- Code review delegation to 'code-reviewer'\n- Tasks that genuinely need parallel web research\n\nDo NOT create subagents for:\n- Drawing/diagrams (they lack Excalidraw)\n- Simple questions you can answer yourself\n- Single-topic explanations\n- File operations\n- Anything you can do in 1-3 tool calls\n\n## Research Process (for complex research tasks only)\n\n1. **PLAN**: Use planner subagent for complex, multi-step research\n2. **SEARCH**: Use web search (Tavily, Jina) to find sources\n3. **COLLECT**: Save findings to `/workspace/notes/` as markdown\n4. **SYNTHESIZE**: Write report to `/workspace/report.md` with citations [1][2]\n5. **REVIEW**: Verify completeness and citations\n\n## Report Format\n\n```markdown\n# [Title]\n## Executive Summary\n## 1. [Section] — with inline citations [1][2]\n## Conclusions\n## References\n[1] Author, \"Title\", URL, Accessed: YYYY-MM-DD\n```\n\n## Source Guidelines\n\n- Prefer primary sources, check dates, cross-reference claims\n- Hierarchy: Academic papers > Official docs > News > Blogs > Forums\n\n## Parallel Research (complex research only)\n\nFor 3+ independent sub-topics, dispatch async tasks in parallel:\n- `task(description=\"...\", subagent_type=\"general-purpose\", mode=\"async\")`\n- Monitor with `check_task(task_id)` and `list_active_tasks()`\n- Synthesize all results yourself\n\n## Important Rules\n\n1. **Be proportionate** — match effort to task complexity\n2. **Cite sources** in research reports\n3. **Save notes** to `/workspace/notes/` incrementally\n4. **Write reports** to `/workspace/report.md`\n5. **Draw diagrams yourself** — never delegate to subagents\n\n\n## Available Tools\n\n- **Web Search**: Tavily, Brave Search, Jina URL reader, Firecrawl\n- **Browser Automation**: Playwright MCP (navigate, screenshot, click, fill)\n- **File Operations**: read_file, write_file, edit_file, glob, grep\n- **Code Execution**: `execute` — Docker sandbox with Python 3.12, pandas, numpy, matplotlib\n- **Diagrams**: Excalidraw MCP — `excalidraw_read_diagram_guide`, `excalidraw_batch_create_elements`, `excalidraw_create_element` (ONLY you have these — NEVER use `excalidraw_create_from_mermaid`)\n- **Code Review**: Delegate to 'code-reviewer' subagent\n- **Subagents**: task(), create_agent() — for complex RESEARCH only, never for diagrams\n- **Skills**: load_skill() for domain knowledge\n- **Plan Mode**: 'planner' subagent for complex multi-step research\n\n## Shell Commands\n\nYou have `execute` for shell commands. It may need user approval — just call it.\n\n## Error Handling\n\nFix errors yourself: install missing modules, fix paths, retry. Don't ask permission.\n\n## Persistent Memory — CRITICAL\n\nYour memory resets every session. `/workspace/MEMORY.md` is the ONLY way to persist facts.\n- When the user tells you ANYTHING about themselves (name, preferences, project, team, etc.) — call `edit_file` on `/workspace/MEMORY.md` IMMEDIATELY, before replying.\n- This includes casual statements like \"my name is X\", \"I work at Y\", \"I prefer Z\".\n- If you don't call `edit_file`, the information is lost forever. Never just say \"I'll remember\".\n\n## File Locations\n\n- Uploads: /uploads/\n- Workspace: /workspace/\n- Memory: /workspace/MEMORY.md\n\n\n## Task Management\n\nYou have access to todo tools to track your tasks:\n- `read_todos` - View current tasks with their IDs and statuses\n- `write_todos` - Replace the entire todo list\n- `add_todo` - Add a single new task\n- `update_todo_status` - Change a task's status by ID\n- `remove_todo` - Delete a task by ID\n\nWhen working on tasks:\n1. Break down complex tasks into smaller steps\n2. Mark exactly one task as in_progress at a time\n3. Mark tasks as completed immediately after finishing\n\n\n\n## Console Tools\n\nYou have access to console tools for file operations and command execution:\n\n### File Operations\n- `ls`: List files in a directory\n- `read_file`: Read file content with line numbers\n- `write_file`: Create or overwrite a file\n- `edit_file`: Replace strings in a file\n- `glob`: Find files matching a pattern\n- `grep`: Search for patterns in files\n\n### Shell Execution\n- `execute`: Run shell commands (if enabled)\n\n### Best Practices\n- Always read a file before editing it\n- Use edit_file for small changes, write_file for complete rewrites\n- Use glob to find files before operating on them\n- Be careful with destructive shell commands\n\n\n## Available Subagents\n\nYou can delegate tasks to the following specialized subagents:\n\n### code-reviewer\nReviews Python code for quality, security, and best practices. Delegate code review tasks to this subagent.\n\n### planner\nPlans implementation of complex tasks. Analyzes code, asks clarifying questions, and creates detailed step-by-step implementation plans. Use for tasks that need architectural decisions, multi-file changes, or when the user says 'use plan mode'. Do NOT use for simple tasks.\n\n### general-purpose\nA general-purpose agent for a wide variety of tasks.\nUse this when no specialized subagent matches the task requirements.\nCapable of research, analysis, writing, and problem-solving.\n\n## Subagent Execution Modes\n\nYou can delegate tasks to subagents in two modes:\n\n### Sync Mode (Default)\n- Use for simple, quick tasks\n- Use when you need the result immediately\n- Use when the task requires back-and-forth communication\n- The task runs and you wait for the result\n\n### Async Mode (Background)\n- Use for complex, long-running tasks\n- Use when you can continue with other work while waiting\n- Use for tasks that can run independently\n- Returns a task handle immediately - check status later","kind":"request","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null},{"parts":[{"content":"# Latest Advances in Protein Folding Since AlphaFold 3\n\n## Executive Summary\n\nSince the release of AlphaFold 3 in 2024, the field of protein folding has experienced substantial advances driven by new AI models, deeper integration with drug discovery, and innovations in generative protein design. Recent breakthroughs include improved modeling of complex biomolecular structures (beyond proteins), enhanced predictive accuracy in protein-ligand and protein-protein interactions, and the emergence of novel AI-driven drug design engines that surpass AlphaFold 3 in specific benchmarks.\n\n---\n\n## 1. Landscape After AlphaFold 3\n\n**AlphaFold 3 Capabilities and Impact**\n- AlphaFold 3 significantly expanded the prediction scope beyond proteins to include nucleic acids (RNA/DNA), small molecules, ligands, ions, and full molecular complexes, thanks to architectural improvements like the \"Pairformer\" module and atom-centric modeling. This flexibility revolutionizes drug discovery and the design of new molecules [1][2][3].\n- AlphaFold 3 predicts atom-level coordinates via a generative diffusion module, enabling it to model static structures of complex systems, including antibody-antigen and protein-ligand assemblies [2][3].\n- The model made structure prediction more accessible with public servers, benefiting millions of researchers and accelerating science globally [3][6].\n\n**Continued Value of AlphaFold 2**\n- AlphaFold 2 remains relevant: recent community innovation (massive sampling, drop-out at inference) boosts its performance (especially for multimers and antibody-antigen complexes) and it serves in integrated drug-design pipelines alongside newer models [2].\n\n---\n\n## 2. Emerging Solutions Beyond AlphaFold 3\n\n**AI-driven Models and Generative Design**\n- Numerous AI solutions now compete or build upon AlphaFold’s foundation, including EvoBind, ESMFold, RoseTTAFold All-Atom, and RFDiffusion. These offer unique capabilities in de novo protein design, molecular binding, and complex assemblies, enabling tasks such as creating new enzymes, antibodies, and protein drugs [1][2][5].\n- Large \"protein language models\"—analogous to LLMs for text—can generate novel protein sequences with desired functions. These models are easier to tune and democratize custom protein design [5].\n\n**Isomorphic Labs Drug Design Engine (IsoDDE)**\n- Developed by Isomorphic Labs, IsoDDE is a next-generation computational engine for drug design. It:\n  - More than doubles AlphaFold 3’s accuracy in generalizing to novel protein–ligand structures (tested on challenging benchmarks).\n  - Greatly outperforms AlphaFold 3 for predicting antibody–antigen interfaces and small-molecule binding affinities—surpassing even physics-based approaches for binding.\n  - Can identify novel “cryptic” binding pockets and tackle induced fit scenarios, which are vital for rational drug discovery [4].\n- IsoDDE exemplifies the trend: from just folding prediction to actionable insights that directly accelerate lead optimization and design of new therapeutics, even in “unseen” regions of chemical space [4].\n\n**Practical and Structural Trends**\n- Integrating AI-based structure prediction with molecular dynamics and physics-based simulations yields dynamic insights, not just static folds [2].\n- Community workflow improvements (cloud access, GPU acceleration, parallel sampling) further speed and democratize protein engineering [2].\n- AI–driven approaches extend to non-protein molecules and entire molecular complexes, including DNA/RNA, which is transformative for synthetic biology and multi-modal biological research [3][5].\n\n---\n\n## 3. Limitations and Ongoing Research\n\n- **Shortcomings of AlphaFold 3:** struggles with chiral molecules, may generate atomic “clashes” in large proteins, and fundamentally produces static (not dynamic) predictions—so downstream physics simulations remain necessary [2][3].\n- **Benchmarking:** While proprietary and open third-party models are pushing the boundary, peer-reviewed benchmarks and fully open implementations are still catching up [2][4].\n- **Generalization:** Even state-of-the-art models may struggle to generalize to structures highly dissimilar from their training set—a challenge the new IsoDDE system specifically addresses [4].\n\n---\n\n## Conclusions\n\n- Since AlphaFold 3, protein folding research has shifted from solving static protein shapes to deeply integrating AI for actionable, systems-level molecular design and drug discovery.\n- New AI engines such as IsoDDE, generative protein design tools, and protein language models are powering breakthroughs in rational drug design and synthetic biology.\n- The future will likely bring even tighter integration of generative models, physics-based validation, and accessible platforms, democratizing molecular discovery and enabling faster cycles from idea to therapeutic candidate.\n\n## References\n\n[1] Yuktika Malhotra et al., \"Advancements in protein structure prediction: A comparative overview of AlphaFold and its derivatives,\" Comput Biol Med, 2025. [PubMed](https://pubmed.ncbi.nlm.nih.gov/39970826/)  \n[2] S. Lemal, “Beyond AlphaFold 2: The next frontier in macromolecular structure prediction,” MindWalkAI, 2025. [Read](https://www.mindwalkai.com/blog/beyond-alphafold-2)  \n[3] “How does AlphaFold 3 work?” EMBL-EBI, 2025. [Read](https://www.ebi.ac.uk/training/online/courses/alphafold/alphafold-3-and-alphafold-server/introducing-alphafold-3/how-does-alphafold-3-work/)  \n[4] Isomorphic Labs, “IsoDDE: The Isomorphic Labs Drug Design Engine unlocks a new frontier,” 2026. [Read](https://www.isomorphiclabs.com/articles/the-isomorphic-labs-drug-design-engine-unlocks-a-new-frontier)  \n[5] SynBioBeta, “Folding the Future: How AI is Reshaping Protein Engineering,” 2025. [Read](https://www.synbiobeta.com/read/folding-the-future-how-ai-is-reshaping-protein-engineering)  \n[6] DeepMind, \"AlphaFold: Five years of impact,\" 2025. [Read](https://deepmind.google/blog/alphafold-five-years-of-impact)\n\nLet me know if you need details on any subfield or want insights on practical applications!","id":null,"provider_name":null,"provider_details":null,"part_kind":"text"}],"usage":{"input_tokens":40826,"cache_write_tokens":0,"cache_read_tokens":39168,"output_tokens":1315,"input_audio_tokens":0,"cache_audio_read_tokens":0,"output_audio_tokens":0,"details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0}},"model_name":"gpt-4.1-2025-04-14","timestamp":"2026-02-16T21:47:36.026771Z","kind":"response","provider_name":"openai","provider_url":"https://api.openai.com/v1/","provider_details":{"timestamp":"2026-02-16T21:47:35Z","finish_reason":"stop"},"provider_response_id":"chatcmpl-DA0jHWlfnzR6pdsHChrZLWonSxTQ7","finish_reason":"stop","run_id":"b055bc97-28b7-494a-a513-9dfaeaaf228c","metadata":null}]