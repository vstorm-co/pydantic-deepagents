+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   src/_pytest/config/__init__.py

no changes added to commit (use "git add" and/or "git commit -a")
+ git show
commit aa48a850a1020a0206661c39700a69b57f9d96ac
Author: SWE-bench <setup@swebench.config>
Date:   Wed Sep 10 15:11:11 2025 +0000

    SWE-bench
+ git -c core.fileMode=false diff 73c5b7f4b11a81e971f7d1bb18072e06a87060f4
diff --git a/src/_pytest/config/__init__.py b/src/_pytest/config/__init__.py
index 3a0eca546..bc6ca3f32 100644
--- a/src/_pytest/config/__init__.py
+++ b/src/_pytest/config/__init__.py
@@ -432,12 +432,14 @@ class PytestPluginManager(PluginManager):
         raise KeyError(name)
 
     def _importconftest(self, conftestpath):
-        # Use realpath to avoid loading the same conftest twice
+        # Use a resolved Path object as key to avoid loading the same conftest twice
         # with build systems that create build directories containing
         # symlinks to actual files.
-        conftestpath = unique_path(conftestpath)
+        # Using Path().resolve() is better than py.path.realpath because
+        # it resolves to the correct path/drive in case-insensitive file systems (#5792)
+        key = Path(str(conftestpath)).resolve()
         try:
-            return self._conftestpath2mod[conftestpath]
+            return self._conftestpath2mod[key]
         except KeyError:
             pkgpath = conftestpath.pypkgpath()
             if pkgpath is None:
@@ -454,21 +456,7 @@ class PytestPluginManager(PluginManager):
                 raise ConftestImportFailure(conftestpath, sys.exc_info())
 
             self._conftest_plugins.add(mod)
-            self._conftestpath2mod[conftestpath] = mod
-            dirpath = conftestpath.dirpath()
-            if dirpath in self._dirpath2confmods:
-                for path, mods in self._dirpath2confmods.items():
-                    if path and path.relto(dirpath) or path == dirpath:
-                        assert mod not in mods
-                        mods.append(mod)
-            self.trace("loaded conftestmodule %r" % (mod))
-            self.consider_conftest(mod)
-            return mod
-
-    #
-    # API for bootstrapping plugin loading
-    #
-    #
+            self._conftestpath2mod[key] = mod
 
     def consider_preparse(self, args):
         i = 0
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -e .
Obtaining file:///testbed
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: py>=1.5.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (1.11.0)
Requirement already satisfied: packaging in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (23.1)
Requirement already satisfied: attrs>=17.4.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (23.1.0)
Requirement already satisfied: more-itertools>=4.0.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (10.1.0)
Requirement already satisfied: atomicwrites>=1.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (1.4.1)
Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (0.13.1)
Requirement already satisfied: wcwidth in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.1.3.dev15+gaa48a850a.d20260222) (0.2.6)
Building wheels for collected packages: pytest
  Building editable for pytest (pyproject.toml): started
  Building editable for pytest (pyproject.toml): finished with status 'done'
  Created wheel for pytest: filename=pytest-5.1.3.dev15+gaa48a850a.d20260222-0.editable-py3-none-any.whl size=4963 sha256=719375cb66766e07a951cf4efd6dff2719e85794a2765a41f7d65d06ed36f48d
  Stored in directory: /tmp/pip-ephem-wheel-cache-8cdeunr8/wheels/7d/66/67/70d1ee2124ccf21d601c352e25cdca10f611f7c8b3f9ffb9e4
Successfully built pytest
Installing collected packages: pytest
  Attempting uninstall: pytest
    Found existing installation: pytest 5.1.3.dev14+g73c5b7f4b
    Uninstalling pytest-5.1.3.dev14+g73c5b7f4b:
      Successfully uninstalled pytest-5.1.3.dev14+g73c5b7f4b
Successfully installed pytest-5.1.3.dev15+gaa48a850a.d20260222
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
+ git checkout 73c5b7f4b11a81e971f7d1bb18072e06a87060f4 testing/test_conftest.py
Updated 0 paths from 6869f107f
+ git apply -v -
Checking patch testing/test_conftest.py...
Applied patch testing/test_conftest.py cleanly.
+ : '>>>>> Start Test Output'
+ pytest -rA testing/test_conftest.py
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /testbed, inifile: tox.ini
collected 54 items

testing/test_conftest.py F..FFF..FF...FFF....FFFs..FF..FFFFFFFFFFFFFFFFF [ 87%]
FFF..F.                                                                  [100%]

=================================== FAILURES ===================================
____________ TestConftestValueAccessGlobal.test_basic_init[global] _____________

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe9ea970>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir0')

    def test_basic_init(self, basedir):
        conftest = PytestPluginManager()
        p = basedir.join("adir")
>       assert conftest._rget_with_confmod("a", p)[1] == 1

testing/test_conftest.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe9ea7f0>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir0/adir')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
_______ TestConftestValueAccessGlobal.test_value_access_by_path[global] ________

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe9c7070>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir0')

    def test_value_access_by_path(self, basedir):
        conftest = ConftestWithSetinitial(basedir)
        adir = basedir.join("adir")
>       assert conftest._rget_with_confmod("a", adir)[1] == 1

testing/test_conftest.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe51c790>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir0/adir')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
_____ TestConftestValueAccessGlobal.test_value_access_with_confmod[global] _____

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe99c430>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir0')

    def test_value_access_with_confmod(self, basedir):
        startdir = basedir.join("adir", "b")
        startdir.ensure("xx", dir=True)
        conftest = ConftestWithSetinitial(startdir)
>       mod, value = conftest._rget_with_confmod("a", startdir)

testing/test_conftest.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe9c0e80>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir0/adir/b')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
___________ TestConftestValueAccessGlobal.test_basic_init[inpackage] ___________

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe9e5640>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir1')

    def test_basic_init(self, basedir):
        conftest = PytestPluginManager()
        p = basedir.join("adir")
>       assert conftest._rget_with_confmod("a", p)[1] == 1

testing/test_conftest.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe9e5040>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir1/adir')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
______ TestConftestValueAccessGlobal.test_value_access_by_path[inpackage] ______

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe999670>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir1')

    def test_value_access_by_path(self, basedir):
        conftest = ConftestWithSetinitial(basedir)
        adir = basedir.join("adir")
>       assert conftest._rget_with_confmod("a", adir)[1] == 1

testing/test_conftest.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe991f10>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir1/adir')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
___ TestConftestValueAccessGlobal.test_value_access_with_confmod[inpackage] ____

self = <test_conftest.TestConftestValueAccessGlobal object at 0x7ffffe85ccd0>
basedir = local('/tmp/pytest-of-root/pytest-0/basedir1')

    def test_value_access_with_confmod(self, basedir):
        startdir = basedir.join("adir", "b")
        startdir.ensure("xx", dir=True)
        conftest = ConftestWithSetinitial(startdir)
>       mod, value = conftest._rget_with_confmod("a", startdir)

testing/test_conftest.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7ffffe85cc70>, name = 'a'
path = local('/tmp/pytest-of-root/pytest-0/basedir1/adir/b')

    def _rget_with_confmod(self, name, path):
        modules = self._getconftestmodules(path)
        for mod in reversed(modules):
            try:
                return mod, getattr(mod, name)
            except AttributeError:
                continue
>       raise KeyError(name)
E       KeyError: 'a'

src/_pytest/config/__init__.py:432: KeyError
_________________________ test_conftest_global_import __________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_global_import0')>

    def test_conftest_global_import(testdir):
        testdir.makeconftest("x=3")
        p = testdir.makepyfile(
            """
            import py, pytest
            from _pytest.config import PytestPluginManager
            conf = PytestPluginManager()
            mod = conf._importconftest(py.path.local("conftest.py"))
            assert mod.x == 3
            import conftest
            assert conftest is mod, (conftest, mod)
            subconf = py.path.local().ensure("sub", "conftest.py")
            subconf.write("y=4")
            mod2 = conf._importconftest(subconf)
            assert mod != mod2
            assert mod2.y == 4
            import conftest
            assert conftest is mod2, (conftest, mod)
        """
        )
        res = testdir.runpython(p)
>       assert res.ret == 0
E       assert 1 == 0
E        +  where 1 = <RunResult ret=1 len(stdout.lines)=0 len(stderr.lines)=4 duration=0.29s>.ret

/testbed/testing/test_conftest.py:130: AssertionError
----------------------------- Captured stdout call -----------------------------
running: /opt/miniconda3/envs/testbed/bin/python /tmp/pytest-of-root/pytest-0/test_conftest_global_import0/test_conftest_global_import.py
     in: /tmp/pytest-of-root/pytest-0/test_conftest_global_import0
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/tmp/pytest-of-root/pytest-0/test_conftest_global_import0/test_conftest_global_import.py", line 5, in <module>
    assert mod.x == 3
AttributeError: 'NoneType' object has no attribute 'x'
_____________________________ test_conftestcutdir ______________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftestcutdir0')>

    def test_conftestcutdir(testdir):
        conf = testdir.makeconftest("")
        p = testdir.mkdir("x")
        conftest = PytestPluginManager()
        conftest_setinitial(conftest, [testdir.tmpdir], confcutdir=p)
        values = conftest._getconftestmodules(p)
        assert len(values) == 0
        values = conftest._getconftestmodules(conf.dirpath())
        assert len(values) == 0
        assert conf not in conftest._conftestpath2mod
        # but we can still import a conftest directly
        conftest._importconftest(conf)
        values = conftest._getconftestmodules(conf.dirpath())
>       assert values[0].__file__.startswith(str(conf))
E       IndexError: list index out of range

/testbed/testing/test_conftest.py:146: IndexError
____________________ test_conftestcutdir_inplace_considered ____________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftestcutdir_inplace_considered0')>

    def test_conftestcutdir_inplace_considered(testdir):
        conf = testdir.makeconftest("")
        conftest = PytestPluginManager()
        conftest_setinitial(conftest, [conf.dirpath()], confcutdir=conf.dirpath())
        values = conftest._getconftestmodules(conf.dirpath())
        assert len(values) == 1
>       assert values[0].__file__.startswith(str(conf))
E       AttributeError: 'NoneType' object has no attribute '__file__'

/testbed/testing/test_conftest.py:159: AttributeError
___________________________ test_conftest_confcutdir ___________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_confcutdir0')>

    def test_conftest_confcutdir(testdir):
        testdir.makeconftest("assert 0")
        x = testdir.mkdir("x")
        x.join("conftest.py").write(
            textwrap.dedent(
                """\
                def pytest_addoption(parser):
                    parser.addoption("--xyz", action="store_true")
                """
            )
        )
        result = testdir.runpytest("-h", "--confcutdir=%s" % x, x)
>       result.stdout.fnmatch_lines(["*--xyz*"])
E       Failed: nomatch: '*--xyz*'
E           and: 'usage: pytest [options] [file_or_dir] [file_or_dir] [...]'
E           and: ''
E           and: 'positional arguments:'
E           and: '  file_or_dir'
E           and: ''
E           and: 'general:'
E           and: '  -k EXPRESSION         only run tests which match the given substring'
E           and: '                        expression. An expression is a python evaluatable'
E           and: '                        expression where all names are substring-matched'
E           and: '                        against test names and their parent classes. Example:'
E           and: "                        -k 'test_method or test_other' matches all test"
E           and: '                        functions and classes whose name contains'
E           and: "                        'test_method' or 'test_other', while -k 'not"
E           and: "                        test_method' matches those that don't contain"
E           and: "                        'test_method' in their names. -k 'not test_method and"
E           and: "                        not test_other' will eliminate the matches."
E           and: '                        Additionally keywords are matched to classes and'
E           and: '                        functions containing extra names in their'
E           and: "                        'extra_keyword_matches' set, as well as functions"
E           and: '                        which have names assigned directly to them.'
E           and: '  -m MARKEXPR           only run tests matching given mark expression.'
E           and: "                        example: -m 'mark1 and not mark2'."
E           and: '  --markers             show markers (builtin, plugin and per-project ones).'
E           and: '  -x, --exitfirst       exit instantly on first error or failed test.'
E           and: '  --maxfail=num         exit after first num failures or errors.'
E           and: '  --strict-markers, --strict'
E           and: '                        markers not registered in the `markers` section of the'
E           and: '                        configuration file raise errors.'
E           and: '  -c file               load configuration from `file` instead of trying to'
E           and: '                        locate one of the implicit configuration files.'
E           and: '  --continue-on-collection-errors'
E           and: '                        Force test execution even if collection errors occur.'
E           and: '  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:'
E           and: "                        'root_dir', './root_dir', 'root_dir/another_dir/';"
E           and: "                        absolute path: '/home/user/root_dir'; path with"
E           and: "                        variables: '$HOME/root_dir'."
E           and: '  --fixtures, --funcargs'
E           and: '                        show available fixtures, sorted by plugin appearance'
E           and: "                        (fixtures with leading '_' are only shown with '-v')"
E           and: '  --fixtures-per-test   show fixtures per test'
E           and: '  --import-mode={prepend,append}'
E           and: '                        prepend/append to sys.path when importing test'
E           and: '                        modules, default is to prepend.'
E           and: '  --pdb                 start the interactive Python debugger on errors or'
E           and: '                        KeyboardInterrupt.'
E           and: '  --pdbcls=modulename:classname'
E           and: '                        start a custom interactive Python debugger on errors.'
E           and: '                        For example:'
E           and: '                        --pdbcls=IPython.terminal.debugger:TerminalPdb'
E           and: '  --trace               Immediately break when running each test.'
E           and: '  --capture=method      per-test capturing method: one of fd|sys|no.'
E           and: '  -s                    shortcut for --capture=no.'
E           and: '  --runxfail            report the results of xfail tests as if they were not'
E           and: '                        marked'
E           and: '  --lf, --last-failed   rerun only the tests that failed at the last run (or'
E           and: '                        all if none failed)'
E           and: '  --ff, --failed-first  run all tests but run the last failures first. This'
E           and: '                        may re-order tests and thus lead to repeated fixture'
E           and: '                        setup/teardown'
E           and: '  --nf, --new-first     run tests from new files first, then the rest of the'
E           and: '                        tests sorted by file mtime'
E           and: '  --cache-show=[CACHESHOW]'
E           and: "                        show cache contents, don't perform collection or"
E           and: "                        tests. Optional argument: glob (default: '*')."
E           and: '  --cache-clear         remove all cache contents at start of test run.'
E           and: '  --lfnf={all,none}, --last-failed-no-failures={all,none}'
E           and: '                        which tests to run with no previously (known)'
E           and: '                        failures.'
E           and: '  --sw, --stepwise      exit on test failure and continue from last failing'
E           and: '                        test next time'
E           and: '  --stepwise-skip       ignore the first failing test but stop on the next'
E           and: '                        failing test'
E           and: ''
E           and: 'reporting:'
E           and: '  --durations=N         show N slowest setup/test durations (N=0 for all).'
E           and: '  -v, --verbose         increase verbosity.'
E           and: '  -q, --quiet           decrease verbosity.'
E           and: '  --verbosity=VERBOSE   set verbosity'
E           and: '  -r chars              show extra test summary info as specified by chars:'
E           and: '                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,'
E           and: '                        (p)assed, (P)assed with output, (a)ll except passed'
E           and: '                        (p/P), or (A)ll. (w)arnings are enabled by default'
E           and: '                        (see --disable-warnings).'
E           and: '  --disable-warnings, --disable-pytest-warnings'
E           and: '                        disable warnings summary'
E           and: '  -l, --showlocals      show locals in tracebacks (disabled by default).'
E           and: '  --tb=style            traceback print mode (auto/long/short/line/native/no).'
E           and: '  --show-capture={no,stdout,stderr,log,all}'
E           and: '                        Controls how captured stdout/stderr/log is shown on'
E           and: "                        failed tests. Default is 'all'."
E           and: "  --full-trace          don't cut any tracebacks (default is to cut)."
E           and: '  --color=color         color terminal output (yes/no/auto).'
E           and: '  --pastebin=mode       send failed|all info to bpaste.net pastebin service.'
E           and: '  --junit-xml=path      create junit-xml style report file at given path.'
E           and: '  --junit-prefix=str    prepend prefix to classnames in junit-xml output'
E           and: '  --result-log=path     DEPRECATED path for machine-readable result log.'
E           and: ''
E           and: 'collection:'
E           and: "  --collect-only        only collect tests, don't execute them."
E           and: '  --pyargs              try to interpret all arguments as python packages.'
E           and: '  --ignore=path         ignore path during collection (multi-allowed).'
E           and: '  --ignore-glob=path    ignore path pattern during collection (multi-allowed).'
E           and: '  --deselect=nodeid_prefix'
E           and: '                        deselect item during collection (multi-allowed).'
E           and: "  --confcutdir=dir      only load conftest.py's relative to specified dir."
E           and: "  --noconftest          Don't load any conftest.py files."
E           and: '  --keep-duplicates     Keep duplicate tests.'
E           and: '  --collect-in-virtualenv'
E           and: "                        Don't ignore tests in a local virtualenv directory"
E           and: '  --doctest-modules     run doctests in all .py modules'
E           and: '  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}'
E           and: '                        choose another output format for diffs on doctest'
E           and: '                        failure'
E           and: '  --doctest-glob=pat    doctests file matching pattern, default: test*.txt'
E           and: '  --doctest-ignore-import-errors'
E           and: '                        ignore doctest ImportErrors'
E           and: '  --doctest-continue-on-failure'
E           and: '                        for a given doctest, continue to run after the first'
E           and: '                        failure'
E           and: ''
E           and: 'test session debugging and configuration:'
E           and: '  --basetemp=dir        base temporary directory for this test run.(warning:'
E           and: '                        this directory is removed if it exists)'
E           and: '  --version             display pytest lib version and import information.'
E           and: '  -h, --help            show help message and configuration info'
E           and: '  -p name               early-load given plugin module name or entry point'
E           and: '                        (multi-allowed). To avoid loading of plugins, use the'
E           and: '                        `no:` prefix, e.g. `no:doctest`.'
E           and: '  --trace-config        trace considerations of conftest.py files.'
E           and: '  --debug               store internal tracing debug information in'
E           and: "                        'pytestdebug.log'."
E           and: '  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI'
E           and: '                        override ini option with "option=value" style, e.g.'
E           and: '                        `-o xfail_strict=True -o cache_dir=cache`.'
E           and: "  --assert=MODE         Control assertion debugging tools. 'plain' performs no"
E           and: "                        assertion debugging. 'rewrite' (the default) rewrites"
E           and: '                        assert statements in test modules on import to provide'
E           and: '                        assert expression information.'
E           and: '  --setup-only          only setup fixtures, do not execute tests.'
E           and: '  --setup-show          show setup of fixtures while executing tests.'
E           and: '  --setup-plan          show what fixtures and tests would be executed but'
E           and: "                        don't execute anything."
E           and: ''
E           and: 'pytest-warnings:'
E           and: '  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS'
E           and: '                        set which warnings to report, see -W option of python'
E           and: '                        itself.'
E           and: ''
E           and: 'logging:'
E           and: '  --no-print-logs       disable printing caught logs on failed tests.'
E           and: '  --log-level=LOG_LEVEL'
E           and: '                        logging level used by the logging module'
E           and: '  --log-format=LOG_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-date-format=LOG_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-cli-level=LOG_CLI_LEVEL'
E           and: '                        cli logging level.'
E           and: '  --log-cli-format=LOG_CLI_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-cli-date-format=LOG_CLI_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-file=LOG_FILE   path to a file when logging will be written to.'
E           and: '  --log-file-level=LOG_FILE_LEVEL'
E           and: '                        log file logging level.'
E           and: '  --log-file-format=LOG_FILE_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-file-date-format=LOG_FILE_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: ''
E           and: '[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:'
E           and: ''
E           and: '  markers (linelist):   markers for test functions'
E           and: '  empty_parameter_set_mark (string):'
E           and: '                        default marker for empty parametersets'
E           and: '  norecursedirs (args): directory patterns to avoid for recursion'
E           and: '  testpaths (args):     directories to search for tests when no files or'
E           and: '                        directories are given in the command line.'
E           and: '  usefixtures (args):   list of default fixtures to be used with this project'
E           and: '  python_files (args):  glob-style file patterns for Python test module'
E           and: '                        discovery'
E           and: '  python_classes (args):'
E           and: '                        prefixes or glob names for Python test class discovery'
E           and: '  python_functions (args):'
E           and: '                        prefixes or glob names for Python test function and'
E           and: '                        method discovery'
E           and: '  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):'
E           and: '                        disable string escape non-ascii characters, might cause'
E           and: '                        unwanted side effects(use at your own risk)'
E           and: '  console_output_style (string):'
E           and: '                        console output: "classic", or with additional progress'
E           and: '                        information ("progress" (percentage) | "count").'
E           and: '  xfail_strict (bool):  default for the strict parameter of xfail markers when'
E           and: '                        not given explicitly (default: False)'
E           and: '  enable_assertion_pass_hook (bool):'
E           and: '                        Enables the pytest_assertion_pass hook.Make sure to'
E           and: '                        delete any previously generated pyc cache files.'
E           and: '  junit_suite_name (string):'
E           and: '                        Test suite name for JUnit report'
E           and: '  junit_logging (string):'
E           and: '                        Write captured log messages to JUnit report: one of'
E           and: '                        no|system-out|system-err'
E           and: '  junit_log_passing_tests (bool):'
E           and: '                        Capture log information for passing tests to JUnit'
E           and: '                        report:'
E           and: '  junit_duration_report (string):'
E           and: '                        Duration time to report: one of total|call'
E           and: '  junit_family (string):'
E           and: '                        Emit XML for schema: one of legacy|xunit1|xunit2'
E           and: '  doctest_optionflags (args):'
E           and: '                        option flags for doctests'
E           and: '  doctest_encoding (string):'
E           and: '                        encoding used for doctest files'
E           and: '  cache_dir (string):   cache directory path.'
E           and: '  filterwarnings (linelist):'
E           and: '                        Each line specifies a pattern for'
E           and: '                        warnings.filterwarnings. Processed after -W and'
E           and: '                        --pythonwarnings.'
E           and: '  log_print (bool):     default value for --no-print-logs'
E           and: '  log_level (string):   default value for --log-level'
E           and: '  log_format (string):  default value for --log-format'
E           and: '  log_date_format (string):'
E           and: '                        default value for --log-date-format'
E           and: '  log_cli (bool):       enable log display during test run (also known as "live'
E           and: '                        logging").'
E           and: '  log_cli_level (string):'
E           and: '                        default value for --log-cli-level'
E           and: '  log_cli_format (string):'
E           and: '                        default value for --log-cli-format'
E           and: '  log_cli_date_format (string):'
E           and: '                        default value for --log-cli-date-format'
E           and: '  log_file (string):    default value for --log-file'
E           and: '  log_file_level (string):'
E           and: '                        default value for --log-file-level'
E           and: '  log_file_format (string):'
E           and: '                        default value for --log-file-format'
E           and: '  log_file_date_format (string):'
E           and: '                        default value for --log-file-date-format'
E           and: '  faulthandler_timeout (string):'
E           and: '                        Dump the traceback of all threads if a test takes more'
E           and: '                        than TIMEOUT seconds to finish. Not available on'
E           and: '                        Windows.'
E           and: '  addopts (args):       extra command line options'
E           and: '  minversion (string):  minimally required pytest version'
E           and: ''
E           and: 'environment variables:'
E           and: '  PYTEST_ADDOPTS           extra command line options'
E           and: '  PYTEST_PLUGINS           comma-separated plugins to load during startup'
E           and: '  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading'
E           and: "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals"
E           and: ''
E           and: ''
E           and: 'to see available markers type: pytest --markers'
E           and: 'to see available fixtures type: pytest --fixtures'
E           and: "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option"
E           and: ''
E       remains unmatched: '*--xyz*'

/testbed/testing/test_conftest.py:189: Failed
----------------------------- Captured stdout call -----------------------------
usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched
                        against test names and their parent classes. Example:
                        -k 'test_method or test_other' matches all test
                        functions and classes whose name contains
                        'test_method' or 'test_other', while -k 'not
                        test_method' matches those that don't contain
                        'test_method' in their names. -k 'not test_method and
                        not test_other' will eliminate the matches.
                        Additionally keywords are matched to classes and
                        functions containing extra names in their
                        'extra_keyword_matches' set, as well as functions
                        which have names assigned directly to them.
  -m MARKEXPR           only run tests matching given mark expression.
                        example: -m 'mark1 and not mark2'.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --maxfail=num         exit after first num failures or errors.
  --strict-markers, --strict
                        markers not registered in the `markers` section of the
                        configuration file raise errors.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        'root_dir', './root_dir', 'root_dir/another_dir/';
                        absolute path: '/home/user/root_dir'; path with
                        variables: '$HOME/root_dir'.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading '_' are only shown with '-v')
  --fixtures-per-test   show fixtures per test
  --import-mode={prepend,append}
                        prepend/append to sys.path when importing test
                        modules, default is to prepend.
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or
                        all if none failed)
  --ff, --failed-first  run all tests but run the last failures first. This
                        may re-order tests and thus lead to repeated fixture
                        setup/teardown
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don't perform collection or
                        tests. Optional argument: glob (default: '*').
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known)
                        failures.
  --sw, --stepwise      exit on test failure and continue from last failing
                        test next time
  --stepwise-skip       ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  -v, --verbose         increase verbosity.
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default
                        (see --disable-warnings).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is 'all'.
  --full-trace          don't cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --result-log=path     DEPRECATED path for machine-readable result log.

collection:
  --collect-only        only collect tests, don't execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item during collection (multi-allowed).
  --confcutdir=dir      only load conftest.py's relative to specified dir.
  --noconftest          Don't load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don't ignore tests in a local virtualenv directory
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  --version             display pytest lib version and import information.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed). To avoid loading of plugins, use the
                        `no:` prefix, e.g. `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        'pytestdebug.log'.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with "option=value" style, e.g.
                        `-o xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools. 'plain' performs no
                        assertion debugging. 'rewrite' (the default) rewrites
                        assert statements in test modules on import to provide
                        assert expression information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but
                        don't execute anything.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.

logging:
  --no-print-logs       disable printing caught logs on failed tests.
  --log-level=LOG_LEVEL
                        logging level used by the logging module
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: "classic", or with additional progress
                        information ("progress" (percentage) | "count").
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|system-out|system-err
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after -W and
                        --pythonwarnings.
  log_print (bool):     default value for --no-print-logs
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as "live
                        logging").
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish. Not available on
                        Windows.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest's internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option
____________________________ test_conftest_symlink _____________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_symlink0')>

    @pytest.mark.skipif(
        not hasattr(py.path.local, "mksymlinkto"),
        reason="symlink not available on this platform",
    )
    def test_conftest_symlink(testdir):
        """Ensure that conftest.py is used for resolved symlinks."""
        real = testdir.tmpdir.mkdir("real")
        realtests = real.mkdir("app").mkdir("tests")
        testdir.tmpdir.join("symlinktests").mksymlinkto(realtests)
        testdir.tmpdir.join("symlink").mksymlinkto(real)
        testdir.makepyfile(
            **{
                "real/app/tests/test_foo.py": "def test1(fixture): pass",
                "real/conftest.py": textwrap.dedent(
                    """
                    import pytest
    
                    print("conftest_loaded")
    
                    @pytest.fixture
                    def fixture():
                        print("fixture_used")
                    """
                ),
            }
        )
        result = testdir.runpytest("-vs", "symlinktests")
>       result.stdout.fnmatch_lines(
            [
                "*conftest_loaded*",
                "real/app/tests/test_foo.py::test1 fixture_used",
                "PASSED",
            ]
        )
E       Failed: fnmatch: '*conftest_loaded*'
E          with: 'conftest_loaded'
E       nomatch: 'real/app/tests/test_foo.py::test1 fixture_used'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python'
E           and: 'cachedir: .pytest_cache'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_conftest_symlink0'
E           and: 'collecting ... collected 1 item'
E           and: ''
E           and: 'real/app/tests/test_foo.py::test1 ERROR'
E           and: ''
E           and: '==================================== ERRORS ===================================='
E           and: '___________________________ ERROR at setup of test1 ____________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_conftest_symlink0/real/app/tests/test_foo.py, line 1'
E           and: '  def test1(fixture): pass'
E           and: "E       fixture 'fixture' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_conftest_symlink0/real/app/tests/test_foo.py:1'
E           and: '=============================== 1 error in 0.01s ==============================='
E           and: ''
E       remains unmatched: 'real/app/tests/test_foo.py::test1 fixture_used'

/testbed/testing/test_conftest.py:220: Failed
----------------------------- Captured stdout call -----------------------------
conftest_loaded
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python
cachedir: .pytest_cache
rootdir: /tmp/pytest-of-root/pytest-0/test_conftest_symlink0
collecting ... collected 1 item

real/app/tests/test_foo.py::test1 ERROR

==================================== ERRORS ====================================
___________________________ ERROR at setup of test1 ____________________________
file /tmp/pytest-of-root/pytest-0/test_conftest_symlink0/real/app/tests/test_foo.py, line 1
  def test1(fixture): pass
E       fixture 'fixture' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_conftest_symlink0/real/app/tests/test_foo.py:1
=============================== 1 error in 0.01s ===============================
_________________________ test_conftest_symlink_files __________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0')>

    @pytest.mark.skipif(
        not hasattr(py.path.local, "mksymlinkto"),
        reason="symlink not available on this platform",
    )
    def test_conftest_symlink_files(testdir):
        """Check conftest.py loading when running in directory with symlinks."""
        real = testdir.tmpdir.mkdir("real")
        source = {
            "app/test_foo.py": "def test1(fixture): pass",
            "app/__init__.py": "",
            "app/conftest.py": textwrap.dedent(
                """
                import pytest
    
                print("conftest_loaded")
    
                @pytest.fixture
                def fixture():
                    print("fixture_used")
                """
            ),
        }
        testdir.makepyfile(**{"real/%s" % k: v for k, v in source.items()})
    
        # Create a build directory that contains symlinks to actual files
        # but doesn't symlink actual directories.
        build = testdir.tmpdir.mkdir("build")
        build.mkdir("app")
        for f in source:
            build.join(f).mksymlinkto(real.join(f))
        build.chdir()
        result = testdir.runpytest("-vs", "app/test_foo.py")
>       result.stdout.fnmatch_lines(["*conftest_loaded*", "PASSED"])
E       Failed: fnmatch: '*conftest_loaded*'
E          with: 'conftest_loaded'
E       nomatch: 'PASSED'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python'
E           and: 'cachedir: .pytest_cache'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build'
E           and: 'collecting ... collected 1 item'
E           and: ''
E           and: '::test1 ERROR'
E           and: ''
E           and: '==================================== ERRORS ===================================='
E           and: '___________________________ ERROR at setup of test1 ____________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build/app/test_foo.py, line 1'
E           and: '  def test1(fixture): pass'
E           and: "E       fixture 'fixture' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build/app/test_foo.py:1'
E           and: '=============================== 1 error in 0.01s ==============================='
E           and: ''
E       remains unmatched: 'PASSED'

/testbed/testing/test_conftest.py:277: Failed
----------------------------- Captured stdout call -----------------------------
conftest_loaded
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python
cachedir: .pytest_cache
rootdir: /tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build
collecting ... collected 1 item

::test1 ERROR

==================================== ERRORS ====================================
___________________________ ERROR at setup of test1 ____________________________
file /tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build/app/test_foo.py, line 1
  def test1(fixture): pass
E       fixture 'fixture' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_conftest_symlink_files0/build/app/test_foo.py:1
=============================== 1 error in 0.01s ===============================
_______________________ test_conftest_existing_resultlog _______________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_existing_resultlog0')>

    def test_conftest_existing_resultlog(testdir):
        x = testdir.mkdir("tests")
        x.join("conftest.py").write(
            textwrap.dedent(
                """\
                def pytest_addoption(parser):
                    parser.addoption("--xyz", action="store_true")
                """
            )
        )
        testdir.makefile(ext=".log", result="")  # Writes result.log
        result = testdir.runpytest("-h", "--resultlog", "result.log")
>       result.stdout.fnmatch_lines(["*--xyz*"])
E       Failed: nomatch: '*--xyz*'
E           and: 'usage: pytest [options] [file_or_dir] [file_or_dir] [...]'
E           and: ''
E           and: 'positional arguments:'
E           and: '  file_or_dir'
E           and: ''
E           and: 'general:'
E           and: '  -k EXPRESSION         only run tests which match the given substring'
E           and: '                        expression. An expression is a python evaluatable'
E           and: '                        expression where all names are substring-matched'
E           and: '                        against test names and their parent classes. Example:'
E           and: "                        -k 'test_method or test_other' matches all test"
E           and: '                        functions and classes whose name contains'
E           and: "                        'test_method' or 'test_other', while -k 'not"
E           and: "                        test_method' matches those that don't contain"
E           and: "                        'test_method' in their names. -k 'not test_method and"
E           and: "                        not test_other' will eliminate the matches."
E           and: '                        Additionally keywords are matched to classes and'
E           and: '                        functions containing extra names in their'
E           and: "                        'extra_keyword_matches' set, as well as functions"
E           and: '                        which have names assigned directly to them.'
E           and: '  -m MARKEXPR           only run tests matching given mark expression.'
E           and: "                        example: -m 'mark1 and not mark2'."
E           and: '  --markers             show markers (builtin, plugin and per-project ones).'
E           and: '  -x, --exitfirst       exit instantly on first error or failed test.'
E           and: '  --maxfail=num         exit after first num failures or errors.'
E           and: '  --strict-markers, --strict'
E           and: '                        markers not registered in the `markers` section of the'
E           and: '                        configuration file raise errors.'
E           and: '  -c file               load configuration from `file` instead of trying to'
E           and: '                        locate one of the implicit configuration files.'
E           and: '  --continue-on-collection-errors'
E           and: '                        Force test execution even if collection errors occur.'
E           and: '  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:'
E           and: "                        'root_dir', './root_dir', 'root_dir/another_dir/';"
E           and: "                        absolute path: '/home/user/root_dir'; path with"
E           and: "                        variables: '$HOME/root_dir'."
E           and: '  --fixtures, --funcargs'
E           and: '                        show available fixtures, sorted by plugin appearance'
E           and: "                        (fixtures with leading '_' are only shown with '-v')"
E           and: '  --fixtures-per-test   show fixtures per test'
E           and: '  --import-mode={prepend,append}'
E           and: '                        prepend/append to sys.path when importing test'
E           and: '                        modules, default is to prepend.'
E           and: '  --pdb                 start the interactive Python debugger on errors or'
E           and: '                        KeyboardInterrupt.'
E           and: '  --pdbcls=modulename:classname'
E           and: '                        start a custom interactive Python debugger on errors.'
E           and: '                        For example:'
E           and: '                        --pdbcls=IPython.terminal.debugger:TerminalPdb'
E           and: '  --trace               Immediately break when running each test.'
E           and: '  --capture=method      per-test capturing method: one of fd|sys|no.'
E           and: '  -s                    shortcut for --capture=no.'
E           and: '  --runxfail            report the results of xfail tests as if they were not'
E           and: '                        marked'
E           and: '  --lf, --last-failed   rerun only the tests that failed at the last run (or'
E           and: '                        all if none failed)'
E           and: '  --ff, --failed-first  run all tests but run the last failures first. This'
E           and: '                        may re-order tests and thus lead to repeated fixture'
E           and: '                        setup/teardown'
E           and: '  --nf, --new-first     run tests from new files first, then the rest of the'
E           and: '                        tests sorted by file mtime'
E           and: '  --cache-show=[CACHESHOW]'
E           and: "                        show cache contents, don't perform collection or"
E           and: "                        tests. Optional argument: glob (default: '*')."
E           and: '  --cache-clear         remove all cache contents at start of test run.'
E           and: '  --lfnf={all,none}, --last-failed-no-failures={all,none}'
E           and: '                        which tests to run with no previously (known)'
E           and: '                        failures.'
E           and: '  --sw, --stepwise      exit on test failure and continue from last failing'
E           and: '                        test next time'
E           and: '  --stepwise-skip       ignore the first failing test but stop on the next'
E           and: '                        failing test'
E           and: ''
E           and: 'reporting:'
E           and: '  --durations=N         show N slowest setup/test durations (N=0 for all).'
E           and: '  -v, --verbose         increase verbosity.'
E           and: '  -q, --quiet           decrease verbosity.'
E           and: '  --verbosity=VERBOSE   set verbosity'
E           and: '  -r chars              show extra test summary info as specified by chars:'
E           and: '                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,'
E           and: '                        (p)assed, (P)assed with output, (a)ll except passed'
E           and: '                        (p/P), or (A)ll. (w)arnings are enabled by default'
E           and: '                        (see --disable-warnings).'
E           and: '  --disable-warnings, --disable-pytest-warnings'
E           and: '                        disable warnings summary'
E           and: '  -l, --showlocals      show locals in tracebacks (disabled by default).'
E           and: '  --tb=style            traceback print mode (auto/long/short/line/native/no).'
E           and: '  --show-capture={no,stdout,stderr,log,all}'
E           and: '                        Controls how captured stdout/stderr/log is shown on'
E           and: "                        failed tests. Default is 'all'."
E           and: "  --full-trace          don't cut any tracebacks (default is to cut)."
E           and: '  --color=color         color terminal output (yes/no/auto).'
E           and: '  --pastebin=mode       send failed|all info to bpaste.net pastebin service.'
E           and: '  --junit-xml=path      create junit-xml style report file at given path.'
E           and: '  --junit-prefix=str    prepend prefix to classnames in junit-xml output'
E           and: '  --result-log=path     DEPRECATED path for machine-readable result log.'
E           and: ''
E           and: 'collection:'
E           and: "  --collect-only        only collect tests, don't execute them."
E           and: '  --pyargs              try to interpret all arguments as python packages.'
E           and: '  --ignore=path         ignore path during collection (multi-allowed).'
E           and: '  --ignore-glob=path    ignore path pattern during collection (multi-allowed).'
E           and: '  --deselect=nodeid_prefix'
E           and: '                        deselect item during collection (multi-allowed).'
E           and: "  --confcutdir=dir      only load conftest.py's relative to specified dir."
E           and: "  --noconftest          Don't load any conftest.py files."
E           and: '  --keep-duplicates     Keep duplicate tests.'
E           and: '  --collect-in-virtualenv'
E           and: "                        Don't ignore tests in a local virtualenv directory"
E           and: '  --doctest-modules     run doctests in all .py modules'
E           and: '  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}'
E           and: '                        choose another output format for diffs on doctest'
E           and: '                        failure'
E           and: '  --doctest-glob=pat    doctests file matching pattern, default: test*.txt'
E           and: '  --doctest-ignore-import-errors'
E           and: '                        ignore doctest ImportErrors'
E           and: '  --doctest-continue-on-failure'
E           and: '                        for a given doctest, continue to run after the first'
E           and: '                        failure'
E           and: ''
E           and: 'test session debugging and configuration:'
E           and: '  --basetemp=dir        base temporary directory for this test run.(warning:'
E           and: '                        this directory is removed if it exists)'
E           and: '  --version             display pytest lib version and import information.'
E           and: '  -h, --help            show help message and configuration info'
E           and: '  -p name               early-load given plugin module name or entry point'
E           and: '                        (multi-allowed). To avoid loading of plugins, use the'
E           and: '                        `no:` prefix, e.g. `no:doctest`.'
E           and: '  --trace-config        trace considerations of conftest.py files.'
E           and: '  --debug               store internal tracing debug information in'
E           and: "                        'pytestdebug.log'."
E           and: '  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI'
E           and: '                        override ini option with "option=value" style, e.g.'
E           and: '                        `-o xfail_strict=True -o cache_dir=cache`.'
E           and: "  --assert=MODE         Control assertion debugging tools. 'plain' performs no"
E           and: "                        assertion debugging. 'rewrite' (the default) rewrites"
E           and: '                        assert statements in test modules on import to provide'
E           and: '                        assert expression information.'
E           and: '  --setup-only          only setup fixtures, do not execute tests.'
E           and: '  --setup-show          show setup of fixtures while executing tests.'
E           and: '  --setup-plan          show what fixtures and tests would be executed but'
E           and: "                        don't execute anything."
E           and: ''
E           and: 'pytest-warnings:'
E           and: '  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS'
E           and: '                        set which warnings to report, see -W option of python'
E           and: '                        itself.'
E           and: ''
E           and: 'logging:'
E           and: '  --no-print-logs       disable printing caught logs on failed tests.'
E           and: '  --log-level=LOG_LEVEL'
E           and: '                        logging level used by the logging module'
E           and: '  --log-format=LOG_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-date-format=LOG_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-cli-level=LOG_CLI_LEVEL'
E           and: '                        cli logging level.'
E           and: '  --log-cli-format=LOG_CLI_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-cli-date-format=LOG_CLI_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-file=LOG_FILE   path to a file when logging will be written to.'
E           and: '  --log-file-level=LOG_FILE_LEVEL'
E           and: '                        log file logging level.'
E           and: '  --log-file-format=LOG_FILE_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-file-date-format=LOG_FILE_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: ''
E           and: '[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:'
E           and: ''
E           and: '  markers (linelist):   markers for test functions'
E           and: '  empty_parameter_set_mark (string):'
E           and: '                        default marker for empty parametersets'
E           and: '  norecursedirs (args): directory patterns to avoid for recursion'
E           and: '  testpaths (args):     directories to search for tests when no files or'
E           and: '                        directories are given in the command line.'
E           and: '  usefixtures (args):   list of default fixtures to be used with this project'
E           and: '  python_files (args):  glob-style file patterns for Python test module'
E           and: '                        discovery'
E           and: '  python_classes (args):'
E           and: '                        prefixes or glob names for Python test class discovery'
E           and: '  python_functions (args):'
E           and: '                        prefixes or glob names for Python test function and'
E           and: '                        method discovery'
E           and: '  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):'
E           and: '                        disable string escape non-ascii characters, might cause'
E           and: '                        unwanted side effects(use at your own risk)'
E           and: '  console_output_style (string):'
E           and: '                        console output: "classic", or with additional progress'
E           and: '                        information ("progress" (percentage) | "count").'
E           and: '  xfail_strict (bool):  default for the strict parameter of xfail markers when'
E           and: '                        not given explicitly (default: False)'
E           and: '  enable_assertion_pass_hook (bool):'
E           and: '                        Enables the pytest_assertion_pass hook.Make sure to'
E           and: '                        delete any previously generated pyc cache files.'
E           and: '  junit_suite_name (string):'
E           and: '                        Test suite name for JUnit report'
E           and: '  junit_logging (string):'
E           and: '                        Write captured log messages to JUnit report: one of'
E           and: '                        no|system-out|system-err'
E           and: '  junit_log_passing_tests (bool):'
E           and: '                        Capture log information for passing tests to JUnit'
E           and: '                        report:'
E           and: '  junit_duration_report (string):'
E           and: '                        Duration time to report: one of total|call'
E           and: '  junit_family (string):'
E           and: '                        Emit XML for schema: one of legacy|xunit1|xunit2'
E           and: '  doctest_optionflags (args):'
E           and: '                        option flags for doctests'
E           and: '  doctest_encoding (string):'
E           and: '                        encoding used for doctest files'
E           and: '  cache_dir (string):   cache directory path.'
E           and: '  filterwarnings (linelist):'
E           and: '                        Each line specifies a pattern for'
E           and: '                        warnings.filterwarnings. Processed after -W and'
E           and: '                        --pythonwarnings.'
E           and: '  log_print (bool):     default value for --no-print-logs'
E           and: '  log_level (string):   default value for --log-level'
E           and: '  log_format (string):  default value for --log-format'
E           and: '  log_date_format (string):'
E           and: '                        default value for --log-date-format'
E           and: '  log_cli (bool):       enable log display during test run (also known as "live'
E           and: '                        logging").'
E           and: '  log_cli_level (string):'
E           and: '                        default value for --log-cli-level'
E           and: '  log_cli_format (string):'
E           and: '                        default value for --log-cli-format'
E           and: '  log_cli_date_format (string):'
E           and: '                        default value for --log-cli-date-format'
E           and: '  log_file (string):    default value for --log-file'
E           and: '  log_file_level (string):'
E           and: '                        default value for --log-file-level'
E           and: '  log_file_format (string):'
E           and: '                        default value for --log-file-format'
E           and: '  log_file_date_format (string):'
E           and: '                        default value for --log-file-date-format'
E           and: '  faulthandler_timeout (string):'
E           and: '                        Dump the traceback of all threads if a test takes more'
E           and: '                        than TIMEOUT seconds to finish. Not available on'
E           and: '                        Windows.'
E           and: '  addopts (args):       extra command line options'
E           and: '  minversion (string):  minimally required pytest version'
E           and: ''
E           and: 'environment variables:'
E           and: '  PYTEST_ADDOPTS           extra command line options'
E           and: '  PYTEST_PLUGINS           comma-separated plugins to load during startup'
E           and: '  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading'
E           and: "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals"
E           and: ''
E           and: ''
E           and: 'to see available markers type: pytest --markers'
E           and: 'to see available fixtures type: pytest --fixtures'
E           and: "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option"
E           and: ''
E       remains unmatched: '*--xyz*'

/testbed/testing/test_conftest.py:327: Failed
----------------------------- Captured stdout call -----------------------------
usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched
                        against test names and their parent classes. Example:
                        -k 'test_method or test_other' matches all test
                        functions and classes whose name contains
                        'test_method' or 'test_other', while -k 'not
                        test_method' matches those that don't contain
                        'test_method' in their names. -k 'not test_method and
                        not test_other' will eliminate the matches.
                        Additionally keywords are matched to classes and
                        functions containing extra names in their
                        'extra_keyword_matches' set, as well as functions
                        which have names assigned directly to them.
  -m MARKEXPR           only run tests matching given mark expression.
                        example: -m 'mark1 and not mark2'.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --maxfail=num         exit after first num failures or errors.
  --strict-markers, --strict
                        markers not registered in the `markers` section of the
                        configuration file raise errors.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        'root_dir', './root_dir', 'root_dir/another_dir/';
                        absolute path: '/home/user/root_dir'; path with
                        variables: '$HOME/root_dir'.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading '_' are only shown with '-v')
  --fixtures-per-test   show fixtures per test
  --import-mode={prepend,append}
                        prepend/append to sys.path when importing test
                        modules, default is to prepend.
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or
                        all if none failed)
  --ff, --failed-first  run all tests but run the last failures first. This
                        may re-order tests and thus lead to repeated fixture
                        setup/teardown
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don't perform collection or
                        tests. Optional argument: glob (default: '*').
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known)
                        failures.
  --sw, --stepwise      exit on test failure and continue from last failing
                        test next time
  --stepwise-skip       ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  -v, --verbose         increase verbosity.
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default
                        (see --disable-warnings).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is 'all'.
  --full-trace          don't cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --result-log=path     DEPRECATED path for machine-readable result log.

collection:
  --collect-only        only collect tests, don't execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item during collection (multi-allowed).
  --confcutdir=dir      only load conftest.py's relative to specified dir.
  --noconftest          Don't load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don't ignore tests in a local virtualenv directory
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  --version             display pytest lib version and import information.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed). To avoid loading of plugins, use the
                        `no:` prefix, e.g. `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        'pytestdebug.log'.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with "option=value" style, e.g.
                        `-o xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools. 'plain' performs no
                        assertion debugging. 'rewrite' (the default) rewrites
                        assert statements in test modules on import to provide
                        assert expression information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but
                        don't execute anything.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.

logging:
  --no-print-logs       disable printing caught logs on failed tests.
  --log-level=LOG_LEVEL
                        logging level used by the logging module
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: "classic", or with additional progress
                        information ("progress" (percentage) | "count").
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|system-out|system-err
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after -W and
                        --pythonwarnings.
  log_print (bool):     default value for --no-print-logs
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as "live
                        logging").
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish. Not available on
                        Windows.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest's internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option
_______________________ test_conftest_existing_junitxml ________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_existing_junitxml0')>

    def test_conftest_existing_junitxml(testdir):
        x = testdir.mkdir("tests")
        x.join("conftest.py").write(
            textwrap.dedent(
                """\
                def pytest_addoption(parser):
                    parser.addoption("--xyz", action="store_true")
                """
            )
        )
        testdir.makefile(ext=".xml", junit="")  # Writes junit.xml
        result = testdir.runpytest("-h", "--junitxml", "junit.xml")
>       result.stdout.fnmatch_lines(["*--xyz*"])
E       Failed: nomatch: '*--xyz*'
E           and: 'usage: pytest [options] [file_or_dir] [file_or_dir] [...]'
E           and: ''
E           and: 'positional arguments:'
E           and: '  file_or_dir'
E           and: ''
E           and: 'general:'
E           and: '  -k EXPRESSION         only run tests which match the given substring'
E           and: '                        expression. An expression is a python evaluatable'
E           and: '                        expression where all names are substring-matched'
E           and: '                        against test names and their parent classes. Example:'
E           and: "                        -k 'test_method or test_other' matches all test"
E           and: '                        functions and classes whose name contains'
E           and: "                        'test_method' or 'test_other', while -k 'not"
E           and: "                        test_method' matches those that don't contain"
E           and: "                        'test_method' in their names. -k 'not test_method and"
E           and: "                        not test_other' will eliminate the matches."
E           and: '                        Additionally keywords are matched to classes and'
E           and: '                        functions containing extra names in their'
E           and: "                        'extra_keyword_matches' set, as well as functions"
E           and: '                        which have names assigned directly to them.'
E           and: '  -m MARKEXPR           only run tests matching given mark expression.'
E           and: "                        example: -m 'mark1 and not mark2'."
E           and: '  --markers             show markers (builtin, plugin and per-project ones).'
E           and: '  -x, --exitfirst       exit instantly on first error or failed test.'
E           and: '  --maxfail=num         exit after first num failures or errors.'
E           and: '  --strict-markers, --strict'
E           and: '                        markers not registered in the `markers` section of the'
E           and: '                        configuration file raise errors.'
E           and: '  -c file               load configuration from `file` instead of trying to'
E           and: '                        locate one of the implicit configuration files.'
E           and: '  --continue-on-collection-errors'
E           and: '                        Force test execution even if collection errors occur.'
E           and: '  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:'
E           and: "                        'root_dir', './root_dir', 'root_dir/another_dir/';"
E           and: "                        absolute path: '/home/user/root_dir'; path with"
E           and: "                        variables: '$HOME/root_dir'."
E           and: '  --fixtures, --funcargs'
E           and: '                        show available fixtures, sorted by plugin appearance'
E           and: "                        (fixtures with leading '_' are only shown with '-v')"
E           and: '  --fixtures-per-test   show fixtures per test'
E           and: '  --import-mode={prepend,append}'
E           and: '                        prepend/append to sys.path when importing test'
E           and: '                        modules, default is to prepend.'
E           and: '  --pdb                 start the interactive Python debugger on errors or'
E           and: '                        KeyboardInterrupt.'
E           and: '  --pdbcls=modulename:classname'
E           and: '                        start a custom interactive Python debugger on errors.'
E           and: '                        For example:'
E           and: '                        --pdbcls=IPython.terminal.debugger:TerminalPdb'
E           and: '  --trace               Immediately break when running each test.'
E           and: '  --capture=method      per-test capturing method: one of fd|sys|no.'
E           and: '  -s                    shortcut for --capture=no.'
E           and: '  --runxfail            report the results of xfail tests as if they were not'
E           and: '                        marked'
E           and: '  --lf, --last-failed   rerun only the tests that failed at the last run (or'
E           and: '                        all if none failed)'
E           and: '  --ff, --failed-first  run all tests but run the last failures first. This'
E           and: '                        may re-order tests and thus lead to repeated fixture'
E           and: '                        setup/teardown'
E           and: '  --nf, --new-first     run tests from new files first, then the rest of the'
E           and: '                        tests sorted by file mtime'
E           and: '  --cache-show=[CACHESHOW]'
E           and: "                        show cache contents, don't perform collection or"
E           and: "                        tests. Optional argument: glob (default: '*')."
E           and: '  --cache-clear         remove all cache contents at start of test run.'
E           and: '  --lfnf={all,none}, --last-failed-no-failures={all,none}'
E           and: '                        which tests to run with no previously (known)'
E           and: '                        failures.'
E           and: '  --sw, --stepwise      exit on test failure and continue from last failing'
E           and: '                        test next time'
E           and: '  --stepwise-skip       ignore the first failing test but stop on the next'
E           and: '                        failing test'
E           and: ''
E           and: 'reporting:'
E           and: '  --durations=N         show N slowest setup/test durations (N=0 for all).'
E           and: '  -v, --verbose         increase verbosity.'
E           and: '  -q, --quiet           decrease verbosity.'
E           and: '  --verbosity=VERBOSE   set verbosity'
E           and: '  -r chars              show extra test summary info as specified by chars:'
E           and: '                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,'
E           and: '                        (p)assed, (P)assed with output, (a)ll except passed'
E           and: '                        (p/P), or (A)ll. (w)arnings are enabled by default'
E           and: '                        (see --disable-warnings).'
E           and: '  --disable-warnings, --disable-pytest-warnings'
E           and: '                        disable warnings summary'
E           and: '  -l, --showlocals      show locals in tracebacks (disabled by default).'
E           and: '  --tb=style            traceback print mode (auto/long/short/line/native/no).'
E           and: '  --show-capture={no,stdout,stderr,log,all}'
E           and: '                        Controls how captured stdout/stderr/log is shown on'
E           and: "                        failed tests. Default is 'all'."
E           and: "  --full-trace          don't cut any tracebacks (default is to cut)."
E           and: '  --color=color         color terminal output (yes/no/auto).'
E           and: '  --pastebin=mode       send failed|all info to bpaste.net pastebin service.'
E           and: '  --junit-xml=path      create junit-xml style report file at given path.'
E           and: '  --junit-prefix=str    prepend prefix to classnames in junit-xml output'
E           and: '  --result-log=path     DEPRECATED path for machine-readable result log.'
E           and: ''
E           and: 'collection:'
E           and: "  --collect-only        only collect tests, don't execute them."
E           and: '  --pyargs              try to interpret all arguments as python packages.'
E           and: '  --ignore=path         ignore path during collection (multi-allowed).'
E           and: '  --ignore-glob=path    ignore path pattern during collection (multi-allowed).'
E           and: '  --deselect=nodeid_prefix'
E           and: '                        deselect item during collection (multi-allowed).'
E           and: "  --confcutdir=dir      only load conftest.py's relative to specified dir."
E           and: "  --noconftest          Don't load any conftest.py files."
E           and: '  --keep-duplicates     Keep duplicate tests.'
E           and: '  --collect-in-virtualenv'
E           and: "                        Don't ignore tests in a local virtualenv directory"
E           and: '  --doctest-modules     run doctests in all .py modules'
E           and: '  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}'
E           and: '                        choose another output format for diffs on doctest'
E           and: '                        failure'
E           and: '  --doctest-glob=pat    doctests file matching pattern, default: test*.txt'
E           and: '  --doctest-ignore-import-errors'
E           and: '                        ignore doctest ImportErrors'
E           and: '  --doctest-continue-on-failure'
E           and: '                        for a given doctest, continue to run after the first'
E           and: '                        failure'
E           and: ''
E           and: 'test session debugging and configuration:'
E           and: '  --basetemp=dir        base temporary directory for this test run.(warning:'
E           and: '                        this directory is removed if it exists)'
E           and: '  --version             display pytest lib version and import information.'
E           and: '  -h, --help            show help message and configuration info'
E           and: '  -p name               early-load given plugin module name or entry point'
E           and: '                        (multi-allowed). To avoid loading of plugins, use the'
E           and: '                        `no:` prefix, e.g. `no:doctest`.'
E           and: '  --trace-config        trace considerations of conftest.py files.'
E           and: '  --debug               store internal tracing debug information in'
E           and: "                        'pytestdebug.log'."
E           and: '  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI'
E           and: '                        override ini option with "option=value" style, e.g.'
E           and: '                        `-o xfail_strict=True -o cache_dir=cache`.'
E           and: "  --assert=MODE         Control assertion debugging tools. 'plain' performs no"
E           and: "                        assertion debugging. 'rewrite' (the default) rewrites"
E           and: '                        assert statements in test modules on import to provide'
E           and: '                        assert expression information.'
E           and: '  --setup-only          only setup fixtures, do not execute tests.'
E           and: '  --setup-show          show setup of fixtures while executing tests.'
E           and: '  --setup-plan          show what fixtures and tests would be executed but'
E           and: "                        don't execute anything."
E           and: ''
E           and: 'pytest-warnings:'
E           and: '  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS'
E           and: '                        set which warnings to report, see -W option of python'
E           and: '                        itself.'
E           and: ''
E           and: 'logging:'
E           and: '  --no-print-logs       disable printing caught logs on failed tests.'
E           and: '  --log-level=LOG_LEVEL'
E           and: '                        logging level used by the logging module'
E           and: '  --log-format=LOG_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-date-format=LOG_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-cli-level=LOG_CLI_LEVEL'
E           and: '                        cli logging level.'
E           and: '  --log-cli-format=LOG_CLI_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-cli-date-format=LOG_CLI_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-file=LOG_FILE   path to a file when logging will be written to.'
E           and: '  --log-file-level=LOG_FILE_LEVEL'
E           and: '                        log file logging level.'
E           and: '  --log-file-format=LOG_FILE_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-file-date-format=LOG_FILE_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: ''
E           and: '[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:'
E           and: ''
E           and: '  markers (linelist):   markers for test functions'
E           and: '  empty_parameter_set_mark (string):'
E           and: '                        default marker for empty parametersets'
E           and: '  norecursedirs (args): directory patterns to avoid for recursion'
E           and: '  testpaths (args):     directories to search for tests when no files or'
E           and: '                        directories are given in the command line.'
E           and: '  usefixtures (args):   list of default fixtures to be used with this project'
E           and: '  python_files (args):  glob-style file patterns for Python test module'
E           and: '                        discovery'
E           and: '  python_classes (args):'
E           and: '                        prefixes or glob names for Python test class discovery'
E           and: '  python_functions (args):'
E           and: '                        prefixes or glob names for Python test function and'
E           and: '                        method discovery'
E           and: '  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):'
E           and: '                        disable string escape non-ascii characters, might cause'
E           and: '                        unwanted side effects(use at your own risk)'
E           and: '  console_output_style (string):'
E           and: '                        console output: "classic", or with additional progress'
E           and: '                        information ("progress" (percentage) | "count").'
E           and: '  xfail_strict (bool):  default for the strict parameter of xfail markers when'
E           and: '                        not given explicitly (default: False)'
E           and: '  enable_assertion_pass_hook (bool):'
E           and: '                        Enables the pytest_assertion_pass hook.Make sure to'
E           and: '                        delete any previously generated pyc cache files.'
E           and: '  junit_suite_name (string):'
E           and: '                        Test suite name for JUnit report'
E           and: '  junit_logging (string):'
E           and: '                        Write captured log messages to JUnit report: one of'
E           and: '                        no|system-out|system-err'
E           and: '  junit_log_passing_tests (bool):'
E           and: '                        Capture log information for passing tests to JUnit'
E           and: '                        report:'
E           and: '  junit_duration_report (string):'
E           and: '                        Duration time to report: one of total|call'
E           and: '  junit_family (string):'
E           and: '                        Emit XML for schema: one of legacy|xunit1|xunit2'
E           and: '  doctest_optionflags (args):'
E           and: '                        option flags for doctests'
E           and: '  doctest_encoding (string):'
E           and: '                        encoding used for doctest files'
E           and: '  cache_dir (string):   cache directory path.'
E           and: '  filterwarnings (linelist):'
E           and: '                        Each line specifies a pattern for'
E           and: '                        warnings.filterwarnings. Processed after -W and'
E           and: '                        --pythonwarnings.'
E           and: '  log_print (bool):     default value for --no-print-logs'
E           and: '  log_level (string):   default value for --log-level'
E           and: '  log_format (string):  default value for --log-format'
E           and: '  log_date_format (string):'
E           and: '                        default value for --log-date-format'
E           and: '  log_cli (bool):       enable log display during test run (also known as "live'
E           and: '                        logging").'
E           and: '  log_cli_level (string):'
E           and: '                        default value for --log-cli-level'
E           and: '  log_cli_format (string):'
E           and: '                        default value for --log-cli-format'
E           and: '  log_cli_date_format (string):'
E           and: '                        default value for --log-cli-date-format'
E           and: '  log_file (string):    default value for --log-file'
E           and: '  log_file_level (string):'
E           and: '                        default value for --log-file-level'
E           and: '  log_file_format (string):'
E           and: '                        default value for --log-file-format'
E           and: '  log_file_date_format (string):'
E           and: '                        default value for --log-file-date-format'
E           and: '  faulthandler_timeout (string):'
E           and: '                        Dump the traceback of all threads if a test takes more'
E           and: '                        than TIMEOUT seconds to finish. Not available on'
E           and: '                        Windows.'
E           and: '  addopts (args):       extra command line options'
E           and: '  minversion (string):  minimally required pytest version'
E           and: ''
E           and: 'environment variables:'
E           and: '  PYTEST_ADDOPTS           extra command line options'
E           and: '  PYTEST_PLUGINS           comma-separated plugins to load during startup'
E           and: '  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading'
E           and: "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals"
E           and: ''
E           and: ''
E           and: 'to see available markers type: pytest --markers'
E           and: 'to see available fixtures type: pytest --fixtures'
E           and: "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option"
E           and: ''
E       remains unmatched: '*--xyz*'

/testbed/testing/test_conftest.py:342: Failed
----------------------------- Captured stdout call -----------------------------
usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched
                        against test names and their parent classes. Example:
                        -k 'test_method or test_other' matches all test
                        functions and classes whose name contains
                        'test_method' or 'test_other', while -k 'not
                        test_method' matches those that don't contain
                        'test_method' in their names. -k 'not test_method and
                        not test_other' will eliminate the matches.
                        Additionally keywords are matched to classes and
                        functions containing extra names in their
                        'extra_keyword_matches' set, as well as functions
                        which have names assigned directly to them.
  -m MARKEXPR           only run tests matching given mark expression.
                        example: -m 'mark1 and not mark2'.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --maxfail=num         exit after first num failures or errors.
  --strict-markers, --strict
                        markers not registered in the `markers` section of the
                        configuration file raise errors.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        'root_dir', './root_dir', 'root_dir/another_dir/';
                        absolute path: '/home/user/root_dir'; path with
                        variables: '$HOME/root_dir'.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading '_' are only shown with '-v')
  --fixtures-per-test   show fixtures per test
  --import-mode={prepend,append}
                        prepend/append to sys.path when importing test
                        modules, default is to prepend.
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or
                        all if none failed)
  --ff, --failed-first  run all tests but run the last failures first. This
                        may re-order tests and thus lead to repeated fixture
                        setup/teardown
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don't perform collection or
                        tests. Optional argument: glob (default: '*').
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known)
                        failures.
  --sw, --stepwise      exit on test failure and continue from last failing
                        test next time
  --stepwise-skip       ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  -v, --verbose         increase verbosity.
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default
                        (see --disable-warnings).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is 'all'.
  --full-trace          don't cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --result-log=path     DEPRECATED path for machine-readable result log.

collection:
  --collect-only        only collect tests, don't execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item during collection (multi-allowed).
  --confcutdir=dir      only load conftest.py's relative to specified dir.
  --noconftest          Don't load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don't ignore tests in a local virtualenv directory
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  --version             display pytest lib version and import information.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed). To avoid loading of plugins, use the
                        `no:` prefix, e.g. `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        'pytestdebug.log'.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with "option=value" style, e.g.
                        `-o xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools. 'plain' performs no
                        assertion debugging. 'rewrite' (the default) rewrites
                        assert statements in test modules on import to provide
                        assert expression information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but
                        don't execute anything.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.

logging:
  --no-print-logs       disable printing caught logs on failed tests.
  --log-level=LOG_LEVEL
                        logging level used by the logging module
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: "classic", or with additional progress
                        information ("progress" (percentage) | "count").
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|system-out|system-err
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after -W and
                        --pythonwarnings.
  log_print (bool):     default value for --no-print-logs
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as "live
                        logging").
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish. Not available on
                        Windows.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest's internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option
_____________________ test_conftest_found_with_double_dash _____________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_conftest_found_with_double_dash0')>

    def test_conftest_found_with_double_dash(testdir):
        sub = testdir.mkdir("sub")
        sub.join("conftest.py").write(
            textwrap.dedent(
                """\
                def pytest_addoption(parser):
                    parser.addoption("--hello-world", action="store_true")
                """
            )
        )
        p = sub.join("test_hello.py")
        p.write("def test_hello(): pass")
        result = testdir.runpytest(str(p) + "::test_hello", "-h")
>       result.stdout.fnmatch_lines(
            """
            *--hello-world*
        """
        )
E       Failed: nomatch: '*--hello-world*'
E           and: 'usage: pytest [options] [file_or_dir] [file_or_dir] [...]'
E           and: ''
E           and: 'positional arguments:'
E           and: '  file_or_dir'
E           and: ''
E           and: 'general:'
E           and: '  -k EXPRESSION         only run tests which match the given substring'
E           and: '                        expression. An expression is a python evaluatable'
E           and: '                        expression where all names are substring-matched'
E           and: '                        against test names and their parent classes. Example:'
E           and: "                        -k 'test_method or test_other' matches all test"
E           and: '                        functions and classes whose name contains'
E           and: "                        'test_method' or 'test_other', while -k 'not"
E           and: "                        test_method' matches those that don't contain"
E           and: "                        'test_method' in their names. -k 'not test_method and"
E           and: "                        not test_other' will eliminate the matches."
E           and: '                        Additionally keywords are matched to classes and'
E           and: '                        functions containing extra names in their'
E           and: "                        'extra_keyword_matches' set, as well as functions"
E           and: '                        which have names assigned directly to them.'
E           and: '  -m MARKEXPR           only run tests matching given mark expression.'
E           and: "                        example: -m 'mark1 and not mark2'."
E           and: '  --markers             show markers (builtin, plugin and per-project ones).'
E           and: '  -x, --exitfirst       exit instantly on first error or failed test.'
E           and: '  --maxfail=num         exit after first num failures or errors.'
E           and: '  --strict-markers, --strict'
E           and: '                        markers not registered in the `markers` section of the'
E           and: '                        configuration file raise errors.'
E           and: '  -c file               load configuration from `file` instead of trying to'
E           and: '                        locate one of the implicit configuration files.'
E           and: '  --continue-on-collection-errors'
E           and: '                        Force test execution even if collection errors occur.'
E           and: '  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:'
E           and: "                        'root_dir', './root_dir', 'root_dir/another_dir/';"
E           and: "                        absolute path: '/home/user/root_dir'; path with"
E           and: "                        variables: '$HOME/root_dir'."
E           and: '  --fixtures, --funcargs'
E           and: '                        show available fixtures, sorted by plugin appearance'
E           and: "                        (fixtures with leading '_' are only shown with '-v')"
E           and: '  --fixtures-per-test   show fixtures per test'
E           and: '  --import-mode={prepend,append}'
E           and: '                        prepend/append to sys.path when importing test'
E           and: '                        modules, default is to prepend.'
E           and: '  --pdb                 start the interactive Python debugger on errors or'
E           and: '                        KeyboardInterrupt.'
E           and: '  --pdbcls=modulename:classname'
E           and: '                        start a custom interactive Python debugger on errors.'
E           and: '                        For example:'
E           and: '                        --pdbcls=IPython.terminal.debugger:TerminalPdb'
E           and: '  --trace               Immediately break when running each test.'
E           and: '  --capture=method      per-test capturing method: one of fd|sys|no.'
E           and: '  -s                    shortcut for --capture=no.'
E           and: '  --runxfail            report the results of xfail tests as if they were not'
E           and: '                        marked'
E           and: '  --lf, --last-failed   rerun only the tests that failed at the last run (or'
E           and: '                        all if none failed)'
E           and: '  --ff, --failed-first  run all tests but run the last failures first. This'
E           and: '                        may re-order tests and thus lead to repeated fixture'
E           and: '                        setup/teardown'
E           and: '  --nf, --new-first     run tests from new files first, then the rest of the'
E           and: '                        tests sorted by file mtime'
E           and: '  --cache-show=[CACHESHOW]'
E           and: "                        show cache contents, don't perform collection or"
E           and: "                        tests. Optional argument: glob (default: '*')."
E           and: '  --cache-clear         remove all cache contents at start of test run.'
E           and: '  --lfnf={all,none}, --last-failed-no-failures={all,none}'
E           and: '                        which tests to run with no previously (known)'
E           and: '                        failures.'
E           and: '  --sw, --stepwise      exit on test failure and continue from last failing'
E           and: '                        test next time'
E           and: '  --stepwise-skip       ignore the first failing test but stop on the next'
E           and: '                        failing test'
E           and: ''
E           and: 'reporting:'
E           and: '  --durations=N         show N slowest setup/test durations (N=0 for all).'
E           and: '  -v, --verbose         increase verbosity.'
E           and: '  -q, --quiet           decrease verbosity.'
E           and: '  --verbosity=VERBOSE   set verbosity'
E           and: '  -r chars              show extra test summary info as specified by chars:'
E           and: '                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,'
E           and: '                        (p)assed, (P)assed with output, (a)ll except passed'
E           and: '                        (p/P), or (A)ll. (w)arnings are enabled by default'
E           and: '                        (see --disable-warnings).'
E           and: '  --disable-warnings, --disable-pytest-warnings'
E           and: '                        disable warnings summary'
E           and: '  -l, --showlocals      show locals in tracebacks (disabled by default).'
E           and: '  --tb=style            traceback print mode (auto/long/short/line/native/no).'
E           and: '  --show-capture={no,stdout,stderr,log,all}'
E           and: '                        Controls how captured stdout/stderr/log is shown on'
E           and: "                        failed tests. Default is 'all'."
E           and: "  --full-trace          don't cut any tracebacks (default is to cut)."
E           and: '  --color=color         color terminal output (yes/no/auto).'
E           and: '  --pastebin=mode       send failed|all info to bpaste.net pastebin service.'
E           and: '  --junit-xml=path      create junit-xml style report file at given path.'
E           and: '  --junit-prefix=str    prepend prefix to classnames in junit-xml output'
E           and: '  --result-log=path     DEPRECATED path for machine-readable result log.'
E           and: ''
E           and: 'collection:'
E           and: "  --collect-only        only collect tests, don't execute them."
E           and: '  --pyargs              try to interpret all arguments as python packages.'
E           and: '  --ignore=path         ignore path during collection (multi-allowed).'
E           and: '  --ignore-glob=path    ignore path pattern during collection (multi-allowed).'
E           and: '  --deselect=nodeid_prefix'
E           and: '                        deselect item during collection (multi-allowed).'
E           and: "  --confcutdir=dir      only load conftest.py's relative to specified dir."
E           and: "  --noconftest          Don't load any conftest.py files."
E           and: '  --keep-duplicates     Keep duplicate tests.'
E           and: '  --collect-in-virtualenv'
E           and: "                        Don't ignore tests in a local virtualenv directory"
E           and: '  --doctest-modules     run doctests in all .py modules'
E           and: '  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}'
E           and: '                        choose another output format for diffs on doctest'
E           and: '                        failure'
E           and: '  --doctest-glob=pat    doctests file matching pattern, default: test*.txt'
E           and: '  --doctest-ignore-import-errors'
E           and: '                        ignore doctest ImportErrors'
E           and: '  --doctest-continue-on-failure'
E           and: '                        for a given doctest, continue to run after the first'
E           and: '                        failure'
E           and: ''
E           and: 'test session debugging and configuration:'
E           and: '  --basetemp=dir        base temporary directory for this test run.(warning:'
E           and: '                        this directory is removed if it exists)'
E           and: '  --version             display pytest lib version and import information.'
E           and: '  -h, --help            show help message and configuration info'
E           and: '  -p name               early-load given plugin module name or entry point'
E           and: '                        (multi-allowed). To avoid loading of plugins, use the'
E           and: '                        `no:` prefix, e.g. `no:doctest`.'
E           and: '  --trace-config        trace considerations of conftest.py files.'
E           and: '  --debug               store internal tracing debug information in'
E           and: "                        'pytestdebug.log'."
E           and: '  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI'
E           and: '                        override ini option with "option=value" style, e.g.'
E           and: '                        `-o xfail_strict=True -o cache_dir=cache`.'
E           and: "  --assert=MODE         Control assertion debugging tools. 'plain' performs no"
E           and: "                        assertion debugging. 'rewrite' (the default) rewrites"
E           and: '                        assert statements in test modules on import to provide'
E           and: '                        assert expression information.'
E           and: '  --setup-only          only setup fixtures, do not execute tests.'
E           and: '  --setup-show          show setup of fixtures while executing tests.'
E           and: '  --setup-plan          show what fixtures and tests would be executed but'
E           and: "                        don't execute anything."
E           and: ''
E           and: 'pytest-warnings:'
E           and: '  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS'
E           and: '                        set which warnings to report, see -W option of python'
E           and: '                        itself.'
E           and: ''
E           and: 'logging:'
E           and: '  --no-print-logs       disable printing caught logs on failed tests.'
E           and: '  --log-level=LOG_LEVEL'
E           and: '                        logging level used by the logging module'
E           and: '  --log-format=LOG_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-date-format=LOG_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-cli-level=LOG_CLI_LEVEL'
E           and: '                        cli logging level.'
E           and: '  --log-cli-format=LOG_CLI_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-cli-date-format=LOG_CLI_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: '  --log-file=LOG_FILE   path to a file when logging will be written to.'
E           and: '  --log-file-level=LOG_FILE_LEVEL'
E           and: '                        log file logging level.'
E           and: '  --log-file-format=LOG_FILE_FORMAT'
E           and: '                        log format as used by the logging module.'
E           and: '  --log-file-date-format=LOG_FILE_DATE_FORMAT'
E           and: '                        log date format as used by the logging module.'
E           and: ''
E           and: '[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:'
E           and: ''
E           and: '  markers (linelist):   markers for test functions'
E           and: '  empty_parameter_set_mark (string):'
E           and: '                        default marker for empty parametersets'
E           and: '  norecursedirs (args): directory patterns to avoid for recursion'
E           and: '  testpaths (args):     directories to search for tests when no files or'
E           and: '                        directories are given in the command line.'
E           and: '  usefixtures (args):   list of default fixtures to be used with this project'
E           and: '  python_files (args):  glob-style file patterns for Python test module'
E           and: '                        discovery'
E           and: '  python_classes (args):'
E           and: '                        prefixes or glob names for Python test class discovery'
E           and: '  python_functions (args):'
E           and: '                        prefixes or glob names for Python test function and'
E           and: '                        method discovery'
E           and: '  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):'
E           and: '                        disable string escape non-ascii characters, might cause'
E           and: '                        unwanted side effects(use at your own risk)'
E           and: '  console_output_style (string):'
E           and: '                        console output: "classic", or with additional progress'
E           and: '                        information ("progress" (percentage) | "count").'
E           and: '  xfail_strict (bool):  default for the strict parameter of xfail markers when'
E           and: '                        not given explicitly (default: False)'
E           and: '  enable_assertion_pass_hook (bool):'
E           and: '                        Enables the pytest_assertion_pass hook.Make sure to'
E           and: '                        delete any previously generated pyc cache files.'
E           and: '  junit_suite_name (string):'
E           and: '                        Test suite name for JUnit report'
E           and: '  junit_logging (string):'
E           and: '                        Write captured log messages to JUnit report: one of'
E           and: '                        no|system-out|system-err'
E           and: '  junit_log_passing_tests (bool):'
E           and: '                        Capture log information for passing tests to JUnit'
E           and: '                        report:'
E           and: '  junit_duration_report (string):'
E           and: '                        Duration time to report: one of total|call'
E           and: '  junit_family (string):'
E           and: '                        Emit XML for schema: one of legacy|xunit1|xunit2'
E           and: '  doctest_optionflags (args):'
E           and: '                        option flags for doctests'
E           and: '  doctest_encoding (string):'
E           and: '                        encoding used for doctest files'
E           and: '  cache_dir (string):   cache directory path.'
E           and: '  filterwarnings (linelist):'
E           and: '                        Each line specifies a pattern for'
E           and: '                        warnings.filterwarnings. Processed after -W and'
E           and: '                        --pythonwarnings.'
E           and: '  log_print (bool):     default value for --no-print-logs'
E           and: '  log_level (string):   default value for --log-level'
E           and: '  log_format (string):  default value for --log-format'
E           and: '  log_date_format (string):'
E           and: '                        default value for --log-date-format'
E           and: '  log_cli (bool):       enable log display during test run (also known as "live'
E           and: '                        logging").'
E           and: '  log_cli_level (string):'
E           and: '                        default value for --log-cli-level'
E           and: '  log_cli_format (string):'
E           and: '                        default value for --log-cli-format'
E           and: '  log_cli_date_format (string):'
E           and: '                        default value for --log-cli-date-format'
E           and: '  log_file (string):    default value for --log-file'
E           and: '  log_file_level (string):'
E           and: '                        default value for --log-file-level'
E           and: '  log_file_format (string):'
E           and: '                        default value for --log-file-format'
E           and: '  log_file_date_format (string):'
E           and: '                        default value for --log-file-date-format'
E           and: '  faulthandler_timeout (string):'
E           and: '                        Dump the traceback of all threads if a test takes more'
E           and: '                        than TIMEOUT seconds to finish. Not available on'
E           and: '                        Windows.'
E           and: '  addopts (args):       extra command line options'
E           and: '  minversion (string):  minimally required pytest version'
E           and: ''
E           and: 'environment variables:'
E           and: '  PYTEST_ADDOPTS           extra command line options'
E           and: '  PYTEST_PLUGINS           comma-separated plugins to load during startup'
E           and: '  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading'
E           and: "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals"
E           and: ''
E           and: ''
E           and: 'to see available markers type: pytest --markers'
E           and: 'to see available fixtures type: pytest --fixtures'
E           and: "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option"
E           and: ''
E       remains unmatched: '*--hello-world*'

/testbed/testing/test_conftest.py:418: Failed
----------------------------- Captured stdout call -----------------------------
usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched
                        against test names and their parent classes. Example:
                        -k 'test_method or test_other' matches all test
                        functions and classes whose name contains
                        'test_method' or 'test_other', while -k 'not
                        test_method' matches those that don't contain
                        'test_method' in their names. -k 'not test_method and
                        not test_other' will eliminate the matches.
                        Additionally keywords are matched to classes and
                        functions containing extra names in their
                        'extra_keyword_matches' set, as well as functions
                        which have names assigned directly to them.
  -m MARKEXPR           only run tests matching given mark expression.
                        example: -m 'mark1 and not mark2'.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --maxfail=num         exit after first num failures or errors.
  --strict-markers, --strict
                        markers not registered in the `markers` section of the
                        configuration file raise errors.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        'root_dir', './root_dir', 'root_dir/another_dir/';
                        absolute path: '/home/user/root_dir'; path with
                        variables: '$HOME/root_dir'.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading '_' are only shown with '-v')
  --fixtures-per-test   show fixtures per test
  --import-mode={prepend,append}
                        prepend/append to sys.path when importing test
                        modules, default is to prepend.
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or
                        all if none failed)
  --ff, --failed-first  run all tests but run the last failures first. This
                        may re-order tests and thus lead to repeated fixture
                        setup/teardown
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don't perform collection or
                        tests. Optional argument: glob (default: '*').
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known)
                        failures.
  --sw, --stepwise      exit on test failure and continue from last failing
                        test next time
  --stepwise-skip       ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  -v, --verbose         increase verbosity.
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default
                        (see --disable-warnings).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is 'all'.
  --full-trace          don't cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --result-log=path     DEPRECATED path for machine-readable result log.

collection:
  --collect-only        only collect tests, don't execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item during collection (multi-allowed).
  --confcutdir=dir      only load conftest.py's relative to specified dir.
  --noconftest          Don't load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don't ignore tests in a local virtualenv directory
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  --version             display pytest lib version and import information.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed). To avoid loading of plugins, use the
                        `no:` prefix, e.g. `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        'pytestdebug.log'.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with "option=value" style, e.g.
                        `-o xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools. 'plain' performs no
                        assertion debugging. 'rewrite' (the default) rewrites
                        assert statements in test modules on import to provide
                        assert expression information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but
                        don't execute anything.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.

logging:
  --no-print-logs       disable printing caught logs on failed tests.
  --log-level=LOG_LEVEL
                        logging level used by the logging module
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: "classic", or with additional progress
                        information ("progress" (percentage) | "count").
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|system-out|system-err
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after -W and
                        --pythonwarnings.
  log_print (bool):     default value for --no-print-logs
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as "live
                        logging").
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish. Not available on
                        Windows.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest's internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option
__ TestConftestVisibility.test_parsefactories_relative_node_ids[runner-..-3] ___

self = <test_conftest.TestConftestVisibility object at 0x7ffffe2c5dc0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0')>
chdir = 'runner', testarg = '..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe255670>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: empty
pytestarg        : ..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe28bc70>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe25e190>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe28bcd0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids0 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe1f1040>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe1f1550>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe2555b0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe255160>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe263ca0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe27a6d0>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids0/package/swc/test_with_conftest.py:1
3 error in 0.04s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[package-..-3] __

self = <test_conftest.TestConftestVisibility object at 0x7ffffe3d6d90>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1')>
chdir = 'package', testarg = '..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe159130>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package
pytestarg        : ..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe1e9490>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe255a90>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe1e9460>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids1 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe159b20>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe1594c0>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe159ca0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe1598e0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe159790>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe1b21f0>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids1/package/swc/test_with_conftest.py:1
3 error in 0.05s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[swc-../..-3] ___

self = <test_conftest.TestConftestVisibility object at 0x7ffffe16a880>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2')>
chdir = 'swc', testarg = '../..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe0e9d90>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/swc
pytestarg        : ../..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe176af0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe0e4f40>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe176a90>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids2 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe0e9fd0>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe0e9fa0>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe0fadc0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe0c2310>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe0c21f0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe0c1700>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids2/package/swc/test_with_conftest.py:1
3 error in 0.04s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[snc-../..-3] ___

self = <test_conftest.TestConftestVisibility object at 0x7ffffe0d4910>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3')>
chdir = 'snc', testarg = '../..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe12ab20>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/snc
pytestarg        : ../..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe0fa880>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe1278e0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe0fa850>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids3 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe12a940>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe12a7f0>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe12ab80>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdf88f70>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdf884c0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe1237c0>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids3/package/swc/test_with_conftest.py:1
3 error in 0.03s
_ TestConftestVisibility.test_parsefactories_relative_node_ids[runner-../package-3] _

self = <test_conftest.TestConftestVisibility object at 0x7ffffdfbdc10>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4')>
chdir = 'runner', testarg = '../package', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdf42640>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: empty
pytestarg        : ../package
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdfbd910>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdfaaa00>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdfbdc70>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids4 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdf424f0>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdf42580>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdf42dc0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdf42a60>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdf426a0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffdf82be0>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids4/package/swc/test_with_conftest.py:1
3 error in 0.02s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[package-.-3] ___

self = <test_conftest.TestConftestVisibility object at 0x7ffffdf6bbb0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5')>
chdir = 'package', testarg = '.', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdecb040>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package
pytestarg        : .
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdf6b1f0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffde20fd0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdf6b4c0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdecb520>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdecb460>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdecb340>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdefaf70>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdefaee0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffdee0550>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids5/package/swc/test_with_conftest.py:1
3 error in 0.03s
____ TestConftestVisibility.test_parsefactories_relative_node_ids[swc-..-3] ____

self = <test_conftest.TestConftestVisibility object at 0x7ffffdedb670>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6')>
chdir = 'swc', testarg = '..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdd97b20>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/swc
pytestarg        : ..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdecbb50>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffecb76d0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdecbb20>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdd972b0>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdd97910>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdd97a00>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffde751f0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffde75520>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffde66d00>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids6/package/swc/test_with_conftest.py:1
3 error in 0.03s
____ TestConftestVisibility.test_parsefactories_relative_node_ids[snc-..-3] ____

self = <test_conftest.TestConftestVisibility object at 0x7ffffde68d30>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7')>
chdir = 'snc', testarg = '..', expect_ntests_passed = 3

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffddebdf0>, passed = 3
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 3 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/snc
pytestarg        : ..
expected pass    : 3
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdd8c490>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffddbdf70>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdd8c430>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffddeb9a0>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffddeba00>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffddebcd0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffddc34c0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffddc3160>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffddfce50>
EEE                                                                      [100%]
==================================== ERRORS ====================================
________________________ ERROR at setup of test_pkgroot ________________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/test_pkgroot.py, line 1
  def test_pkgroot(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/test_pkgroot.py:1
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/snc/test_no_conftest.py:1
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids7/package/swc/test_with_conftest.py:1
3 error in 0.04s
_ TestConftestVisibility.test_parsefactories_relative_node_ids[runner-../package/swc-1] _

self = <test_conftest.TestConftestVisibility object at 0x7ffffddc30a0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids8')>
chdir = 'runner', testarg = '../package/swc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdd79e50>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: empty
pytestarg        : ../package/swc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdddbac0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdd87b50>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdddba00>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids8 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdd79d00>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdd79d90>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdff7460>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdff70a0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdff75e0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffdff8070>
E                                                                        [100%]
==================================== ERRORS ====================================
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids8/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids8/package/swc/test_with_conftest.py:1
1 error in 0.02s
_ TestConftestVisibility.test_parsefactories_relative_node_ids[package-./swc-1] _

self = <test_conftest.TestConftestVisibility object at 0x7ffffdffe0d0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids9')>
chdir = 'package', testarg = './swc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdc642b0>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package
pytestarg        : ./swc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdd57af0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdfe08e0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdd57a90>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdcd5b20>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdcd5040>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdc643d0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdc64850>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdc64730>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffdc5cd60>
E                                                                        [100%]
==================================== ERRORS ====================================
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids9/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids9/package/swc/test_with_conftest.py:1
1 error in 0.02s
____ TestConftestVisibility.test_parsefactories_relative_node_ids[swc-.-1] _____

self = <test_conftest.TestConftestVisibility object at 0x7ffffdc536a0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids10')>
chdir = 'swc', testarg = '.', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe40c040>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/swc
pytestarg        : .
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdc55700>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdcca190>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdc556d0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session swc exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe40c280>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe40c550>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe435400>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe4354f0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe437700>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe438940>
E                                                                        [100%]
==================================== ERRORS ====================================
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids10/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids10/package/swc/test_with_conftest.py:1
1 error in 0.01s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[snc-../swc-1] __

self = <test_conftest.TestConftestVisibility object at 0x7ffffe7bd6a0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids11')>
chdir = 'snc', testarg = '../swc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe7553a0>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/snc
pytestarg        : ../swc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffead9970>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdce0ac0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffead9760>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe9996a0>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe9995e0>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe9c0df0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe9c08e0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe9c0d00>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe59df10>
E                                                                        [100%]
==================================== ERRORS ====================================
_____________________ ERROR at setup of test_with_conftest _____________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids11/package/swc/test_with_conftest.py, line 1
  def test_with_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids11/package/swc/test_with_conftest.py:1
1 error in 0.01s
_ TestConftestVisibility.test_parsefactories_relative_node_ids[runner-../package/snc-1] _

self = <test_conftest.TestConftestVisibility object at 0x7ffffea7ed60>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids12')>
chdir = 'runner', testarg = '../package/snc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffeb39640>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: empty
pytestarg        : ../package/snc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe7962e0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe5566a0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe796280>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session test_parsefactories_relative_node_ids12 exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffeb39b50>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffeb399d0>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffeaa6910>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe7a13d0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe7a1460>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe5368e0>
E                                                                        [100%]
==================================== ERRORS ====================================
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids12/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids12/package/snc/test_no_conftest.py:1
1 error in 0.04s
_ TestConftestVisibility.test_parsefactories_relative_node_ids[package-./snc-1] _

self = <test_conftest.TestConftestVisibility object at 0x7ffffe7c5cd0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids13')>
chdir = 'package', testarg = './snc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe581f40>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package
pytestarg        : ./snc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffe7c52e0>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe9ead90>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffe7c50a0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe581670>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe581d30>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe581430>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe7a7550>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe7a7580>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe7af7f0>
E                                                                        [100%]
==================================== ERRORS ====================================
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids13/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids13/package/snc/test_no_conftest.py:1
1 error in 0.01s
__ TestConftestVisibility.test_parsefactories_relative_node_ids[swc-../snc-1] __

self = <test_conftest.TestConftestVisibility object at 0x7ffffdc1b370>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids14')>
chdir = 'swc', testarg = '../snc', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffdc2df10>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/swc
pytestarg        : ../snc
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdc1bc40>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffdc21190>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdc1bbe0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session package exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffdc2dc70>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffdc2dc10>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffdc2deb0>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffdcbc4f0>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffdcbc1c0>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffdc9aa60>
E                                                                        [100%]
==================================== ERRORS ====================================
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids14/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids14/package/snc/test_no_conftest.py:1
1 error in 0.01s
____ TestConftestVisibility.test_parsefactories_relative_node_ids[snc-.-1] _____

self = <test_conftest.TestConftestVisibility object at 0x7ffffdca48e0>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids15')>
chdir = 'snc', testarg = '.', expect_ntests_passed = 1

    @pytest.mark.parametrize(
        "chdir,testarg,expect_ntests_passed",
        [
            # Effective target: package/..
            ("runner", "..", 3),
            ("package", "..", 3),
            ("swc", "../..", 3),
            ("snc", "../..", 3),
            # Effective target: package
            ("runner", "../package", 3),
            ("package", ".", 3),
            ("swc", "..", 3),
            ("snc", "..", 3),
            # Effective target: package/swc
            ("runner", "../package/swc", 1),
            ("package", "./swc", 1),
            ("swc", ".", 1),
            ("snc", "../swc", 1),
            # Effective target: package/snc
            ("runner", "../package/snc", 1),
            ("package", "./snc", 1),
            ("swc", "../snc", 1),
            ("snc", ".", 1),
        ],
    )
    def test_parsefactories_relative_node_ids(
        self, testdir, chdir, testarg, expect_ntests_passed
    ):
        """#616"""
        dirs = self._setup_tree(testdir)
        print("pytest run in cwd: %s" % (dirs[chdir].relto(testdir.tmpdir)))
        print("pytestarg        : %s" % (testarg))
        print("expected pass    : %s" % (expect_ntests_passed))
        with dirs[chdir].as_cwd():
            reprec = testdir.inline_run(testarg, "-q", "--traceconfig")
>           reprec.assertoutcome(passed=expect_ntests_passed)

/testbed/testing/test_conftest.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.pytester.HookRecorder object at 0x7ffffe345a30>, passed = 1
skipped = 0, failed = 0

    def assertoutcome(self, passed=0, skipped=0, failed=0):
        realpassed, realskipped, realfailed = self.listoutcomes()
>       assert passed == len(realpassed)
E       assert 1 == 0
E        +  where 0 = len([])

/testbed/src/_pytest/pytester.py:302: AssertionError
----------------------------- Captured stdout call -----------------------------
created directory structure:
   package/snc/test_no_conftest.py
   package/snc/__init__.py
   package/swc/test_with_conftest.py
   package/swc/conftest.py
   package/swc/__init__.py
   package/snc
   package/conftest.py
   package/test_pkgroot.py
   package/swc
   package
   empty
pytest run in cwd: package/snc
pytestarg        : .
expected pass    : 1
PLUGIN registered: <_pytest.config.PytestPluginManager object at 0x7ffffdc1b610>
PLUGIN registered: <_pytest.config.Config object at 0x7ffffe5798b0>
PLUGIN registered: <module '_pytest.mark' from '/testbed/src/_pytest/mark/__init__.py'>
PLUGIN registered: <module '_pytest.main' from '/testbed/src/_pytest/main.py'>
PLUGIN registered: <module '_pytest.runner' from '/testbed/src/_pytest/runner.py'>
PLUGIN registered: <module '_pytest.fixtures' from '/testbed/src/_pytest/fixtures.py'>
PLUGIN registered: <module '_pytest.helpconfig' from '/testbed/src/_pytest/helpconfig.py'>
PLUGIN registered: <module '_pytest.python' from '/testbed/src/_pytest/python.py'>
PLUGIN registered: <module '_pytest.terminal' from '/testbed/src/_pytest/terminal.py'>
PLUGIN registered: <module '_pytest.debugging' from '/testbed/src/_pytest/debugging.py'>
PLUGIN registered: <module '_pytest.unittest' from '/testbed/src/_pytest/unittest.py'>
PLUGIN registered: <module '_pytest.capture' from '/testbed/src/_pytest/capture.py'>
PLUGIN registered: <module '_pytest.skipping' from '/testbed/src/_pytest/skipping.py'>
PLUGIN registered: <module '_pytest.tmpdir' from '/testbed/src/_pytest/tmpdir.py'>
PLUGIN registered: <module '_pytest.monkeypatch' from '/testbed/src/_pytest/monkeypatch.py'>
PLUGIN registered: <module '_pytest.recwarn' from '/testbed/src/_pytest/recwarn.py'>
PLUGIN registered: <module '_pytest.pastebin' from '/testbed/src/_pytest/pastebin.py'>
PLUGIN registered: <module '_pytest.nose' from '/testbed/src/_pytest/nose.py'>
PLUGIN registered: <module '_pytest.assertion' from '/testbed/src/_pytest/assertion/__init__.py'>
PLUGIN registered: <module '_pytest.junitxml' from '/testbed/src/_pytest/junitxml.py'>
PLUGIN registered: <module '_pytest.resultlog' from '/testbed/src/_pytest/resultlog.py'>
PLUGIN registered: <module '_pytest.doctest' from '/testbed/src/_pytest/doctest.py'>
PLUGIN registered: <module '_pytest.cacheprovider' from '/testbed/src/_pytest/cacheprovider.py'>
PLUGIN registered: <module '_pytest.freeze_support' from '/testbed/src/_pytest/freeze_support.py'>
PLUGIN registered: <module '_pytest.setuponly' from '/testbed/src/_pytest/setuponly.py'>
PLUGIN registered: <module '_pytest.setupplan' from '/testbed/src/_pytest/setupplan.py'>
PLUGIN registered: <module '_pytest.stepwise' from '/testbed/src/_pytest/stepwise.py'>
PLUGIN registered: <module '_pytest.warnings' from '/testbed/src/_pytest/warnings.py'>
PLUGIN registered: <module '_pytest.logging' from '/testbed/src/_pytest/logging.py'>
PLUGIN registered: <module '_pytest.reports' from '/testbed/src/_pytest/reports.py'>
PLUGIN registered: <module '_pytest.faulthandler' from '/testbed/src/_pytest/faulthandler.py'>
PLUGIN registered: <_pytest.pytester.Testdir.inline_run.<locals>.Collect object at 0x7ffffdc1b4c0>
PLUGIN registered: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=18 _state='suspended'> err=<FDCapture 2 oldfd=19 _state='suspended'> in_=<FDCapture 0 oldfd=16 _state='started'> _state='suspended' _in_suspended='<UNSET>'> _current_item=None>
PLUGIN registered: <Session snc exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
PLUGIN registered: <_pytest.cacheprovider.LFPlugin object at 0x7ffffe345850>
PLUGIN registered: <_pytest.cacheprovider.NFPlugin object at 0x7ffffe345130>
PLUGIN registered: <_pytest.stepwise.StepwisePlugin object at 0x7ffffe345880>
PLUGIN registered: <_pytest.terminal.TerminalReporter object at 0x7ffffe345160>
PLUGIN registered: <_pytest.logging.LoggingPlugin object at 0x7ffffe345d60>
PLUGIN registered: <_pytest.fixtures.FixtureManager object at 0x7ffffe2faf10>
E                                                                        [100%]
==================================== ERRORS ====================================
______________________ ERROR at setup of test_no_conftest ______________________
file /tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids15/package/snc/test_no_conftest.py, line 1
  def test_no_conftest(fxtr):
E       fixture 'fxtr' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_parsefactories_relative_node_ids15/package/snc/test_no_conftest.py:1
1 error in 0.01s
__________________ test_search_conftest_up_to_inifile[.-2-0] ___________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0')>
confcutdir = '.', passed = 2, error = 0

    @pytest.mark.parametrize(
        "confcutdir,passed,error", [(".", 2, 0), ("src", 1, 1), (None, 1, 1)]
    )
    def test_search_conftest_up_to_inifile(testdir, confcutdir, passed, error):
        """Test that conftest files are detected only up to an ini file, unless
        an explicit --confcutdir option is given.
        """
        root = testdir.tmpdir
        src = root.join("src").ensure(dir=1)
        src.join("pytest.ini").write("[pytest]")
        src.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def fix1(): pass
                """
            )
        )
        src.join("test_foo.py").write(
            textwrap.dedent(
                """\
                def test_1(fix1):
                    pass
                def test_2(out_of_reach):
                    pass
                """
            )
        )
        root.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def out_of_reach(): pass
                """
            )
        )
    
        args = [str(src)]
        if confcutdir:
            args = ["--confcutdir=%s" % root.join(confcutdir)]
        result = testdir.runpytest(*args)
        match = ""
        if passed:
            match += "*%d passed*" % passed
        if error:
            match += "*%d error*" % error
>       result.stdout.fnmatch_lines(match)
E       Failed: nomatch: '*2 passed*'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0'
E           and: 'collected 2 items'
E           and: ''
E           and: 'src/test_foo.py EE                                                       [100%]'
E           and: ''
E           and: '==================================== ERRORS ===================================='
E           and: '___________________________ ERROR at setup of test_1 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py, line 1'
E           and: '  def test_1(fix1):'
E           and: "E       fixture 'fix1' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py:1'
E           and: '___________________________ ERROR at setup of test_2 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py, line 3'
E           and: '  def test_2(out_of_reach):'
E           and: "E       fixture 'out_of_reach' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py:3'
E           and: '=============================== 2 error in 0.01s ==============================='
E           and: ''
E       remains unmatched: '*2 passed*'

/testbed/testing/test_conftest.py:577: Failed
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0
collected 2 items

src/test_foo.py EE                                                       [100%]

==================================== ERRORS ====================================
___________________________ ERROR at setup of test_1 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py, line 1
  def test_1(fix1):
E       fixture 'fix1' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py:1
___________________________ ERROR at setup of test_2 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py, line 3
  def test_2(out_of_reach):
E       fixture 'out_of_reach' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile0/src/test_foo.py:3
=============================== 2 error in 0.01s ===============================
_________________ test_search_conftest_up_to_inifile[src-1-1] __________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1')>
confcutdir = 'src', passed = 1, error = 1

    @pytest.mark.parametrize(
        "confcutdir,passed,error", [(".", 2, 0), ("src", 1, 1), (None, 1, 1)]
    )
    def test_search_conftest_up_to_inifile(testdir, confcutdir, passed, error):
        """Test that conftest files are detected only up to an ini file, unless
        an explicit --confcutdir option is given.
        """
        root = testdir.tmpdir
        src = root.join("src").ensure(dir=1)
        src.join("pytest.ini").write("[pytest]")
        src.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def fix1(): pass
                """
            )
        )
        src.join("test_foo.py").write(
            textwrap.dedent(
                """\
                def test_1(fix1):
                    pass
                def test_2(out_of_reach):
                    pass
                """
            )
        )
        root.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def out_of_reach(): pass
                """
            )
        )
    
        args = [str(src)]
        if confcutdir:
            args = ["--confcutdir=%s" % root.join(confcutdir)]
        result = testdir.runpytest(*args)
        match = ""
        if passed:
            match += "*%d passed*" % passed
        if error:
            match += "*%d error*" % error
>       result.stdout.fnmatch_lines(match)
E       Failed: nomatch: '*1 passed**1 error*'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1'
E           and: 'collected 2 items'
E           and: ''
E           and: 'src/test_foo.py EE                                                       [100%]'
E           and: ''
E           and: '==================================== ERRORS ===================================='
E           and: '___________________________ ERROR at setup of test_1 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py, line 1'
E           and: '  def test_1(fix1):'
E           and: "E       fixture 'fix1' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py:1'
E           and: '___________________________ ERROR at setup of test_2 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py, line 3'
E           and: '  def test_2(out_of_reach):'
E           and: "E       fixture 'out_of_reach' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py:3'
E           and: '=============================== 2 error in 0.01s ==============================='
E           and: ''
E       remains unmatched: '*1 passed**1 error*'

/testbed/testing/test_conftest.py:577: Failed
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1
collected 2 items

src/test_foo.py EE                                                       [100%]

==================================== ERRORS ====================================
___________________________ ERROR at setup of test_1 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py, line 1
  def test_1(fix1):
E       fixture 'fix1' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py:1
___________________________ ERROR at setup of test_2 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py, line 3
  def test_2(out_of_reach):
E       fixture 'out_of_reach' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile1/src/test_foo.py:3
=============================== 2 error in 0.01s ===============================
_________________ test_search_conftest_up_to_inifile[None-1-1] _________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2')>
confcutdir = None, passed = 1, error = 1

    @pytest.mark.parametrize(
        "confcutdir,passed,error", [(".", 2, 0), ("src", 1, 1), (None, 1, 1)]
    )
    def test_search_conftest_up_to_inifile(testdir, confcutdir, passed, error):
        """Test that conftest files are detected only up to an ini file, unless
        an explicit --confcutdir option is given.
        """
        root = testdir.tmpdir
        src = root.join("src").ensure(dir=1)
        src.join("pytest.ini").write("[pytest]")
        src.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def fix1(): pass
                """
            )
        )
        src.join("test_foo.py").write(
            textwrap.dedent(
                """\
                def test_1(fix1):
                    pass
                def test_2(out_of_reach):
                    pass
                """
            )
        )
        root.join("conftest.py").write(
            textwrap.dedent(
                """\
                import pytest
                @pytest.fixture
                def out_of_reach(): pass
                """
            )
        )
    
        args = [str(src)]
        if confcutdir:
            args = ["--confcutdir=%s" % root.join(confcutdir)]
        result = testdir.runpytest(*args)
        match = ""
        if passed:
            match += "*%d passed*" % passed
        if error:
            match += "*%d error*" % error
>       result.stdout.fnmatch_lines(match)
E       Failed: nomatch: '*1 passed**1 error*'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src, inifile: pytest.ini'
E           and: 'collected 2 items'
E           and: ''
E           and: 'src/test_foo.py EE                                                       [100%]'
E           and: ''
E           and: '==================================== ERRORS ===================================='
E           and: '___________________________ ERROR at setup of test_1 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py, line 1'
E           and: '  def test_1(fix1):'
E           and: "E       fixture 'fix1' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py:1'
E           and: '___________________________ ERROR at setup of test_2 ___________________________'
E           and: 'file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py, line 3'
E           and: '  def test_2(out_of_reach):'
E           and: "E       fixture 'out_of_reach' not found"
E           and: '>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory'
E           and: ">       use 'pytest --fixtures [testpath]' for help on them."
E           and: ''
E           and: '/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py:3'
E           and: '=============================== 2 error in 0.01s ==============================='
E           and: ''
E       remains unmatched: '*1 passed**1 error*'

/testbed/testing/test_conftest.py:577: Failed
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src, inifile: pytest.ini
collected 2 items

src/test_foo.py EE                                                       [100%]

==================================== ERRORS ====================================
___________________________ ERROR at setup of test_1 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py, line 1
  def test_1(fix1):
E       fixture 'fix1' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py:1
___________________________ ERROR at setup of test_2 ___________________________
file /tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py, line 3
  def test_2(out_of_reach):
E       fixture 'out_of_reach' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/tmp/pytest-of-root/pytest-0/test_search_conftest_up_to_inifile2/src/test_foo.py:3
=============================== 2 error in 0.01s ===============================
_______________________________ test_hook_proxy ________________________________

testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_hook_proxy0')>

    def test_hook_proxy(testdir):
        """Session's gethookproxy() would cache conftests incorrectly (#2016).
        It was decided to remove the cache altogether.
        """
        testdir.makepyfile(
            **{
                "root/demo-0/test_foo1.py": "def test1(): pass",
                "root/demo-a/test_foo2.py": "def test1(): pass",
                "root/demo-a/conftest.py": """\
                def pytest_ignore_collect(path, config):
                    return True
                """,
                "root/demo-b/test_foo3.py": "def test1(): pass",
                "root/demo-c/test_foo4.py": "def test1(): pass",
            }
        )
        result = testdir.runpytest()
>       result.stdout.fnmatch_lines(
            ["*test_foo1.py*", "*test_foo3.py*", "*test_foo4.py*", "*3 passed*"]
        )
E       Failed: nomatch: '*test_foo1.py*'
E           and: '============================= test session starts =============================='
E           and: 'platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1'
E           and: 'rootdir: /tmp/pytest-of-root/pytest-0/test_hook_proxy0'
E           and: 'collected 4 items'
E           and: ''
E       fnmatch: '*test_foo1.py*'
E          with: 'root/demo-0/test_foo1.py .                                               [ 25%]'
E       nomatch: '*test_foo3.py*'
E           and: 'root/demo-a/test_foo2.py .                                               [ 50%]'
E       fnmatch: '*test_foo3.py*'
E          with: 'root/demo-b/test_foo3.py .                                               [ 75%]'
E       fnmatch: '*test_foo4.py*'
E          with: 'root/demo-c/test_foo4.py .                                               [100%]'
E       nomatch: '*3 passed*'
E           and: ''
E           and: '============================== 4 passed in 0.02s ==============================='
E           and: ''
E       remains unmatched: '*3 passed*'

/testbed/testing/test_conftest.py:634: Failed
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_hook_proxy0
collected 4 items

root/demo-0/test_foo1.py .                                               [ 25%]
root/demo-a/test_foo2.py .                                               [ 50%]
root/demo-b/test_foo3.py .                                               [ 75%]
root/demo-c/test_foo4.py .                                               [100%]

============================== 4 passed in 0.02s ===============================
==================================== PASSES ====================================
___________________________ test_conftest_uppercase ____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_conftest_uppercase0
collected 0 items

============================ no tests ran in 0.01s =============================
_______________________________ test_no_conftest _______________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_no_conftest0
collected 0 items

============================ no tests ran in 0.00s =============================
----------------------------- Captured stderr call -----------------------------
ImportError while loading conftest '/tmp/pytest-of-root/pytest-0/test_no_conftest0/conftest.py'.
conftest.py:1: in <module>
    assert 0
E   AssertionError: assert 0
___________________________ test_fixture_dependency ____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_fixture_dependency0
collected 1 item

sub/subsub/test_bar.py .                                                 [100%]

============================== 1 passed in 0.01s ===============================
___________________ test_issue1073_conftest_special_objects ____________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-5.1.3.dev15+gaa48a850a.d20260222, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_issue1073_conftest_special_objects0
collected 1 item

test_issue1073_conftest_special_objects.py .                             [100%]

============================== 1 passed in 0.01s ===============================
_______________________ test_conftest_exception_handling _______________________
----------------------------- Captured stderr call -----------------------------
ImportError while loading conftest '/tmp/pytest-of-root/pytest-0/test_conftest_exception_handling0/conftest.py'.
conftest.py:1: in <module>
    raise ValueError()
E   ValueError
__________________________ test_required_option_help ___________________________
----------------------------- Captured stdout call -----------------------------
usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched
                        against test names and their parent classes. Example:
                        -k 'test_method or test_other' matches all test
                        functions and classes whose name contains
                        'test_method' or 'test_other', while -k 'not
                        test_method' matches those that don't contain
                        'test_method' in their names. -k 'not test_method and
                        not test_other' will eliminate the matches.
                        Additionally keywords are matched to classes and
                        functions containing extra names in their
                        'extra_keyword_matches' set, as well as functions
                        which have names assigned directly to them.
  -m MARKEXPR           only run tests matching given mark expression.
                        example: -m 'mark1 and not mark2'.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --maxfail=num         exit after first num failures or errors.
  --strict-markers, --strict
                        markers not registered in the `markers` section of the
                        configuration file raise errors.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        'root_dir', './root_dir', 'root_dir/another_dir/';
                        absolute path: '/home/user/root_dir'; path with
                        variables: '$HOME/root_dir'.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading '_' are only shown with '-v')
  --fixtures-per-test   show fixtures per test
  --import-mode={prepend,append}
                        prepend/append to sys.path when importing test
                        modules, default is to prepend.
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or
                        all if none failed)
  --ff, --failed-first  run all tests but run the last failures first. This
                        may re-order tests and thus lead to repeated fixture
                        setup/teardown
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don't perform collection or
                        tests. Optional argument: glob (default: '*').
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known)
                        failures.
  --sw, --stepwise      exit on test failure and continue from last failing
                        test next time
  --stepwise-skip       ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  -v, --verbose         increase verbosity.
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default
                        (see --disable-warnings).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is 'all'.
  --full-trace          don't cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --result-log=path     DEPRECATED path for machine-readable result log.

collection:
  --collect-only        only collect tests, don't execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item during collection (multi-allowed).
  --confcutdir=dir      only load conftest.py's relative to specified dir.
  --noconftest          Don't load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don't ignore tests in a local virtualenv directory
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  --version             display pytest lib version and import information.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed). To avoid loading of plugins, use the
                        `no:` prefix, e.g. `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        'pytestdebug.log'.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with "option=value" style, e.g.
                        `-o xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools. 'plain' performs no
                        assertion debugging. 'rewrite' (the default) rewrites
                        assert statements in test modules on import to provide
                        assert expression information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but
                        don't execute anything.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.

logging:
  --no-print-logs       disable printing caught logs on failed tests.
  --log-level=LOG_LEVEL
                        logging level used by the logging module
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: "classic", or with additional progress
                        information ("progress" (percentage) | "count").
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|system-out|system-err
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after -W and
                        --pythonwarnings.
  log_print (bool):     default value for --no-print-logs
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as "live
                        logging").
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish. Not available on
                        Windows.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest's internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option
warning : /testbed/src/_pytest/config/__init__.py:881: PytestConfigWarning: could not load initial conftests: /tmp/pytest-of-root/pytest-0/test_required_option_help0/conftest.py
  _issue_warning_captured(

=========================== short test summary info ============================
PASSED testing/test_conftest.py::TestConftestValueAccessGlobal::test_immediate_initialiation_and_incremental_are_the_same[global]
PASSED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_not_existing[global]
PASSED testing/test_conftest.py::TestConftestValueAccessGlobal::test_immediate_initialiation_and_incremental_are_the_same[inpackage]
PASSED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_not_existing[inpackage]
PASSED testing/test_conftest.py::test_conftest_in_nonpkg_with_init
PASSED testing/test_conftest.py::test_doubledash_considered
PASSED testing/test_conftest.py::test_issue151_load_all_conftests
PASSED testing/test_conftest.py::test_setinitial_conftest_subdirs[test]
PASSED testing/test_conftest.py::test_setinitial_conftest_subdirs[tests]
PASSED testing/test_conftest.py::test_setinitial_conftest_subdirs[whatever]
PASSED testing/test_conftest.py::test_setinitial_conftest_subdirs[.dotdir]
PASSED testing/test_conftest.py::test_conftest_uppercase
PASSED testing/test_conftest.py::test_no_conftest
PASSED testing/test_conftest.py::test_conftest_import_order
PASSED testing/test_conftest.py::test_fixture_dependency
PASSED testing/test_conftest.py::test_issue1073_conftest_special_objects
PASSED testing/test_conftest.py::test_conftest_exception_handling
PASSED testing/test_conftest.py::test_required_option_help
SKIPPED [1] testing/test_conftest.py:281: only relevant for case insensitive file systems
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_basic_init[global]
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_by_path[global]
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_with_confmod[global]
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_basic_init[inpackage]
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_by_path[inpackage]
FAILED testing/test_conftest.py::TestConftestValueAccessGlobal::test_value_access_with_confmod[inpackage]
FAILED testing/test_conftest.py::test_conftest_global_import - assert 1 == 0
FAILED testing/test_conftest.py::test_conftestcutdir - IndexError: list index...
FAILED testing/test_conftest.py::test_conftestcutdir_inplace_considered - Att...
FAILED testing/test_conftest.py::test_conftest_confcutdir - Failed: nomatch: ...
FAILED testing/test_conftest.py::test_conftest_symlink - Failed: fnmatch: '*c...
FAILED testing/test_conftest.py::test_conftest_symlink_files - Failed: fnmatc...
FAILED testing/test_conftest.py::test_conftest_existing_resultlog - Failed: n...
FAILED testing/test_conftest.py::test_conftest_existing_junitxml - Failed: no...
FAILED testing/test_conftest.py::test_conftest_found_with_double_dash - Faile...
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-../..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-../..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-.-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-..-3]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package/swc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-./swc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-.-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-../swc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package/snc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-./snc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-../snc-1]
FAILED testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-.-1]
FAILED testing/test_conftest.py::test_search_conftest_up_to_inifile[.-2-0] - ...
FAILED testing/test_conftest.py::test_search_conftest_up_to_inifile[src-1-1]
FAILED testing/test_conftest.py::test_search_conftest_up_to_inifile[None-1-1]
FAILED testing/test_conftest.py::test_hook_proxy - Failed: nomatch: '*test_fo...
=================== 35 failed, 18 passed, 1 skipped in 3.21s ===================
+ : '>>>>> End Test Output'
+ git checkout 73c5b7f4b11a81e971f7d1bb18072e06a87060f4 testing/test_conftest.py
Updated 1 path from 6869f107f
